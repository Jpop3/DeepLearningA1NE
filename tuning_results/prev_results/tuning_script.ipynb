{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import itertools\n",
    "import numpy as np\n",
    "from MLP import *\n",
    "import os\n",
    "import time\n",
    "import itertools\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load('Assignment1-Dataset/train_data.npy')\n",
    "labels = np.load('Assignment1-Dataset/train_label.npy')\n",
    "y = np.array([MLP.class_to_one_hot(label, 10) for label in labels])\n",
    "\n",
    "# Seed for reproducibility\n",
    "np.random.seed(0)\n",
    "\n",
    "# 5-fold Cross Validation\n",
    "n_samples = X.shape[0]\n",
    "n_folds = 5\n",
    "indices = np.arange(n_samples)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "fold_sizes = np.full(n_folds, n_samples // n_folds, dtype=int)\n",
    "fold_sizes[:n_samples % n_folds] += 1\n",
    "current = 0\n",
    "folds = []\n",
    "for fold_size in fold_sizes:\n",
    "    start, stop = current, current + fold_size\n",
    "    folds.append(indices[start:stop])\n",
    "    current = stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Constants ###\n",
    "\n",
    "# SETUP = {\n",
    "#     'epochs': 40,\n",
    "#     'activations': [None, 'ReLU', 'ReLU', 'softmax'],\n",
    "#     'input_size': 128,\n",
    "#     'early_stopping': (10, 0.001)\n",
    "# }\n",
    "\n",
    "# ### Options for Hyper-Parameters ###\n",
    "\n",
    "# hidden_layer_options = [[128, 64, 32, 10], [128, 96, 64, 10]]\n",
    "# lr_options = [0.001, 0.0001, 0.01]\n",
    "# batch_size_options = [128, 64, 16, 4]\n",
    "# optimiser_options = [None, 'Adam', 'Momentum']\n",
    "# bn_option = [False, True]\n",
    "# weight_decay_options = [0.0, 0.0001, 0.001]\n",
    "# drop_out_options = [[0.0, 0.0, 0.0, 0.0], [0.0, 0.2, 0.2, 0.0], [0.2, 0.3, 0.3, 0.2], [0.0, 0.4, 0.4, 0.0]]\n",
    "\n",
    "# lr_options = [0.005, 0.001, 0.0001]\n",
    "# batch_size_options = [4,8,16]\n",
    "# optimiser_options = ['Adam', 'Momentum'] # None as well\n",
    "# bn_option = [False, True]\n",
    "\n",
    "# dropout_options = []\n",
    "# size_options = []\n",
    "# weight_decay_options\n",
    "# activation_options = ['ReLU', 'tanh']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_to_file(hyperparams_dict, cv_metrics, time_taken, filename=\"model_performance_2.csv\"):\n",
    "    # Check if file exists, if not, write headers\n",
    "    file_exists = os.path.isfile(filename)\n",
    "    hidden_layers = hyperparams_dict['hidden_layers']\n",
    "    lr = hyperparams_dict['lr']\n",
    "    batch_size = hyperparams_dict['batch_size']\n",
    "    optimiser = hyperparams_dict['optimiser']\n",
    "    bn = hyperparams_dict['bn']\n",
    "    weight_decay = hyperparams_dict['weight_decay']\n",
    "    dropout = hyperparams_dict['dropout']\n",
    "    \n",
    "    with open(filename, mode='a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        if not file_exists:\n",
    "            # Write the header\n",
    "            # CV Metrics structure cv_train_loss_scores, cv_val_loss_scores, cv_val_accuracy_scores, cv_val_recall_scores, cv_val_f1_scores, cv_times, detailed_train_losses, detailed_val_losses\n",
    "            \n",
    "            writer.writerow([\"Train Loss\", \"Val Loss\", \"Val Accuracy\", \"Val Recall\", \"Val F1\", \"Learning Rate\", \"Batch Size\", \"Optimizer\", \"Batch Norm\", \"Weight Decay\", \"Dropout\", \"Hidden Layers\", \"Time Taken Test\", \"Time Taken CV\", \"Train Losses CV\", \"Val Losses CV\"])\n",
    "        \n",
    "        # Write the data\n",
    "        for i, (cv_train_loss_score, cv_val_loss_score, cv_val_accuracy_score, cv_val_recall_score, cv_val_f1_score, cv_time, detailed_train_loss, detailed_val_loss) in enumerate(zip(*cv_metrics)):\n",
    "            # writer.writerow([hyperparams[0], hyperparams[1], hyperparams[2], hyperparams[3], hyperparams[4], hyperparams[5], hyperparams[6], cv_train_loss_scores, cv_val_loss_scores, cv_val_accuracy_scores, cv_val_recall_scores, cv_val_f1_scores, time_taken, cv_times, detailed_train_losses, detailed_val_losses])\n",
    "            # writer.writerow([round(cv_train_loss_scores, 4), round(cv_val_loss_scores, 4), round(cv_val_accuracy_scores, 4), round(cv_val_recall_scores, 4), round(cv_val_f1_scores, 4), lr, batch_size, optimiser, bn, weight_decay, dropout, hidden_layers, time_taken, cv_times, detailed_train_losses, detailed_val_losses])\n",
    "            writer.writerow([\\\n",
    "                round(cv_train_loss_score, 4), \n",
    "                round(cv_val_loss_score, 4), \n",
    "                round(cv_val_accuracy_score, 4), \n",
    "                round(cv_val_recall_score, 4), \n",
    "                round(cv_val_f1_score, 4), \n",
    "                hyperparams_dict['lr'], \n",
    "                hyperparams_dict['batch_size'], \n",
    "                hyperparams_dict['optimiser'], \n",
    "                hyperparams_dict['bn'], \n",
    "                hyperparams_dict['weight_decay'], \n",
    "                hyperparams_dict['dropout'], \n",
    "                hyperparams_dict['hidden_layers'], \n",
    "                time_taken, \n",
    "                cv_time,\n",
    "                detailed_train_loss, \n",
    "                detailed_val_loss\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Constants ###\n",
    "\n",
    "SETUP = {\n",
    "    'epochs': 50,\n",
    "    'activations': [None, 'ReLU', 'ReLU', 'softmax'],\n",
    "    'input_size': 128,\n",
    "    'early_stopping': (10, 0.001)\n",
    "}\n",
    "\n",
    "### Options for Hyper-Parameters ###\n",
    "\n",
    "weight_decay_options = [0.0, 0.0001, 0.001]\n",
    "drop_out_options = [[0.0, 0.0, 0.0, 0.0], [0.5, 0.2, 0.2, 0.0], [0.1, 0.3, 0.3, 0.1]]\n",
    "hidden_layer_options = [[128, 64, 32, 10], [128, 96, 64, 10]]\n",
    "lr_options = [0.001, 0.0001, 0.01]\n",
    "optimiser_options = [None, 'Adam', 'Momentum']\n",
    "bn_option = [False, True]\n",
    "batch_size_options = [16,8,4,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to run your model\n",
    "def run_model_cv(fold_splits, hyperparams_dict):\n",
    "    # weight_decay_options, drop_out_options, hidden_layer_options, lr_options, optimiser_options, bn_option, batch_size_options\n",
    "    cv_train_loss_scores = []\n",
    "    cv_val_loss_scores = []\n",
    "    detailed_train_losses = []\n",
    "    detailed_val_losses = []\n",
    "    cv_val_accuracy_scores = []\n",
    "    cv_val_recall_scores = []\n",
    "    cv_val_f1_scores = []\n",
    "    cv_times = []\n",
    "    hidden_layers, lr, batch_size = hyperparams_dict['hidden_layers'], hyperparams_dict['lr'], hyperparams_dict['batch_size']\n",
    "    optimiser, bn, weight_decay, dropout = hyperparams_dict['optimiser'], hyperparams_dict['bn'], hyperparams_dict['weight_decay'], hyperparams_dict['dropout']\n",
    "    \n",
    "    for i in range(n_folds):\n",
    "        X_train, y_train, X_val, y_val = fold_splits[i]\n",
    "        \n",
    "        # Initialize MLP model with current hyperparameters\n",
    "        nn = MLP(hidden_layers, SETUP['activations'], bn, weight_decay, dropout)\n",
    "        \n",
    "        # Fit the model\n",
    "        start = time.time()\n",
    "        nn_output = nn.fit(X_train, y_train, X_val, y_val, learning_rate=lr, epochs=SETUP['epochs'], batch_size=batch_size, optimiser=optimiser, early_stopping=SETUP['early_stopping'])\n",
    "        end = time.time()\n",
    "        \n",
    "        # Extract metrics\n",
    "        train_loss, val_loss, early_stop_epoch = nn_output\n",
    "        time_taken = end - start\n",
    "        cv_times.append(time_taken)\n",
    "        \n",
    "        # Predict on validation set\n",
    "        output_val = nn.predict(X_val)\n",
    "        \n",
    "        # Calulate metrics\n",
    "        val_accuracy = accuracy_score(np.argmax(y_val, axis=1), np.argmax(output_val, axis=1))\n",
    "        val_recall = recall_score(np.argmax(y_val, axis=1), np.argmax(output_val, axis=1), average='macro')\n",
    "        val_f1 = f1_score(np.argmax(y_val, axis=1), np.argmax(output_val, axis=1), average='macro')\n",
    "        \n",
    "        # Append metrics for this fold\n",
    "        cv_train_loss_scores.append(train_loss[-1])\n",
    "        cv_val_loss_scores.append(val_loss[-1])\n",
    "        detailed_train_losses.append(train_loss)\n",
    "        detailed_val_losses.append(val_loss)\n",
    "        cv_val_accuracy_scores.append(val_accuracy)\n",
    "        cv_val_recall_scores.append(val_recall)\n",
    "        cv_val_f1_scores.append(val_f1)\n",
    "\n",
    "        print(f'CV_{i+1} ({time_taken:.2f}s) -\\t train_loss: {train_loss[-1]:.5f}\\t val_loss: {val_loss[-1]:.5f}\\t val_acc: {val_accuracy:.5f}\\t val_recall: {val_recall:.5f}\\t val_f1: {val_f1:.5f}')\n",
    "    \n",
    "    return cv_train_loss_scores, cv_val_loss_scores, cv_val_accuracy_scores, cv_val_recall_scores, cv_val_f1_scores, cv_times, detailed_train_losses, detailed_val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin Hyper-parameter Tuning -\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.001, 'optimiser': None, 'bn': False, 'batch_size': 16}\n",
      "\tearly stopping at ep.17\t train loss: 0.88703\t val loss: 2.44500\t best val loss: 2.37403\n",
      "CV_1 (7.34s) -\t train_loss: 0.88703\t val_loss: 2.44500\t val_acc: 0.33450\t val_recall: 0.33385\t val_f1: 0.32795\n",
      "\tearly stopping at ep.11\t train loss: 0.78747\t val loss: 2.82258\t best val loss: 2.37687\n",
      "CV_2 (5.21s) -\t train_loss: 0.78747\t val_loss: 2.82258\t val_acc: 0.32850\t val_recall: 0.32779\t val_f1: 0.32160\n",
      "\tearly stopping at ep.30\t train loss: 0.83872\t val loss: 2.44399\t best val loss: 2.38876\n",
      "CV_3 (12.88s) -\t train_loss: 0.83872\t val_loss: 2.44399\t val_acc: 0.35430\t val_recall: 0.35525\t val_f1: 0.34826\n",
      "\tearly stopping at ep.12\t train loss: 0.82074\t val loss: 2.47603\t best val loss: 2.46724\n",
      "CV_4 (5.09s) -\t train_loss: 0.82074\t val_loss: 2.47603\t val_acc: 0.34810\t val_recall: 0.34858\t val_f1: 0.34445\n",
      "\tearly stopping at ep.33\t train loss: 0.76007\t val loss: 2.75853\t best val loss: 2.32553\n",
      "CV_5 (14.25s) -\t train_loss: 0.76007\t val_loss: 2.75853\t val_acc: 0.35730\t val_recall: 0.35617\t val_f1: 0.35308\n",
      "## Experiment results (44.95s) - {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.001, 'optimiser': None, 'bn': False, 'batch_size': 16}\n",
      "# Metrics - train loss: 0.81881 \t val loss: 2.58923 \t val accuracy: 0.34454 \t val recall: 0.34433 \t val f1: 0.33907\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.001, 'optimiser': None, 'bn': False, 'batch_size': 8}\n",
      "\tearly stopping at ep.14\t train loss: 0.86718\t val loss: 1.96615\t best val loss: 1.88581\n",
      "CV_1 (10.19s) -\t train_loss: 0.86718\t val_loss: 1.96615\t val_acc: 0.39330\t val_recall: 0.39384\t val_f1: 0.38720\n",
      "\tearly stopping at ep.32\t train loss: 0.78311\t val loss: 2.15054\t best val loss: 1.91465\n",
      "CV_2 (28.71s) -\t train_loss: 0.78311\t val_loss: 2.15054\t val_acc: 0.39620\t val_recall: 0.39462\t val_f1: 0.39793\n",
      "\tearly stopping at ep.33\t train loss: 0.81120\t val loss: 2.06469\t best val loss: 1.96513\n",
      "CV_3 (27.36s) -\t train_loss: 0.81120\t val_loss: 2.06469\t val_acc: 0.41180\t val_recall: 0.41165\t val_f1: 0.40550\n",
      "\tearly stopping at ep.14\t train loss: 0.86021\t val loss: 2.12154\t best val loss: 1.92820\n",
      "CV_4 (10.14s) -\t train_loss: 0.86021\t val_loss: 2.12154\t val_acc: 0.37540\t val_recall: 0.37661\t val_f1: 0.37412\n",
      "\tearly stopping at ep.16\t train loss: 0.85920\t val loss: 2.12743\t best val loss: 1.87576\n",
      "CV_5 (12.03s) -\t train_loss: 0.85920\t val_loss: 2.12743\t val_acc: 0.40060\t val_recall: 0.39909\t val_f1: 0.38818\n",
      "## Experiment results (88.60s) - {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.001, 'optimiser': None, 'bn': False, 'batch_size': 8}\n",
      "# Metrics - train loss: 0.83618 \t val loss: 2.08607 \t val accuracy: 0.39546 \t val recall: 0.39516 \t val f1: 0.39059\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.001, 'optimiser': None, 'bn': False, 'batch_size': 4}\n",
      "\tearly stopping at ep.21\t train loss: 1.05286\t val loss: 1.70487\t best val loss: 1.59948\n",
      "CV_1 (27.89s) -\t train_loss: 1.05286\t val_loss: 1.70487\t val_acc: 0.43910\t val_recall: 0.44166\t val_f1: 0.42470\n",
      "\tearly stopping at ep.30\t train loss: 1.01642\t val loss: 1.62413\t best val loss: 1.59429\n",
      "CV_2 (39.69s) -\t train_loss: 1.01642\t val_loss: 1.62413\t val_acc: 0.45460\t val_recall: 0.45341\t val_f1: 0.45248\n",
      "\tearly stopping at ep.15\t train loss: 1.07209\t val loss: 1.66409\t best val loss: 1.62657\n",
      "CV_3 (20.86s) -\t train_loss: 1.07209\t val_loss: 1.66409\t val_acc: 0.44220\t val_recall: 0.44172\t val_f1: 0.43774\n",
      "\tearly stopping at ep.32\t train loss: 1.00083\t val loss: 1.68042\t best val loss: 1.57493\n",
      "CV_4 (42.38s) -\t train_loss: 1.00083\t val_loss: 1.68042\t val_acc: 0.44690\t val_recall: 0.44604\t val_f1: 0.44427\n",
      "\tearly stopping at ep.38\t train loss: 1.03120\t val loss: 1.66753\t best val loss: 1.55952\n",
      "CV_5 (50.06s) -\t train_loss: 1.03120\t val_loss: 1.66753\t val_acc: 0.45090\t val_recall: 0.45083\t val_f1: 0.43804\n",
      "## Experiment results (181.06s) - {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.001, 'optimiser': None, 'bn': False, 'batch_size': 4}\n",
      "# Metrics - train loss: 1.03468 \t val loss: 1.66821 \t val accuracy: 0.44674 \t val recall: 0.44673 \t val f1: 0.43945\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.001, 'optimiser': None, 'bn': False, 'batch_size': 2}\n",
      "\tearly stopping at ep.37\t train loss: 1.23708\t val loss: 1.44786\t best val loss: 1.42962\n",
      "CV_1 (93.76s) -\t train_loss: 1.23708\t val_loss: 1.44786\t val_acc: 0.48900\t val_recall: 0.49077\t val_f1: 0.48058\n",
      "\tearly stopping at ep.43\t train loss: 1.23243\t val loss: 1.43095\t best val loss: 1.41871\n",
      "CV_2 (116.62s) -\t train_loss: 1.23243\t val_loss: 1.43095\t val_acc: 0.48630\t val_recall: 0.48469\t val_f1: 0.48216\n",
      "\tearly stopping at ep.28\t train loss: 1.24398\t val loss: 1.48947\t best val loss: 1.47187\n",
      "CV_3 (76.31s) -\t train_loss: 1.24398\t val_loss: 1.48947\t val_acc: 0.47700\t val_recall: 0.47738\t val_f1: 0.47103\n",
      "\tearly stopping at ep.47\t train loss: 1.23498\t val loss: 1.45687\t best val loss: 1.43657\n",
      "CV_4 (120.69s) -\t train_loss: 1.23498\t val_loss: 1.45687\t val_acc: 0.48910\t val_recall: 0.48913\t val_f1: 0.48866\n",
      "\tearly stopping at ep.33\t train loss: 1.23873\t val loss: 1.46013\t best val loss: 1.42760\n",
      "CV_5 (86.96s) -\t train_loss: 1.23873\t val_loss: 1.46013\t val_acc: 0.48620\t val_recall: 0.48705\t val_f1: 0.48292\n",
      "## Experiment results (494.53s) - {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.001, 'optimiser': None, 'bn': False, 'batch_size': 2}\n",
      "# Metrics - train loss: 1.23744 \t val loss: 1.45706 \t val accuracy: 0.48552 \t val recall: 0.48580 \t val f1: 0.48107\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.001, 'optimiser': None, 'bn': True, 'batch_size': 16}\n",
      "\tearly stopping at ep.11\t train loss: 0.85672\t val loss: 2.87876\t best val loss: 2.64957\n",
      "CV_1 (11.63s) -\t train_loss: 0.85672\t val_loss: 2.87876\t val_acc: 0.27430\t val_recall: 0.27447\t val_f1: 0.27587\n",
      "\tearly stopping at ep.13\t train loss: 0.89339\t val loss: 3.12291\t best val loss: 2.75233\n",
      "CV_2 (13.98s) -\t train_loss: 0.89339\t val_loss: 3.12291\t val_acc: 0.26250\t val_recall: 0.26163\t val_f1: 0.26095\n",
      "\tearly stopping at ep.11\t train loss: 0.84573\t val loss: 2.93766\t best val loss: 2.68660\n",
      "CV_3 (11.84s) -\t train_loss: 0.84573\t val_loss: 2.93766\t val_acc: 0.26410\t val_recall: 0.26369\t val_f1: 0.26120\n",
      "\tearly stopping at ep.11\t train loss: 0.85626\t val loss: 2.75295\t best val loss: 2.67460\n",
      "CV_4 (11.93s) -\t train_loss: 0.85626\t val_loss: 2.75295\t val_acc: 0.30290\t val_recall: 0.30303\t val_f1: 0.29654\n",
      "\tearly stopping at ep.11\t train loss: 0.84862\t val loss: 2.77385\t best val loss: 2.65860\n",
      "CV_5 (11.71s) -\t train_loss: 0.84862\t val_loss: 2.77385\t val_acc: 0.28830\t val_recall: 0.28733\t val_f1: 0.28198\n",
      "## Experiment results (61.39s) - {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.001, 'optimiser': None, 'bn': True, 'batch_size': 16}\n",
      "# Metrics - train loss: 0.86015 \t val loss: 2.89323 \t val accuracy: 0.27842 \t val recall: 0.27803 \t val f1: 0.27531\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.001, 'optimiser': None, 'bn': True, 'batch_size': 8}\n",
      "\tearly stopping at ep.21\t train loss: 1.36252\t val loss: 1.85305\t best val loss: 1.75563\n",
      "CV_1 (40.19s) -\t train_loss: 1.36252\t val_loss: 1.85305\t val_acc: 0.37380\t val_recall: 0.37445\t val_f1: 0.36806\n",
      "\tearly stopping at ep.15\t train loss: 1.36782\t val loss: 1.87332\t best val loss: 1.80363\n",
      "CV_2 (29.10s) -\t train_loss: 1.36782\t val_loss: 1.87332\t val_acc: 0.37970\t val_recall: 0.37871\t val_f1: 0.37654\n",
      "\tearly stopping at ep.16\t train loss: 1.44037\t val loss: 1.87343\t best val loss: 1.78689\n",
      "CV_3 (30.48s) -\t train_loss: 1.44037\t val_loss: 1.87343\t val_acc: 0.35970\t val_recall: 0.35897\t val_f1: 0.35252\n",
      "\tearly stopping at ep.16\t train loss: 1.41059\t val loss: 1.85946\t best val loss: 1.79366\n",
      "CV_4 (31.52s) -\t train_loss: 1.41059\t val_loss: 1.85946\t val_acc: 0.37330\t val_recall: 0.37309\t val_f1: 0.36538\n",
      "\tearly stopping at ep.28\t train loss: 1.40067\t val loss: 1.93786\t best val loss: 1.83506\n",
      "CV_5 (53.72s) -\t train_loss: 1.40067\t val_loss: 1.93786\t val_acc: 0.34600\t val_recall: 0.34728\t val_f1: 0.34031\n",
      "## Experiment results (185.30s) - {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.001, 'optimiser': None, 'bn': True, 'batch_size': 8}\n",
      "# Metrics - train loss: 1.39640 \t val loss: 1.87943 \t val accuracy: 0.36650 \t val recall: 0.36650 \t val f1: 0.36056\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.001, 'optimiser': None, 'bn': True, 'batch_size': 4}\n",
      "\tearly stopping at ep.26\t train loss: 1.88341\t val loss: 1.71538\t best val loss: 1.69360\n",
      "CV_1 (101.46s) -\t train_loss: 1.88341\t val_loss: 1.71538\t val_acc: 0.39730\t val_recall: 0.39841\t val_f1: 0.38883\n",
      "\tearly stopping at ep.25\t train loss: 1.88773\t val loss: 1.72332\t best val loss: 1.69668\n",
      "CV_2 (90.43s) -\t train_loss: 1.88773\t val_loss: 1.72332\t val_acc: 0.39890\t val_recall: 0.39780\t val_f1: 0.39008\n",
      "\tearly stopping at ep.21\t train loss: 1.88016\t val loss: 1.75129\t best val loss: 1.71029\n",
      "CV_3 (76.29s) -\t train_loss: 1.88016\t val_loss: 1.75129\t val_acc: 0.37960\t val_recall: 0.37939\t val_f1: 0.37752\n",
      "\tearly stopping at ep.23\t train loss: 1.89378\t val loss: 1.71599\t best val loss: 1.71000\n",
      "CV_4 (83.36s) -\t train_loss: 1.89378\t val_loss: 1.71599\t val_acc: 0.39880\t val_recall: 0.39870\t val_f1: 0.38976\n",
      "\tearly stopping at ep.25\t train loss: 1.87608\t val loss: 1.72632\t best val loss: 1.71195\n",
      "CV_5 (89.69s) -\t train_loss: 1.87608\t val_loss: 1.72632\t val_acc: 0.39420\t val_recall: 0.39437\t val_f1: 0.38458\n",
      "## Experiment results (441.49s) - {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.001, 'optimiser': None, 'bn': True, 'batch_size': 4}\n",
      "# Metrics - train loss: 1.88423 \t val loss: 1.72646 \t val accuracy: 0.39376 \t val recall: 0.39373 \t val f1: 0.38615\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.001, 'optimiser': None, 'bn': True, 'batch_size': 2}\n",
      "\tearly stopping at ep.26\t train loss: 2.30252\t val loss: 2.30356\t best val loss: 2.17064\n",
      "CV_1 (181.44s) -\t train_loss: 2.30252\t val_loss: 2.30356\t val_acc: 0.09530\t val_recall: 0.10000\t val_f1: 0.01740\n",
      "\tearly stopping at ep.13\t train loss: 2.28856\t val loss: 2.26322\t best val loss: 2.22687\n",
      "CV_2 (93.16s) -\t train_loss: 2.28856\t val_loss: 2.26322\t val_acc: 0.15610\t val_recall: 0.15362\t val_f1: 0.10786\n",
      "\tearly stopping at ep.24\t train loss: 2.30258\t val loss: 2.30238\t best val loss: 2.16847\n",
      "CV_3 (170.29s) -\t train_loss: 2.30258\t val_loss: 2.30238\t val_acc: 0.09800\t val_recall: 0.10000\t val_f1: 0.01785\n",
      "\tearly stopping at ep.26\t train loss: 2.29479\t val loss: 2.29318\t best val loss: 2.18127\n",
      "CV_4 (181.66s) -\t train_loss: 2.29479\t val_loss: 2.29318\t val_acc: 0.12380\t val_recall: 0.12318\t val_f1: 0.06606\n",
      "\tearly stopping at ep.26\t train loss: 2.30253\t val loss: 2.30403\t best val loss: 2.18006\n",
      "CV_5 (185.49s) -\t train_loss: 2.30253\t val_loss: 2.30403\t val_acc: 0.09790\t val_recall: 0.10000\t val_f1: 0.01783\n",
      "## Experiment results (812.34s) - {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.001, 'optimiser': None, 'bn': True, 'batch_size': 2}\n",
      "# Metrics - train loss: 2.29820 \t val loss: 2.29327 \t val accuracy: 0.11422 \t val recall: 0.11536 \t val f1: 0.04540\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.001, 'optimiser': 'Adam', 'bn': False, 'batch_size': 16}\n",
      "\tearly stopping at ep.22\t train loss: 1.54703\t val loss: 2.00533\t best val loss: 1.92574\n",
      "CV_1 (15.95s) -\t train_loss: 1.54703\t val_loss: 2.00533\t val_acc: 0.33610\t val_recall: 0.33603\t val_f1: 0.32538\n",
      "\tearly stopping at ep.14\t train loss: 1.43407\t val loss: 2.05378\t best val loss: 1.94769\n",
      "CV_2 (12.72s) -\t train_loss: 1.43407\t val_loss: 2.05378\t val_acc: 0.33660\t val_recall: 0.33484\t val_f1: 0.31685\n",
      "\tearly stopping at ep.30\t train loss: 1.45570\t val loss: 1.99932\t best val loss: 1.92199\n",
      "CV_3 (20.57s) -\t train_loss: 1.45570\t val_loss: 1.99932\t val_acc: 0.35070\t val_recall: 0.34897\t val_f1: 0.33566\n",
      "\tearly stopping at ep.36\t train loss: 1.45909\t val loss: 1.96865\t best val loss: 1.87899\n",
      "CV_4 (25.43s) -\t train_loss: 1.45909\t val_loss: 1.96865\t val_acc: 0.34040\t val_recall: 0.33873\t val_f1: 0.31388\n",
      "\tearly stopping at ep.38\t train loss: 1.44176\t val loss: 2.04502\t best val loss: 1.97204\n",
      "CV_5 (27.39s) -\t train_loss: 1.44176\t val_loss: 2.04502\t val_acc: 0.34550\t val_recall: 0.34490\t val_f1: 0.33419\n",
      "## Experiment results (102.26s) - {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.001, 'optimiser': 'Adam', 'bn': False, 'batch_size': 16}\n",
      "# Metrics - train loss: 1.46753 \t val loss: 2.01442 \t val accuracy: 0.34186 \t val recall: 0.34069 \t val f1: 0.32519\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.001, 'optimiser': 'Adam', 'bn': False, 'batch_size': 8}\n",
      "\tearly stopping at ep.33\t train loss: 1.45264\t val loss: 1.75524\t best val loss: 1.71159\n",
      "CV_1 (40.96s) -\t train_loss: 1.45264\t val_loss: 1.75524\t val_acc: 0.41430\t val_recall: 0.41612\t val_f1: 0.41082\n",
      "\tearly stopping at ep.48\t train loss: 1.43801\t val loss: 1.72160\t best val loss: 1.65153\n",
      "CV_2 (60.66s) -\t train_loss: 1.43801\t val_loss: 1.72160\t val_acc: 0.40960\t val_recall: 0.40941\t val_f1: 0.39571\n",
      "\tearly stopping at ep.23\t train loss: 1.45574\t val loss: 1.76745\t best val loss: 1.71303\n",
      "CV_3 (28.85s) -\t train_loss: 1.45574\t val_loss: 1.76745\t val_acc: 0.38620\t val_recall: 0.38823\t val_f1: 0.37733\n",
      "\tearly stopping at ep.33\t train loss: 1.48513\t val loss: 1.75944\t best val loss: 1.72890\n",
      "CV_4 (41.65s) -\t train_loss: 1.48513\t val_loss: 1.75944\t val_acc: 0.38170\t val_recall: 0.38219\t val_f1: 0.37633\n",
      "\tearly stopping at ep.36\t train loss: 1.46808\t val loss: 1.76210\t best val loss: 1.73201\n",
      "CV_5 (45.09s) -\t train_loss: 1.46808\t val_loss: 1.76210\t val_acc: 0.40820\t val_recall: 0.40825\t val_f1: 0.39681\n",
      "## Experiment results (217.43s) - {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.001, 'optimiser': 'Adam', 'bn': False, 'batch_size': 8}\n",
      "# Metrics - train loss: 1.45992 \t val loss: 1.75317 \t val accuracy: 0.40000 \t val recall: 0.40084 \t val f1: 0.39140\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.001, 'optimiser': 'Adam', 'bn': False, 'batch_size': 4}\n",
      "\tearly stopping at ep.40\t train loss: 1.41704\t val loss: 1.59136\t best val loss: 1.56526\n",
      "CV_1 (94.72s) -\t train_loss: 1.41704\t val_loss: 1.59136\t val_acc: 0.43850\t val_recall: 0.43982\t val_f1: 0.43279\n",
      "\tearly stopping at ep.33\t train loss: 1.40963\t val loss: 1.62225\t best val loss: 1.57550\n",
      "CV_2 (80.56s) -\t train_loss: 1.40963\t val_loss: 1.62225\t val_acc: 0.43010\t val_recall: 0.42867\t val_f1: 0.40976\n",
      "\tearly stopping at ep.46\t train loss: 1.42468\t val loss: 1.59877\t best val loss: 1.55895\n",
      "CV_3 (109.17s) -\t train_loss: 1.42468\t val_loss: 1.59877\t val_acc: 0.44550\t val_recall: 0.44541\t val_f1: 0.43824\n",
      "\tearly stopping at ep.22\t train loss: 1.41978\t val loss: 1.61667\t best val loss: 1.58093\n",
      "CV_4 (52.62s) -\t train_loss: 1.41978\t val_loss: 1.61667\t val_acc: 0.43720\t val_recall: 0.43710\t val_f1: 0.43858\n",
      "\tearly stopping at ep.35\t train loss: 1.40998\t val loss: 1.65730\t best val loss: 1.54811\n",
      "CV_5 (85.30s) -\t train_loss: 1.40998\t val_loss: 1.65730\t val_acc: 0.44310\t val_recall: 0.44387\t val_f1: 0.42899\n",
      "## Experiment results (422.56s) - {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.001, 'optimiser': 'Adam', 'bn': False, 'batch_size': 4}\n",
      "# Metrics - train loss: 1.41622 \t val loss: 1.61727 \t val accuracy: 0.43888 \t val recall: 0.43897 \t val f1: 0.42967\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.001, 'optimiser': 'Adam', 'bn': False, 'batch_size': 2}\n",
      "\tearly stopping at ep.25\t train loss: 1.38701\t val loss: 1.50587\t best val loss: 1.49426\n",
      "CV_1 (116.24s) -\t train_loss: 1.38701\t val_loss: 1.50587\t val_acc: 0.46630\t val_recall: 0.46755\t val_f1: 0.45899\n",
      "\tearly stopping at ep.31\t train loss: 1.37993\t val loss: 1.50484\t best val loss: 1.49409\n",
      "CV_2 (144.80s) -\t train_loss: 1.37993\t val_loss: 1.50484\t val_acc: 0.47840\t val_recall: 0.47781\t val_f1: 0.47196\n",
      "\tearly stopping at ep.32\t train loss: 1.36759\t val loss: 1.49585\t best val loss: 1.48084\n",
      "CV_3 (148.86s) -\t train_loss: 1.36759\t val_loss: 1.49585\t val_acc: 0.47790\t val_recall: 0.47751\t val_f1: 0.47323\n",
      "\tearly stopping at ep.40\t train loss: 1.37747\t val loss: 1.47258\t best val loss: 1.46269\n",
      "CV_4 (185.75s) -\t train_loss: 1.37747\t val_loss: 1.47258\t val_acc: 0.48120\t val_recall: 0.48152\t val_f1: 0.47850\n",
      "\tearly stopping at ep.20\t train loss: 1.38225\t val loss: 1.50154\t best val loss: 1.48920\n",
      "CV_5 (92.44s) -\t train_loss: 1.38225\t val_loss: 1.50154\t val_acc: 0.47360\t val_recall: 0.47334\t val_f1: 0.46659\n",
      "## Experiment results (688.31s) - {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.001, 'optimiser': 'Adam', 'bn': False, 'batch_size': 2}\n",
      "# Metrics - train loss: 1.37885 \t val loss: 1.49613 \t val accuracy: 0.47548 \t val recall: 0.47554 \t val f1: 0.46985\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.001, 'optimiser': 'Adam', 'bn': True, 'batch_size': 16}\n",
      "\tearly stopping at ep.31\t train loss: 1.36803\t val loss: 1.59583\t best val loss: 1.54915\n",
      "CV_1 (41.16s) -\t train_loss: 1.36803\t val_loss: 1.59583\t val_acc: 0.43850\t val_recall: 0.44018\t val_f1: 0.42590\n",
      "\tearly stopping at ep.45\t train loss: 1.42214\t val loss: 1.57983\t best val loss: 1.53681\n",
      "CV_2 (59.94s) -\t train_loss: 1.42214\t val_loss: 1.57983\t val_acc: 0.43740\t val_recall: 0.43592\t val_f1: 0.43225\n",
      "\tearly stopping at ep.29\t train loss: 1.43298\t val loss: 1.60723\t best val loss: 1.55981\n",
      "CV_3 (38.58s) -\t train_loss: 1.43298\t val_loss: 1.60723\t val_acc: 0.43000\t val_recall: 0.43068\t val_f1: 0.42568\n",
      "\tearly stopping at ep.29\t train loss: 1.43666\t val loss: 1.57140\t best val loss: 1.55941\n",
      "CV_4 (39.52s) -\t train_loss: 1.43666\t val_loss: 1.57140\t val_acc: 0.44520\t val_recall: 0.44481\t val_f1: 0.44145\n",
      "\tearly stopping at ep.42\t train loss: 1.41140\t val loss: 1.54820\t best val loss: 1.54305\n",
      "CV_5 (56.65s) -\t train_loss: 1.41140\t val_loss: 1.54820\t val_acc: 0.44430\t val_recall: 0.44456\t val_f1: 0.43745\n",
      "## Experiment results (236.17s) - {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.001, 'optimiser': 'Adam', 'bn': True, 'batch_size': 16}\n",
      "# Metrics - train loss: 1.41424 \t val loss: 1.58050 \t val accuracy: 0.43908 \t val recall: 0.43923 \t val f1: 0.43254\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.001, 'optimiser': 'Adam', 'bn': True, 'batch_size': 8}\n",
      "\tearly stopping at ep.24\t train loss: 1.64052\t val loss: 1.52839\t best val loss: 1.51619\n",
      "CV_1 (59.05s) -\t train_loss: 1.64052\t val_loss: 1.52839\t val_acc: 0.45550\t val_recall: 0.45679\t val_f1: 0.45092\n",
      "\tearly stopping at ep.32\t train loss: 1.60485\t val loss: 1.51209\t best val loss: 1.51144\n",
      "CV_2 (78.02s) -\t train_loss: 1.60485\t val_loss: 1.51209\t val_acc: 0.46430\t val_recall: 0.46256\t val_f1: 0.45904\n",
      "\tearly stopping at ep.36\t train loss: 1.59317\t val loss: 1.52857\t best val loss: 1.51648\n",
      "CV_3 (88.66s) -\t train_loss: 1.59317\t val_loss: 1.52857\t val_acc: 0.45390\t val_recall: 0.45456\t val_f1: 0.44864\n",
      "\tearly stopping at ep.34\t train loss: 1.58172\t val loss: 1.53177\t best val loss: 1.50147\n",
      "CV_4 (83.05s) -\t train_loss: 1.58172\t val_loss: 1.53177\t val_acc: 0.45410\t val_recall: 0.45471\t val_f1: 0.45008\n",
      "\tearly stopping at ep.46\t train loss: 1.59368\t val loss: 1.51831\t best val loss: 1.50799\n",
      "CV_5 (112.18s) -\t train_loss: 1.59368\t val_loss: 1.51831\t val_acc: 0.46290\t val_recall: 0.46260\t val_f1: 0.45819\n",
      "## Experiment results (421.26s) - {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.001, 'optimiser': 'Adam', 'bn': True, 'batch_size': 8}\n",
      "# Metrics - train loss: 1.60279 \t val loss: 1.52382 \t val accuracy: 0.45814 \t val recall: 0.45824 \t val f1: 0.45337\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.001, 'optimiser': 'Adam', 'bn': True, 'batch_size': 4}\n",
      "\tearly stopping at ep.47\t train loss: 1.88131\t val loss: 1.63325\t best val loss: 1.62577\n",
      "CV_1 (222.58s) -\t train_loss: 1.88131\t val_loss: 1.63325\t val_acc: 0.42660\t val_recall: 0.42806\t val_f1: 0.42036\n",
      "\tearly stopping at ep.47\t train loss: 1.88779\t val loss: 1.64337\t best val loss: 1.61426\n",
      "CV_2 (222.46s) -\t train_loss: 1.88779\t val_loss: 1.64337\t val_acc: 0.43950\t val_recall: 0.43836\t val_f1: 0.43193\n",
      "\tearly stopping at ep.37\t train loss: 1.88397\t val loss: 1.64744\t best val loss: 1.63644\n",
      "CV_3 (174.93s) -\t train_loss: 1.88397\t val_loss: 1.64744\t val_acc: 0.42390\t val_recall: 0.42418\t val_f1: 0.41920\n",
      "\tearly stopping at ep.43\t train loss: 1.88271\t val loss: 1.62410\t best val loss: 1.61783\n",
      "CV_4 (204.23s) -\t train_loss: 1.88271\t val_loss: 1.62410\t val_acc: 0.44130\t val_recall: 0.44203\t val_f1: 0.43195\n",
      "\tearly stopping at ep.39\t train loss: 1.88040\t val loss: 1.64508\t best val loss: 1.62449\n",
      "CV_5 (182.02s) -\t train_loss: 1.88040\t val_loss: 1.64508\t val_acc: 0.42950\t val_recall: 0.42891\t val_f1: 0.41612\n",
      "## Experiment results (1006.49s) - {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.001, 'optimiser': 'Adam', 'bn': True, 'batch_size': 4}\n",
      "# Metrics - train loss: 1.88324 \t val loss: 1.63865 \t val accuracy: 0.43216 \t val recall: 0.43231 \t val f1: 0.42391\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.001, 'optimiser': 'Adam', 'bn': True, 'batch_size': 2}\n",
      "\tearly stopping at ep.14\t train loss: 2.27600\t val loss: 2.22754\t best val loss: 2.19373\n",
      "CV_1 (128.73s) -\t train_loss: 2.27600\t val_loss: 2.22754\t val_acc: 0.18780\t val_recall: 0.18668\t val_f1: 0.12762\n",
      "\tearly stopping at ep.18\t train loss: 2.26362\t val loss: 2.21390\t best val loss: 2.16833\n",
      "CV_2 (166.43s) -\t train_loss: 2.26362\t val_loss: 2.21390\t val_acc: 0.18340\t val_recall: 0.18284\t val_f1: 0.14929\n",
      "\tearly stopping at ep.15\t train loss: 2.27437\t val loss: 2.21510\t best val loss: 2.18295\n",
      "CV_3 (136.66s) -\t train_loss: 2.27437\t val_loss: 2.21510\t val_acc: 0.18330\t val_recall: 0.18074\t val_f1: 0.12182\n",
      "\tearly stopping at ep.14\t train loss: 2.26603\t val loss: 2.18982\t best val loss: 2.18790\n",
      "CV_4 (128.73s) -\t train_loss: 2.26603\t val_loss: 2.18982\t val_acc: 0.19380\t val_recall: 0.19496\t val_f1: 0.16550\n",
      "\tearly stopping at ep.14\t train loss: 2.26124\t val loss: 2.18253\t best val loss: 2.16725\n",
      "CV_5 (127.85s) -\t train_loss: 2.26124\t val_loss: 2.18253\t val_acc: 0.20380\t val_recall: 0.20553\t val_f1: 0.16725\n",
      "## Experiment results (688.72s) - {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.001, 'optimiser': 'Adam', 'bn': True, 'batch_size': 2}\n",
      "# Metrics - train loss: 2.26825 \t val loss: 2.20578 \t val accuracy: 0.19042 \t val recall: 0.19015 \t val f1: 0.14630\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.001, 'optimiser': 'Momentum', 'bn': False, 'batch_size': 16}\n",
      "\tearly stopping at ep.12\t train loss: 2.27311\t val loss: 2.32512\t best val loss: 2.30510\n",
      "CV_1 (5.70s) -\t train_loss: 2.27311\t val_loss: 2.32512\t val_acc: 0.10320\t val_recall: 0.10268\t val_f1: 0.02404\n",
      "\tearly stopping at ep.17\t train loss: 2.25499\t val loss: 2.36690\t best val loss: 2.31376\n",
      "CV_2 (10.43s) -\t train_loss: 2.25499\t val_loss: 2.36690\t val_acc: 0.11520\t val_recall: 0.11647\t val_f1: 0.04704\n",
      "\tearly stopping at ep.19\t train loss: 2.26428\t val loss: 2.32747\t best val loss: 2.31022\n",
      "CV_3 (10.31s) -\t train_loss: 2.26428\t val_loss: 2.32747\t val_acc: 0.11240\t val_recall: 0.11464\t val_f1: 0.04680\n",
      "\tearly stopping at ep.22\t train loss: 2.27525\t val loss: 2.32314\t best val loss: 2.30342\n",
      "CV_4 (10.43s) -\t train_loss: 2.27525\t val_loss: 2.32314\t val_acc: 0.11260\t val_recall: 0.11473\t val_f1: 0.04564\n",
      "\tearly stopping at ep.29\t train loss: 2.16325\t val loss: 2.31200\t best val loss: 2.23632\n",
      "CV_5 (14.09s) -\t train_loss: 2.16325\t val_loss: 2.31200\t val_acc: 0.15470\t val_recall: 0.15451\t val_f1: 0.08904\n",
      "## Experiment results (51.16s) - {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.001, 'optimiser': 'Momentum', 'bn': False, 'batch_size': 16}\n",
      "# Metrics - train loss: 2.24618 \t val loss: 2.33093 \t val accuracy: 0.11962 \t val recall: 0.12061 \t val f1: 0.05051\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.001, 'optimiser': 'Momentum', 'bn': False, 'batch_size': 8}\n",
      "\tearly stopping at ep.14\t train loss: 2.10868\t val loss: 2.23784\t best val loss: 2.17145\n",
      "CV_1 (11.42s) -\t train_loss: 2.10868\t val_loss: 2.23784\t val_acc: 0.16690\t val_recall: 0.16433\t val_f1: 0.09465\n",
      "\tearly stopping at ep.23\t train loss: 2.11754\t val loss: 2.17854\t best val loss: 2.16708\n",
      "CV_2 (19.29s) -\t train_loss: 2.11754\t val_loss: 2.17854\t val_acc: 0.18540\t val_recall: 0.18401\t val_f1: 0.12268\n",
      "\tearly stopping at ep.22\t train loss: 2.11407\t val loss: 2.21845\t best val loss: 2.17015\n",
      "CV_3 (17.72s) -\t train_loss: 2.11407\t val_loss: 2.21845\t val_acc: 0.17780\t val_recall: 0.17913\t val_f1: 0.10685\n",
      "\tearly stopping at ep.30\t train loss: 2.10576\t val loss: 2.25586\t best val loss: 2.14332\n",
      "CV_4 (24.34s) -\t train_loss: 2.10576\t val_loss: 2.25586\t val_acc: 0.16750\t val_recall: 0.17017\t val_f1: 0.10448\n",
      "\tearly stopping at ep.36\t train loss: 2.06554\t val loss: 2.22383\t best val loss: 2.13606\n",
      "CV_5 (30.02s) -\t train_loss: 2.06554\t val_loss: 2.22383\t val_acc: 0.17520\t val_recall: 0.17376\t val_f1: 0.10778\n",
      "## Experiment results (103.03s) - {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.001, 'optimiser': 'Momentum', 'bn': False, 'batch_size': 8}\n",
      "# Metrics - train loss: 2.10232 \t val loss: 2.22290 \t val accuracy: 0.17456 \t val recall: 0.17428 \t val f1: 0.10729\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.001, 'optimiser': 'Momentum', 'bn': False, 'batch_size': 4}\n",
      "\tearly stopping at ep.15\t train loss: 2.04359\t val loss: 2.08644\t best val loss: 2.06287\n",
      "CV_1 (22.57s) -\t train_loss: 2.04359\t val_loss: 2.08644\t val_acc: 0.19770\t val_recall: 0.19616\t val_f1: 0.14241\n",
      "\tearly stopping at ep.17\t train loss: 2.04485\t val loss: 2.17341\t best val loss: 2.07054\n",
      "CV_2 (25.31s) -\t train_loss: 2.04485\t val_loss: 2.17341\t val_acc: 0.17820\t val_recall: 0.18131\t val_f1: 0.11865\n",
      "\tearly stopping at ep.19\t train loss: 2.01103\t val loss: 2.13258\t best val loss: 2.06718\n",
      "CV_3 (30.84s) -\t train_loss: 2.01103\t val_loss: 2.13258\t val_acc: 0.20940\t val_recall: 0.21238\t val_f1: 0.16193\n",
      "\tearly stopping at ep.13\t train loss: 2.03634\t val loss: 2.09380\t best val loss: 2.09327\n",
      "CV_4 (20.40s) -\t train_loss: 2.03634\t val_loss: 2.09380\t val_acc: 0.22090\t val_recall: 0.22231\t val_f1: 0.18079\n",
      "\tearly stopping at ep.23\t train loss: 2.05113\t val loss: 2.06841\t best val loss: 2.04152\n",
      "CV_5 (34.29s) -\t train_loss: 2.05113\t val_loss: 2.06841\t val_acc: 0.22900\t val_recall: 0.22846\t val_f1: 0.18426\n",
      "## Experiment results (133.68s) - {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.001, 'optimiser': 'Momentum', 'bn': False, 'batch_size': 4}\n",
      "# Metrics - train loss: 2.03739 \t val loss: 2.11093 \t val accuracy: 0.20704 \t val recall: 0.20813 \t val f1: 0.15761\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.001, 'optimiser': 'Momentum', 'bn': False, 'batch_size': 2}\n",
      "\tearly stopping at ep.48\t train loss: 1.70711\t val loss: 1.82731\t best val loss: 1.72895\n",
      "CV_1 (141.05s) -\t train_loss: 1.70711\t val_loss: 1.82731\t val_acc: 0.36930\t val_recall: 0.36784\t val_f1: 0.34750\n",
      "\tearly stopping at ep.16\t train loss: 1.73530\t val loss: 1.79522\t best val loss: 1.75685\n",
      "CV_2 (48.29s) -\t train_loss: 1.73530\t val_loss: 1.79522\t val_acc: 0.37400\t val_recall: 0.37441\t val_f1: 0.35569\n",
      "\tearly stopping at ep.19\t train loss: 1.72785\t val loss: 1.82640\t best val loss: 1.72985\n",
      "CV_3 (54.58s) -\t train_loss: 1.72785\t val_loss: 1.82640\t val_acc: 0.36680\t val_recall: 0.36791\t val_f1: 0.35892\n",
      "\tearly stopping at ep.28\t train loss: 1.70246\t val loss: 1.81210\t best val loss: 1.73881\n",
      "CV_4 (83.19s) -\t train_loss: 1.70246\t val_loss: 1.81210\t val_acc: 0.33730\t val_recall: 0.33722\t val_f1: 0.32389\n",
      "\tearly stopping at ep.43\t train loss: 1.70678\t val loss: 1.86733\t best val loss: 1.71732\n",
      "CV_5 (128.14s) -\t train_loss: 1.70678\t val_loss: 1.86733\t val_acc: 0.34810\t val_recall: 0.34962\t val_f1: 0.32316\n",
      "## Experiment results (455.46s) - {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.001, 'optimiser': 'Momentum', 'bn': False, 'batch_size': 2}\n",
      "# Metrics - train loss: 1.71590 \t val loss: 1.82567 \t val accuracy: 0.35910 \t val recall: 0.35940 \t val f1: 0.34183\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.001, 'optimiser': 'Momentum', 'bn': True, 'batch_size': 16}\n",
      "\tearly stopping at ep.43\t train loss: 1.48749\t val loss: 1.65332\t best val loss: 1.57199\n",
      "CV_1 (47.59s) -\t train_loss: 1.48749\t val_loss: 1.65332\t val_acc: 0.40480\t val_recall: 0.40586\t val_f1: 0.40262\n",
      "\tearly stopping at ep.38\t train loss: 1.53614\t val loss: 1.64571\t best val loss: 1.56846\n",
      "CV_2 (41.98s) -\t train_loss: 1.53614\t val_loss: 1.64571\t val_acc: 0.41960\t val_recall: 0.41895\t val_f1: 0.41412\n",
      "\tearly stopping at ep.31\t train loss: 1.49747\t val loss: 1.59801\t best val loss: 1.58562\n",
      "CV_3 (35.51s) -\t train_loss: 1.49747\t val_loss: 1.59801\t val_acc: 0.42680\t val_recall: 0.42753\t val_f1: 0.42447\n",
      "\tearly stopping at ep.33\t train loss: 1.48924\t val loss: 1.61118\t best val loss: 1.57995\n",
      "CV_4 (36.86s) -\t train_loss: 1.48924\t val_loss: 1.61118\t val_acc: 0.42810\t val_recall: 0.42772\t val_f1: 0.42739\n",
      "\tearly stopping at ep.37\t train loss: 1.47484\t val loss: 1.61917\t best val loss: 1.57943\n",
      "CV_5 (41.30s) -\t train_loss: 1.47484\t val_loss: 1.61917\t val_acc: 0.43150\t val_recall: 0.43109\t val_f1: 0.42372\n",
      "## Experiment results (203.58s) - {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.001, 'optimiser': 'Momentum', 'bn': True, 'batch_size': 16}\n",
      "# Metrics - train loss: 1.49704 \t val loss: 1.62548 \t val accuracy: 0.42216 \t val recall: 0.42223 \t val f1: 0.41847\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.001, 'optimiser': 'Momentum', 'bn': True, 'batch_size': 8}\n",
      "\tearly stopping at ep.47\t train loss: 1.66796\t val loss: 1.56617\t best val loss: 1.52769\n",
      "CV_1 (95.39s) -\t train_loss: 1.66796\t val_loss: 1.56617\t val_acc: 0.43600\t val_recall: 0.43754\t val_f1: 0.42884\n",
      "\tearly stopping at ep.29\t train loss: 1.67712\t val loss: 1.60173\t best val loss: 1.57170\n",
      "CV_2 (58.32s) -\t train_loss: 1.67712\t val_loss: 1.60173\t val_acc: 0.44060\t val_recall: 0.43876\t val_f1: 0.43395\n",
      "CV_3 (100.92s) -\t train_loss: 1.62967\t val_loss: 1.56462\t val_acc: 0.44650\t val_recall: 0.44708\t val_f1: 0.44079\n",
      "\tearly stopping at ep.43\t train loss: 1.66368\t val loss: 1.58260\t best val loss: 1.54058\n",
      "CV_4 (85.84s) -\t train_loss: 1.66368\t val_loss: 1.58260\t val_acc: 0.43990\t val_recall: 0.43885\t val_f1: 0.42786\n",
      "\tearly stopping at ep.45\t train loss: 1.64753\t val loss: 1.61870\t best val loss: 1.55862\n",
      "CV_5 (92.11s) -\t train_loss: 1.64753\t val_loss: 1.61870\t val_acc: 0.42350\t val_recall: 0.42367\t val_f1: 0.41905\n",
      "## Experiment results (432.94s) - {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.001, 'optimiser': 'Momentum', 'bn': True, 'batch_size': 8}\n",
      "# Metrics - train loss: 1.65719 \t val loss: 1.58676 \t val accuracy: 0.43730 \t val recall: 0.43718 \t val f1: 0.43010\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.001, 'optimiser': 'Momentum', 'bn': True, 'batch_size': 4}\n",
      "\tearly stopping at ep.36\t train loss: 1.93524\t val loss: 1.69421\t best val loss: 1.69007\n",
      "CV_1 (136.96s) -\t train_loss: 1.93524\t val_loss: 1.69421\t val_acc: 0.40770\t val_recall: 0.40896\t val_f1: 0.39417\n",
      "\tearly stopping at ep.30\t train loss: 1.93053\t val loss: 1.70799\t best val loss: 1.69067\n",
      "CV_2 (113.53s) -\t train_loss: 1.93053\t val_loss: 1.70799\t val_acc: 0.41230\t val_recall: 0.41031\t val_f1: 0.40185\n",
      "\tearly stopping at ep.26\t train loss: 1.95858\t val loss: 1.73023\t best val loss: 1.70632\n",
      "CV_3 (97.06s) -\t train_loss: 1.95858\t val_loss: 1.73023\t val_acc: 0.39190\t val_recall: 0.39208\t val_f1: 0.38490\n",
      "\tearly stopping at ep.37\t train loss: 1.92854\t val loss: 1.72827\t best val loss: 1.68558\n",
      "CV_4 (138.28s) -\t train_loss: 1.92854\t val_loss: 1.72827\t val_acc: 0.39930\t val_recall: 0.39909\t val_f1: 0.38677\n",
      "\tearly stopping at ep.33\t train loss: 1.92728\t val loss: 1.70769\t best val loss: 1.70037\n",
      "CV_5 (123.12s) -\t train_loss: 1.92728\t val_loss: 1.70769\t val_acc: 0.40660\t val_recall: 0.40592\t val_f1: 0.39786\n",
      "## Experiment results (609.25s) - {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.001, 'optimiser': 'Momentum', 'bn': True, 'batch_size': 4}\n",
      "# Metrics - train loss: 1.93603 \t val loss: 1.71368 \t val accuracy: 0.40356 \t val recall: 0.40327 \t val f1: 0.39311\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.001, 'optimiser': 'Momentum', 'bn': True, 'batch_size': 2}\n",
      "\tearly stopping at ep.23\t train loss: 2.26672\t val loss: 2.21582\t best val loss: 2.21014\n",
      "CV_1 (166.04s) -\t train_loss: 2.26672\t val_loss: 2.21582\t val_acc: 0.19320\t val_recall: 0.19290\t val_f1: 0.13898\n",
      "\tearly stopping at ep.13\t train loss: 2.28395\t val loss: 2.26641\t best val loss: 2.23529\n",
      "CV_2 (95.25s) -\t train_loss: 2.28395\t val_loss: 2.26641\t val_acc: 0.14700\t val_recall: 0.14848\t val_f1: 0.10393\n",
      "\tearly stopping at ep.20\t train loss: 2.27954\t val loss: 2.23177\t best val loss: 2.21597\n",
      "CV_3 (146.47s) -\t train_loss: 2.27954\t val_loss: 2.23177\t val_acc: 0.17960\t val_recall: 0.17801\t val_f1: 0.10359\n",
      "\tearly stopping at ep.19\t train loss: 2.27670\t val loss: 2.25814\t best val loss: 2.19163\n",
      "CV_4 (138.40s) -\t train_loss: 2.27670\t val_loss: 2.25814\t val_acc: 0.14220\t val_recall: 0.14274\t val_f1: 0.11167\n",
      "\tearly stopping at ep.34\t train loss: 2.30267\t val loss: 2.30284\t best val loss: 2.18807\n",
      "CV_5 (247.02s) -\t train_loss: 2.30267\t val_loss: 2.30284\t val_acc: 0.09860\t val_recall: 0.10000\t val_f1: 0.01795\n",
      "## Experiment results (793.45s) - {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.001, 'optimiser': 'Momentum', 'bn': True, 'batch_size': 2}\n",
      "# Metrics - train loss: 2.28192 \t val loss: 2.25500 \t val accuracy: 0.15212 \t val recall: 0.15243 \t val f1: 0.09522\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.0001, 'optimiser': None, 'bn': False, 'batch_size': 16}\n",
      "CV_1 (21.73s) -\t train_loss: 1.29093\t val_loss: 1.52704\t val_acc: 0.45480\t val_recall: 0.45577\t val_f1: 0.44485\n",
      "CV_2 (21.41s) -\t train_loss: 1.24396\t val_loss: 1.53848\t val_acc: 0.46100\t val_recall: 0.45999\t val_f1: 0.45678\n",
      "CV_3 (22.38s) -\t train_loss: 1.23328\t val_loss: 1.55957\t val_acc: 0.44720\t val_recall: 0.44745\t val_f1: 0.43693\n",
      "\tearly stopping at ep.44\t train loss: 1.25108\t val loss: 1.53949\t best val loss: 1.53574\n",
      "CV_4 (18.97s) -\t train_loss: 1.25108\t val_loss: 1.53949\t val_acc: 0.45170\t val_recall: 0.45049\t val_f1: 0.44055\n",
      "CV_5 (21.26s) -\t train_loss: 1.22754\t val_loss: 1.53429\t val_acc: 0.46420\t val_recall: 0.46458\t val_f1: 0.45850\n",
      "## Experiment results (105.96s) - {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.0001, 'optimiser': None, 'bn': False, 'batch_size': 16}\n",
      "# Metrics - train loss: 1.24936 \t val loss: 1.53977 \t val accuracy: 0.45578 \t val recall: 0.45566 \t val f1: 0.44752\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.0001, 'optimiser': None, 'bn': False, 'batch_size': 8}\n",
      "CV_1 (35.92s) -\t train_loss: 1.43040\t val_loss: 1.53882\t val_acc: 0.44650\t val_recall: 0.44765\t val_f1: 0.44109\n",
      "CV_2 (37.15s) -\t train_loss: 1.39875\t val_loss: 1.51404\t val_acc: 0.46590\t val_recall: 0.46482\t val_f1: 0.45736\n",
      "CV_3 (36.33s) -\t train_loss: 1.41331\t val_loss: 1.51985\t val_acc: 0.46530\t val_recall: 0.46587\t val_f1: 0.45978\n",
      "CV_4 (36.70s) -\t train_loss: 1.42163\t val_loss: 1.51559\t val_acc: 0.46040\t val_recall: 0.46015\t val_f1: 0.45215\n",
      "CV_5 (36.92s) -\t train_loss: 1.41940\t val_loss: 1.52053\t val_acc: 0.46170\t val_recall: 0.46111\t val_f1: 0.45523\n",
      "## Experiment results (183.30s) - {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.0001, 'optimiser': None, 'bn': False, 'batch_size': 8}\n",
      "# Metrics - train loss: 1.41670 \t val loss: 1.52177 \t val accuracy: 0.45996 \t val recall: 0.45992 \t val f1: 0.45312\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.0001, 'optimiser': None, 'bn': False, 'batch_size': 4}\n",
      "CV_1 (65.19s) -\t train_loss: 1.63575\t val_loss: 1.65224\t val_acc: 0.41420\t val_recall: 0.41523\t val_f1: 0.40482\n",
      "CV_2 (70.63s) -\t train_loss: 1.63171\t val_loss: 1.66158\t val_acc: 0.41070\t val_recall: 0.40934\t val_f1: 0.39927\n",
      "CV_3 (70.33s) -\t train_loss: 1.64584\t val_loss: 1.66292\t val_acc: 0.41250\t val_recall: 0.41317\t val_f1: 0.40597\n",
      "CV_4 (70.34s) -\t train_loss: 1.61798\t val_loss: 1.64765\t val_acc: 0.41870\t val_recall: 0.41843\t val_f1: 0.40866\n",
      "CV_5 (70.37s) -\t train_loss: 1.63692\t val_loss: 1.66596\t val_acc: 0.41120\t val_recall: 0.41047\t val_f1: 0.39445\n",
      "## Experiment results (347.13s) - {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.0001, 'optimiser': None, 'bn': False, 'batch_size': 4}\n",
      "# Metrics - train loss: 1.63364 \t val loss: 1.65807 \t val accuracy: 0.41346 \t val recall: 0.41333 \t val f1: 0.40263\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.0001, 'optimiser': None, 'bn': False, 'batch_size': 2}\n",
      "CV_1 (133.75s) -\t train_loss: 1.85860\t val_loss: 1.85878\t val_acc: 0.33420\t val_recall: 0.33581\t val_f1: 0.30742\n",
      "\tearly stopping at ep.38\t train loss: 1.85307\t val loss: 1.86650\t best val loss: 1.86607\n",
      "CV_2 (102.39s) -\t train_loss: 1.85307\t val_loss: 1.86650\t val_acc: 0.34230\t val_recall: 0.34114\t val_f1: 0.31941\n",
      "CV_3 (134.92s) -\t train_loss: 1.86230\t val_loss: 1.86584\t val_acc: 0.34050\t val_recall: 0.33957\t val_f1: 0.31421\n",
      "\tearly stopping at ep.33\t train loss: 1.86344\t val loss: 1.86978\t best val loss: 1.86578\n",
      "CV_4 (89.37s) -\t train_loss: 1.86344\t val_loss: 1.86978\t val_acc: 0.32460\t val_recall: 0.32601\t val_f1: 0.29521\n",
      "\tearly stopping at ep.45\t train loss: 1.86485\t val loss: 1.87174\t best val loss: 1.86796\n",
      "CV_5 (121.78s) -\t train_loss: 1.86485\t val_loss: 1.87174\t val_acc: 0.33260\t val_recall: 0.33223\t val_f1: 0.30837\n",
      "## Experiment results (582.43s) - {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.0001, 'optimiser': None, 'bn': False, 'batch_size': 2}\n",
      "# Metrics - train loss: 1.86045 \t val loss: 1.86653 \t val accuracy: 0.33484 \t val recall: 0.33495 \t val f1: 0.30892\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.0001, 'optimiser': None, 'bn': True, 'batch_size': 16}\n",
      "\tearly stopping at ep.24\t train loss: 1.28192\t val loss: 1.80183\t best val loss: 1.67097\n",
      "CV_1 (25.50s) -\t train_loss: 1.28192\t val_loss: 1.80183\t val_acc: 0.39130\t val_recall: 0.39274\t val_f1: 0.38700\n",
      "\tearly stopping at ep.23\t train loss: 1.33351\t val loss: 1.71625\t best val loss: 1.66819\n",
      "CV_2 (24.46s) -\t train_loss: 1.33351\t val_loss: 1.71625\t val_acc: 0.40230\t val_recall: 0.40139\t val_f1: 0.39996\n",
      "\tearly stopping at ep.24\t train loss: 1.33569\t val loss: 1.74031\t best val loss: 1.67479\n",
      "CV_3 (25.44s) -\t train_loss: 1.33569\t val_loss: 1.74031\t val_acc: 0.39240\t val_recall: 0.39293\t val_f1: 0.38967\n",
      "\tearly stopping at ep.23\t train loss: 1.35584\t val loss: 1.77027\t best val loss: 1.66780\n",
      "CV_4 (24.36s) -\t train_loss: 1.35584\t val_loss: 1.77027\t val_acc: 0.38640\t val_recall: 0.38601\t val_f1: 0.38360\n",
      "\tearly stopping at ep.26\t train loss: 1.28113\t val loss: 1.87089\t best val loss: 1.65861\n",
      "CV_5 (27.42s) -\t train_loss: 1.28113\t val_loss: 1.87089\t val_acc: 0.36730\t val_recall: 0.36700\t val_f1: 0.36559\n",
      "## Experiment results (127.54s) - {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.0001, 'optimiser': None, 'bn': True, 'batch_size': 16}\n",
      "# Metrics - train loss: 1.31762 \t val loss: 1.77991 \t val accuracy: 0.38794 \t val recall: 0.38801 \t val f1: 0.38516\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.0001, 'optimiser': None, 'bn': True, 'batch_size': 8}\n",
      "\tearly stopping at ep.30\t train loss: 1.47927\t val loss: 1.70085\t best val loss: 1.64500\n",
      "CV_1 (56.93s) -\t train_loss: 1.47927\t val_loss: 1.70085\t val_acc: 0.40900\t val_recall: 0.41049\t val_f1: 0.41013\n",
      "\tearly stopping at ep.29\t train loss: 1.47807\t val loss: 1.71870\t best val loss: 1.63938\n",
      "CV_2 (55.70s) -\t train_loss: 1.47807\t val_loss: 1.71870\t val_acc: 0.39560\t val_recall: 0.39456\t val_f1: 0.39231\n",
      "\tearly stopping at ep.25\t train loss: 1.57022\t val loss: 1.70346\t best val loss: 1.67232\n",
      "CV_3 (47.34s) -\t train_loss: 1.57022\t val_loss: 1.70346\t val_acc: 0.40000\t val_recall: 0.40000\t val_f1: 0.39580\n",
      "\tearly stopping at ep.29\t train loss: 1.49819\t val loss: 1.75991\t best val loss: 1.64063\n",
      "CV_4 (55.05s) -\t train_loss: 1.49819\t val_loss: 1.75991\t val_acc: 0.39140\t val_recall: 0.39115\t val_f1: 0.38808\n",
      "\tearly stopping at ep.30\t train loss: 1.48769\t val loss: 1.76435\t best val loss: 1.65172\n",
      "CV_5 (57.35s) -\t train_loss: 1.48769\t val_loss: 1.76435\t val_acc: 0.38580\t val_recall: 0.38571\t val_f1: 0.38701\n",
      "## Experiment results (272.72s) - {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.0001, 'optimiser': None, 'bn': True, 'batch_size': 8}\n",
      "# Metrics - train loss: 1.50269 \t val loss: 1.72945 \t val accuracy: 0.39636 \t val recall: 0.39638 \t val f1: 0.39466\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.0001, 'optimiser': None, 'bn': True, 'batch_size': 4}\n",
      "CV_1 (178.36s) -\t train_loss: 1.87348\t val_loss: 1.72922\t val_acc: 0.41570\t val_recall: 0.41688\t val_f1: 0.40904\n",
      "\tearly stopping at ep.49\t train loss: 1.87848\t val loss: 1.74752\t best val loss: 1.72637\n",
      "CV_2 (174.69s) -\t train_loss: 1.87848\t val_loss: 1.74752\t val_acc: 0.40430\t val_recall: 0.40287\t val_f1: 0.39420\n",
      "CV_3 (178.50s) -\t train_loss: 1.87020\t val_loss: 1.73532\t val_acc: 0.40090\t val_recall: 0.40042\t val_f1: 0.39439\n",
      "\tearly stopping at ep.41\t train loss: 1.87988\t val loss: 1.73686\t best val loss: 1.71522\n",
      "CV_4 (145.90s) -\t train_loss: 1.87988\t val_loss: 1.73686\t val_acc: 0.41010\t val_recall: 0.41025\t val_f1: 0.39996\n",
      "\tearly stopping at ep.50\t train loss: 1.88201\t val loss: 1.74509\t best val loss: 1.72207\n",
      "CV_5 (177.58s) -\t train_loss: 1.88201\t val_loss: 1.74509\t val_acc: 0.40480\t val_recall: 0.40500\t val_f1: 0.39954\n",
      "## Experiment results (855.41s) - {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.0001, 'optimiser': None, 'bn': True, 'batch_size': 4}\n",
      "# Metrics - train loss: 1.87681 \t val loss: 1.73880 \t val accuracy: 0.40716 \t val recall: 0.40708 \t val f1: 0.39942\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.0001, 'optimiser': None, 'bn': True, 'batch_size': 2}\n",
      "\tearly stopping at ep.40\t train loss: 2.20012\t val loss: 2.07598\t best val loss: 2.07030\n",
      "CV_1 (275.09s) -\t train_loss: 2.20012\t val_loss: 2.07598\t val_acc: 0.30470\t val_recall: 0.30535\t val_f1: 0.27576\n",
      "\tearly stopping at ep.50\t train loss: 2.20043\t val loss: 2.07923\t best val loss: 2.07132\n",
      "CV_2 (344.99s) -\t train_loss: 2.20043\t val_loss: 2.07923\t val_acc: 0.31080\t val_recall: 0.30980\t val_f1: 0.28291\n",
      "\tearly stopping at ep.48\t train loss: 2.20161\t val loss: 2.07934\t best val loss: 2.07469\n",
      "CV_3 (329.43s) -\t train_loss: 2.20161\t val_loss: 2.07934\t val_acc: 0.31150\t val_recall: 0.31060\t val_f1: 0.27774\n",
      "\tearly stopping at ep.48\t train loss: 2.19946\t val loss: 2.07776\t best val loss: 2.07587\n",
      "CV_4 (330.60s) -\t train_loss: 2.19946\t val_loss: 2.07776\t val_acc: 0.30680\t val_recall: 0.30849\t val_f1: 0.28168\n",
      "\tearly stopping at ep.50\t train loss: 2.19912\t val loss: 2.07396\t best val loss: 2.06882\n",
      "CV_5 (342.83s) -\t train_loss: 2.19912\t val_loss: 2.07396\t val_acc: 0.31850\t val_recall: 0.31870\t val_f1: 0.29794\n",
      "## Experiment results (1623.29s) - {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.0001, 'optimiser': None, 'bn': True, 'batch_size': 2}\n",
      "# Metrics - train loss: 2.20015 \t val loss: 2.07725 \t val accuracy: 0.31046 \t val recall: 0.31059 \t val f1: 0.28321\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.0001, 'optimiser': 'Adam', 'bn': False, 'batch_size': 16}\n",
      "CV_1 (38.60s) -\t train_loss: 1.21522\t val_loss: 1.47322\t val_acc: 0.48110\t val_recall: 0.48213\t val_f1: 0.47969\n",
      "CV_2 (35.62s) -\t train_loss: 1.25892\t val_loss: 1.46791\t val_acc: 0.47650\t val_recall: 0.47552\t val_f1: 0.46973\n",
      "CV_3 (33.92s) -\t train_loss: 1.22468\t val_loss: 1.48251\t val_acc: 0.48140\t val_recall: 0.48168\t val_f1: 0.48171\n",
      "CV_4 (39.82s) -\t train_loss: 1.29639\t val_loss: 1.46410\t val_acc: 0.48340\t val_recall: 0.48322\t val_f1: 0.47884\n",
      "\tearly stopping at ep.48\t train loss: 1.22874\t val loss: 1.48310\t best val loss: 1.47558\n",
      "CV_5 (34.32s) -\t train_loss: 1.22874\t val_loss: 1.48310\t val_acc: 0.47670\t val_recall: 0.47719\t val_f1: 0.46488\n",
      "## Experiment results (182.53s) - {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.0001, 'optimiser': 'Adam', 'bn': False, 'batch_size': 16}\n",
      "# Metrics - train loss: 1.24479 \t val loss: 1.47417 \t val accuracy: 0.47982 \t val recall: 0.47995 \t val f1: 0.47497\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.0001, 'optimiser': 'Adam', 'bn': False, 'batch_size': 8}\n",
      "CV_1 (60.93s) -\t train_loss: 1.26229\t val_loss: 1.42810\t val_acc: 0.48970\t val_recall: 0.49223\t val_f1: 0.47865\n",
      "CV_2 (71.60s) -\t train_loss: 1.28811\t val_loss: 1.41654\t val_acc: 0.49970\t val_recall: 0.49780\t val_f1: 0.49029\n",
      "CV_3 (61.53s) -\t train_loss: 1.27063\t val_loss: 1.43704\t val_acc: 0.49290\t val_recall: 0.49316\t val_f1: 0.49092\n",
      "\tearly stopping at ep.41\t train loss: 1.27773\t val loss: 1.43264\t best val loss: 1.43070\n",
      "CV_4 (57.95s) -\t train_loss: 1.27773\t val_loss: 1.43264\t val_acc: 0.48910\t val_recall: 0.48907\t val_f1: 0.47659\n",
      "CV_5 (61.26s) -\t train_loss: 1.25412\t val_loss: 1.41655\t val_acc: 0.50090\t val_recall: 0.50110\t val_f1: 0.50020\n",
      "## Experiment results (313.52s) - {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.0001, 'optimiser': 'Adam', 'bn': False, 'batch_size': 8}\n",
      "# Metrics - train loss: 1.27058 \t val loss: 1.42617 \t val accuracy: 0.49446 \t val recall: 0.49467 \t val f1: 0.48733\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.0001, 'optimiser': 'Adam', 'bn': False, 'batch_size': 4}\n",
      "CV_1 (116.83s) -\t train_loss: 1.30520\t val_loss: 1.40056\t val_acc: 0.50630\t val_recall: 0.50786\t val_f1: 0.50455\n",
      "\tearly stopping at ep.43\t train loss: 1.29669\t val loss: 1.40751\t best val loss: 1.39859\n",
      "CV_2 (103.95s) -\t train_loss: 1.29669\t val_loss: 1.40751\t val_acc: 0.50390\t val_recall: 0.50247\t val_f1: 0.49483\n",
      "CV_3 (118.62s) -\t train_loss: 1.29466\t val_loss: 1.39854\t val_acc: 0.50390\t val_recall: 0.50481\t val_f1: 0.49975\n",
      "CV_4 (118.92s) -\t train_loss: 1.29223\t val_loss: 1.39730\t val_acc: 0.50410\t val_recall: 0.50447\t val_f1: 0.49630\n",
      "CV_5 (118.03s) -\t train_loss: 1.29548\t val_loss: 1.41166\t val_acc: 0.50190\t val_recall: 0.50195\t val_f1: 0.49456\n",
      "## Experiment results (576.61s) - {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.0001, 'optimiser': 'Adam', 'bn': False, 'batch_size': 4}\n",
      "# Metrics - train loss: 1.29685 \t val loss: 1.40311 \t val accuracy: 0.50402 \t val recall: 0.50431 \t val f1: 0.49800\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.0001, 'optimiser': 'Adam', 'bn': False, 'batch_size': 2}\n",
      "\tearly stopping at ep.46\t train loss: 1.33719\t val loss: 1.40170\t best val loss: 1.40219\n",
      "CV_1 (209.04s) -\t train_loss: 1.33719\t val_loss: 1.40170\t val_acc: 0.50230\t val_recall: 0.50377\t val_f1: 0.49543\n",
      "CV_2 (225.24s) -\t train_loss: 1.31830\t val_loss: 1.39862\t val_acc: 0.50550\t val_recall: 0.50369\t val_f1: 0.49879\n",
      "\tearly stopping at ep.49\t train loss: 1.33097\t val loss: 1.41869\t best val loss: 1.41405\n",
      "CV_3 (221.04s) -\t train_loss: 1.33097\t val_loss: 1.41869\t val_acc: 0.49530\t val_recall: 0.49545\t val_f1: 0.49344\n",
      "CV_4 (225.93s) -\t train_loss: 1.33556\t val_loss: 1.41486\t val_acc: 0.50140\t val_recall: 0.50088\t val_f1: 0.49590\n",
      "CV_5 (225.64s) -\t train_loss: 1.33784\t val_loss: 1.41079\t val_acc: 0.50080\t val_recall: 0.50120\t val_f1: 0.49490\n",
      "## Experiment results (1107.13s) - {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.0001, 'optimiser': 'Adam', 'bn': False, 'batch_size': 2}\n",
      "# Metrics - train loss: 1.33197 \t val loss: 1.40893 \t val accuracy: 0.50106 \t val recall: 0.50100 \t val f1: 0.49569\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.0001, 'optimiser': 'Adam', 'bn': True, 'batch_size': 16}\n",
      "CV_1 (65.54s) -\t train_loss: 1.43781\t val_loss: 1.49178\t val_acc: 0.47370\t val_recall: 0.47525\t val_f1: 0.46804\n",
      "CV_2 (67.44s) -\t train_loss: 1.45079\t val_loss: 1.49143\t val_acc: 0.47210\t val_recall: 0.47093\t val_f1: 0.46509\n",
      "CV_3 (66.06s) -\t train_loss: 1.44493\t val_loss: 1.50208\t val_acc: 0.47400\t val_recall: 0.47422\t val_f1: 0.47141\n",
      "CV_4 (65.70s) -\t train_loss: 1.49073\t val_loss: 1.50095\t val_acc: 0.46320\t val_recall: 0.46287\t val_f1: 0.45954\n",
      "CV_5 (65.59s) -\t train_loss: 1.43451\t val_loss: 1.49938\t val_acc: 0.47140\t val_recall: 0.47143\t val_f1: 0.46702\n",
      "## Experiment results (330.63s) - {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.0001, 'optimiser': 'Adam', 'bn': True, 'batch_size': 16}\n",
      "# Metrics - train loss: 1.45175 \t val loss: 1.49712 \t val accuracy: 0.47088 \t val recall: 0.47094 \t val f1: 0.46622\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.0001, 'optimiser': 'Adam', 'bn': True, 'batch_size': 8}\n",
      "\tearly stopping at ep.40\t train loss: 1.63717\t val loss: 1.53253\t best val loss: 1.52600\n",
      "CV_1 (96.56s) -\t train_loss: 1.63717\t val_loss: 1.53253\t val_acc: 0.46560\t val_recall: 0.46708\t val_f1: 0.46264\n",
      "\tearly stopping at ep.48\t train loss: 1.62106\t val loss: 1.54321\t best val loss: 1.52427\n",
      "CV_2 (115.44s) -\t train_loss: 1.62106\t val_loss: 1.54321\t val_acc: 0.46110\t val_recall: 0.45946\t val_f1: 0.45506\n",
      "\tearly stopping at ep.45\t train loss: 1.63247\t val loss: 1.52828\t best val loss: 1.52530\n",
      "CV_3 (108.17s) -\t train_loss: 1.63247\t val_loss: 1.52828\t val_acc: 0.47040\t val_recall: 0.47076\t val_f1: 0.46433\n",
      "\tearly stopping at ep.40\t train loss: 1.64400\t val loss: 1.53137\t best val loss: 1.52266\n",
      "CV_4 (96.54s) -\t train_loss: 1.64400\t val_loss: 1.53137\t val_acc: 0.46620\t val_recall: 0.46642\t val_f1: 0.46116\n",
      "CV_5 (120.89s) -\t train_loss: 1.62051\t val_loss: 1.53598\t val_acc: 0.47080\t val_recall: 0.47131\t val_f1: 0.46632\n",
      "## Experiment results (537.91s) - {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.0001, 'optimiser': 'Adam', 'bn': True, 'batch_size': 8}\n",
      "# Metrics - train loss: 1.63104 \t val loss: 1.53427 \t val accuracy: 0.46682 \t val recall: 0.46701 \t val f1: 0.46190\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.0001, 'optimiser': 'Adam', 'bn': True, 'batch_size': 4}\n",
      "CV_1 (231.53s) -\t train_loss: 1.90349\t val_loss: 1.69010\t val_acc: 0.43220\t val_recall: 0.43425\t val_f1: 0.42512\n",
      "CV_2 (231.25s) -\t train_loss: 1.91069\t val_loss: 1.69744\t val_acc: 0.43250\t val_recall: 0.43132\t val_f1: 0.42567\n",
      "CV_3 (230.88s) -\t train_loss: 1.89840\t val_loss: 1.66150\t val_acc: 0.44580\t val_recall: 0.44579\t val_f1: 0.43669\n",
      "CV_4 (229.85s) -\t train_loss: 1.90209\t val_loss: 1.68662\t val_acc: 0.43420\t val_recall: 0.43443\t val_f1: 0.42458\n",
      "\tearly stopping at ep.38\t train loss: 1.91099\t val loss: 1.71802\t best val loss: 1.70957\n",
      "CV_5 (174.98s) -\t train_loss: 1.91099\t val_loss: 1.71802\t val_acc: 0.42980\t val_recall: 0.43026\t val_f1: 0.42135\n",
      "## Experiment results (1098.85s) - {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.0001, 'optimiser': 'Adam', 'bn': True, 'batch_size': 4}\n",
      "# Metrics - train loss: 1.90513 \t val loss: 1.69074 \t val accuracy: 0.43490 \t val recall: 0.43521 \t val f1: 0.42668\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.0001, 'optimiser': 'Adam', 'bn': True, 'batch_size': 2}\n",
      "\tearly stopping at ep.37\t train loss: 2.17885\t val loss: 2.02822\t best val loss: 2.01750\n",
      "CV_1 (334.87s) -\t train_loss: 2.17885\t val_loss: 2.02822\t val_acc: 0.33910\t val_recall: 0.34050\t val_f1: 0.31668\n",
      "\tearly stopping at ep.33\t train loss: 2.17491\t val loss: 2.02342\t best val loss: 2.02426\n",
      "CV_2 (299.53s) -\t train_loss: 2.17491\t val_loss: 2.02342\t val_acc: 0.34290\t val_recall: 0.34175\t val_f1: 0.31965\n",
      "\tearly stopping at ep.33\t train loss: 2.17766\t val loss: 2.02900\t best val loss: 2.02179\n",
      "CV_3 (296.52s) -\t train_loss: 2.17766\t val_loss: 2.02900\t val_acc: 0.34430\t val_recall: 0.34365\t val_f1: 0.32544\n",
      "\tearly stopping at ep.47\t train loss: 2.17497\t val loss: 2.02967\t best val loss: 2.02595\n",
      "CV_4 (425.48s) -\t train_loss: 2.17497\t val_loss: 2.02967\t val_acc: 0.33550\t val_recall: 0.33536\t val_f1: 0.30865\n",
      "\tearly stopping at ep.36\t train loss: 2.17599\t val loss: 2.03370\t best val loss: 2.02721\n",
      "CV_5 (324.40s) -\t train_loss: 2.17599\t val_loss: 2.03370\t val_acc: 0.33430\t val_recall: 0.33484\t val_f1: 0.31513\n",
      "## Experiment results (1681.18s) - {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.0001, 'optimiser': 'Adam', 'bn': True, 'batch_size': 2}\n",
      "# Metrics - train loss: 2.17648 \t val loss: 2.02880 \t val accuracy: 0.33922 \t val recall: 0.33922 \t val f1: 0.31711\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.0001, 'optimiser': 'Momentum', 'bn': False, 'batch_size': 16}\n",
      "\tearly stopping at ep.49\t train loss: 1.43220\t val loss: 1.77244\t best val loss: 1.76414\n",
      "CV_1 (24.39s) -\t train_loss: 1.43220\t val_loss: 1.77244\t val_acc: 0.39090\t val_recall: 0.39188\t val_f1: 0.37616\n",
      "\tearly stopping at ep.18\t train loss: 1.51787\t val loss: 1.90600\t best val loss: 1.84059\n",
      "CV_2 (9.48s) -\t train_loss: 1.51787\t val_loss: 1.90600\t val_acc: 0.35390\t val_recall: 0.35342\t val_f1: 0.34593\n",
      "\tearly stopping at ep.23\t train loss: 1.47833\t val loss: 1.87300\t best val loss: 1.79019\n",
      "CV_3 (12.39s) -\t train_loss: 1.47833\t val_loss: 1.87300\t val_acc: 0.35240\t val_recall: 0.35449\t val_f1: 0.34880\n",
      "\tearly stopping at ep.31\t train loss: 1.42994\t val loss: 1.93950\t best val loss: 1.79218\n",
      "CV_4 (15.38s) -\t train_loss: 1.42994\t val_loss: 1.93950\t val_acc: 0.37950\t val_recall: 0.38089\t val_f1: 0.37455\n",
      "\tearly stopping at ep.36\t train loss: 1.48118\t val loss: 1.83251\t best val loss: 1.76657\n",
      "CV_5 (18.90s) -\t train_loss: 1.48118\t val_loss: 1.83251\t val_acc: 0.37590\t val_recall: 0.37680\t val_f1: 0.37642\n",
      "## Experiment results (80.74s) - {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.0001, 'optimiser': 'Momentum', 'bn': False, 'batch_size': 16}\n",
      "# Metrics - train loss: 1.46790 \t val loss: 1.86469 \t val accuracy: 0.37052 \t val recall: 0.37150 \t val f1: 0.36437\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.0001, 'optimiser': 'Momentum', 'bn': False, 'batch_size': 8}\n",
      "\tearly stopping at ep.35\t train loss: 1.35306\t val loss: 1.59219\t best val loss: 1.55871\n",
      "CV_1 (27.98s) -\t train_loss: 1.35306\t val_loss: 1.59219\t val_acc: 0.45680\t val_recall: 0.45728\t val_f1: 0.44674\n",
      "CV_2 (40.95s) -\t train_loss: 1.36321\t val_loss: 1.55167\t val_acc: 0.44710\t val_recall: 0.44633\t val_f1: 0.44460\n",
      "\tearly stopping at ep.46\t train loss: 1.35012\t val loss: 1.57630\t best val loss: 1.55216\n",
      "CV_3 (37.30s) -\t train_loss: 1.35012\t val_loss: 1.57630\t val_acc: 0.42900\t val_recall: 0.42978\t val_f1: 0.41884\n",
      "\tearly stopping at ep.46\t train loss: 1.38243\t val loss: 1.59258\t best val loss: 1.54065\n",
      "CV_4 (37.16s) -\t train_loss: 1.38243\t val_loss: 1.59258\t val_acc: 0.44710\t val_recall: 0.44664\t val_f1: 0.44357\n",
      "CV_5 (41.12s) -\t train_loss: 1.34874\t val_loss: 1.56416\t val_acc: 0.46140\t val_recall: 0.46058\t val_f1: 0.46237\n",
      "## Experiment results (184.79s) - {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.0001, 'optimiser': 'Momentum', 'bn': False, 'batch_size': 8}\n",
      "# Metrics - train loss: 1.35951 \t val loss: 1.57538 \t val accuracy: 0.44828 \t val recall: 0.44812 \t val f1: 0.44322\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.0001, 'optimiser': 'Momentum', 'bn': False, 'batch_size': 4}\n",
      "CV_1 (73.47s) -\t train_loss: 1.35609\t val_loss: 1.46702\t val_acc: 0.47640\t val_recall: 0.47791\t val_f1: 0.47140\n",
      "CV_2 (79.21s) -\t train_loss: 1.35466\t val_loss: 1.48175\t val_acc: 0.48060\t val_recall: 0.47880\t val_f1: 0.47014\n",
      "\tearly stopping at ep.45\t train loss: 1.33686\t val loss: 1.48657\t best val loss: 1.45906\n",
      "CV_3 (69.97s) -\t train_loss: 1.33686\t val_loss: 1.48657\t val_acc: 0.47500\t val_recall: 0.47488\t val_f1: 0.47075\n",
      "CV_4 (77.66s) -\t train_loss: 1.34207\t val_loss: 1.45246\t val_acc: 0.48500\t val_recall: 0.48518\t val_f1: 0.47355\n",
      "\tearly stopping at ep.37\t train loss: 1.35681\t val loss: 1.48633\t best val loss: 1.45491\n",
      "CV_5 (58.58s) -\t train_loss: 1.35681\t val_loss: 1.48633\t val_acc: 0.47480\t val_recall: 0.47392\t val_f1: 0.46697\n",
      "## Experiment results (359.16s) - {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.0001, 'optimiser': 'Momentum', 'bn': False, 'batch_size': 4}\n",
      "# Metrics - train loss: 1.34930 \t val loss: 1.47482 \t val accuracy: 0.47836 \t val recall: 0.47814 \t val f1: 0.47056\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.0001, 'optimiser': 'Momentum', 'bn': False, 'batch_size': 2}\n",
      "CV_1 (145.12s) -\t train_loss: 1.36403\t val_loss: 1.42402\t val_acc: 0.49800\t val_recall: 0.49966\t val_f1: 0.49030\n",
      "\tearly stopping at ep.44\t train loss: 1.37222\t val loss: 1.44589\t best val loss: 1.42053\n",
      "CV_2 (127.59s) -\t train_loss: 1.37222\t val_loss: 1.44589\t val_acc: 0.49400\t val_recall: 0.49252\t val_f1: 0.48818\n",
      "\tearly stopping at ep.37\t train loss: 1.37493\t val loss: 1.45597\t best val loss: 1.43884\n",
      "CV_3 (103.64s) -\t train_loss: 1.37493\t val_loss: 1.45597\t val_acc: 0.48460\t val_recall: 0.48434\t val_f1: 0.47490\n",
      "\tearly stopping at ep.30\t train loss: 1.38186\t val loss: 1.46631\t best val loss: 1.43883\n",
      "CV_4 (87.12s) -\t train_loss: 1.38186\t val_loss: 1.46631\t val_acc: 0.47810\t val_recall: 0.47759\t val_f1: 0.46855\n",
      "\tearly stopping at ep.42\t train loss: 1.37839\t val loss: 1.45507\t best val loss: 1.43253\n",
      "CV_5 (129.08s) -\t train_loss: 1.37839\t val_loss: 1.45507\t val_acc: 0.48330\t val_recall: 0.48307\t val_f1: 0.47833\n",
      "## Experiment results (592.79s) - {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.0001, 'optimiser': 'Momentum', 'bn': False, 'batch_size': 2}\n",
      "# Metrics - train loss: 1.37429 \t val loss: 1.44945 \t val accuracy: 0.48760 \t val recall: 0.48744 \t val f1: 0.48005\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.0001, 'optimiser': 'Momentum', 'bn': True, 'batch_size': 16}\n",
      "CV_1 (54.62s) -\t train_loss: 1.51082\t val_loss: 1.56530\t val_acc: 0.44560\t val_recall: 0.44673\t val_f1: 0.44065\n",
      "CV_2 (56.47s) -\t train_loss: 1.48145\t val_loss: 1.51427\t val_acc: 0.46530\t val_recall: 0.46427\t val_f1: 0.46181\n",
      "CV_3 (55.06s) -\t train_loss: 1.51420\t val_loss: 1.54736\t val_acc: 0.45610\t val_recall: 0.45660\t val_f1: 0.45018\n",
      "CV_4 (55.59s) -\t train_loss: 1.53013\t val_loss: 1.55515\t val_acc: 0.44880\t val_recall: 0.44878\t val_f1: 0.44604\n",
      "CV_5 (54.89s) -\t train_loss: 1.50314\t val_loss: 1.57742\t val_acc: 0.45220\t val_recall: 0.45218\t val_f1: 0.44722\n",
      "## Experiment results (277.03s) - {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.0001, 'optimiser': 'Momentum', 'bn': True, 'batch_size': 16}\n",
      "# Metrics - train loss: 1.50795 \t val loss: 1.55190 \t val accuracy: 0.45360 \t val recall: 0.45371 \t val f1: 0.44918\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.0001, 'optimiser': 'Momentum', 'bn': True, 'batch_size': 8}\n",
      "\tearly stopping at ep.46\t train loss: 1.68675\t val loss: 1.58281\t best val loss: 1.56774\n",
      "CV_1 (92.33s) -\t train_loss: 1.68675\t val_loss: 1.58281\t val_acc: 0.44230\t val_recall: 0.44399\t val_f1: 0.43740\n",
      "\tearly stopping at ep.45\t train loss: 1.68269\t val loss: 1.59639\t best val loss: 1.56824\n",
      "CV_2 (89.92s) -\t train_loss: 1.68269\t val_loss: 1.59639\t val_acc: 0.44400\t val_recall: 0.44305\t val_f1: 0.43885\n",
      "\tearly stopping at ep.48\t train loss: 1.69848\t val loss: 1.57472\t best val loss: 1.55663\n",
      "CV_3 (95.30s) -\t train_loss: 1.69848\t val_loss: 1.57472\t val_acc: 0.45030\t val_recall: 0.45081\t val_f1: 0.44295\n",
      "\tearly stopping at ep.45\t train loss: 1.69219\t val loss: 1.61192\t best val loss: 1.56631\n",
      "CV_4 (90.51s) -\t train_loss: 1.69219\t val_loss: 1.61192\t val_acc: 0.43770\t val_recall: 0.43766\t val_f1: 0.43128\n",
      "\tearly stopping at ep.47\t train loss: 1.68174\t val loss: 1.58631\t best val loss: 1.56870\n",
      "CV_5 (93.51s) -\t train_loss: 1.68174\t val_loss: 1.58631\t val_acc: 0.44860\t val_recall: 0.44842\t val_f1: 0.44179\n",
      "## Experiment results (461.90s) - {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.0001, 'optimiser': 'Momentum', 'bn': True, 'batch_size': 8}\n",
      "# Metrics - train loss: 1.68837 \t val loss: 1.59043 \t val accuracy: 0.44458 \t val recall: 0.44478 \t val f1: 0.43845\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.0001, 'optimiser': 'Momentum', 'bn': True, 'batch_size': 4}\n",
      "\tearly stopping at ep.50\t train loss: 1.95216\t val loss: 1.76002\t best val loss: 1.74067\n",
      "CV_1 (187.76s) -\t train_loss: 1.95216\t val_loss: 1.76002\t val_acc: 0.40360\t val_recall: 0.40510\t val_f1: 0.39287\n",
      "\tearly stopping at ep.43\t train loss: 1.96037\t val loss: 1.74967\t best val loss: 1.74303\n",
      "CV_2 (160.77s) -\t train_loss: 1.96037\t val_loss: 1.74967\t val_acc: 0.41440\t val_recall: 0.41358\t val_f1: 0.40547\n",
      "\tearly stopping at ep.37\t train loss: 1.96078\t val loss: 1.76089\t best val loss: 1.75682\n",
      "CV_3 (137.79s) -\t train_loss: 1.96078\t val_loss: 1.76089\t val_acc: 0.40820\t val_recall: 0.40841\t val_f1: 0.40110\n",
      "\tearly stopping at ep.45\t train loss: 1.96117\t val loss: 1.75139\t best val loss: 1.74656\n",
      "CV_4 (167.92s) -\t train_loss: 1.96117\t val_loss: 1.75139\t val_acc: 0.41360\t val_recall: 0.41319\t val_f1: 0.40660\n",
      "\tearly stopping at ep.49\t train loss: 1.95624\t val loss: 1.75495\t best val loss: 1.75115\n",
      "CV_5 (182.60s) -\t train_loss: 1.95624\t val_loss: 1.75495\t val_acc: 0.41440\t val_recall: 0.41408\t val_f1: 0.40749\n",
      "## Experiment results (837.12s) - {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.0001, 'optimiser': 'Momentum', 'bn': True, 'batch_size': 4}\n",
      "# Metrics - train loss: 1.95814 \t val loss: 1.75538 \t val accuracy: 0.41084 \t val recall: 0.41087 \t val f1: 0.40270\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.0001, 'optimiser': 'Momentum', 'bn': True, 'batch_size': 2}\n",
      "\tearly stopping at ep.34\t train loss: 2.22903\t val loss: 2.12970\t best val loss: 2.12300\n",
      "CV_1 (245.72s) -\t train_loss: 2.22903\t val_loss: 2.12970\t val_acc: 0.27060\t val_recall: 0.27199\t val_f1: 0.23195\n",
      "\tearly stopping at ep.31\t train loss: 2.22830\t val loss: 2.12863\t best val loss: 2.12156\n",
      "CV_2 (226.71s) -\t train_loss: 2.22830\t val_loss: 2.12863\t val_acc: 0.26100\t val_recall: 0.26056\t val_f1: 0.22031\n",
      "\tearly stopping at ep.41\t train loss: 2.22614\t val loss: 2.12769\t best val loss: 2.11803\n",
      "CV_3 (299.63s) -\t train_loss: 2.22614\t val_loss: 2.12769\t val_acc: 0.28730\t val_recall: 0.28499\t val_f1: 0.25177\n",
      "\tearly stopping at ep.44\t train loss: 2.22989\t val loss: 2.14281\t best val loss: 2.12304\n",
      "CV_4 (315.98s) -\t train_loss: 2.22989\t val_loss: 2.14281\t val_acc: 0.24980\t val_recall: 0.25192\t val_f1: 0.21493\n",
      "\tearly stopping at ep.37\t train loss: 2.23820\t val loss: 2.14733\t best val loss: 2.11767\n",
      "CV_5 (267.62s) -\t train_loss: 2.23820\t val_loss: 2.14733\t val_acc: 0.26400\t val_recall: 0.26447\t val_f1: 0.22391\n",
      "## Experiment results (1356.03s) - {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.0001, 'optimiser': 'Momentum', 'bn': True, 'batch_size': 2}\n",
      "# Metrics - train loss: 2.23031 \t val loss: 2.13523 \t val accuracy: 0.26654 \t val recall: 0.26679 \t val f1: 0.22857\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.01, 'optimiser': None, 'bn': False, 'batch_size': 16}\n",
      "\tearly stopping at ep.21\t train loss: 1.71314\t val loss: 2.66421\t best val loss: 2.31367\n",
      "CV_1 (9.38s) -\t train_loss: 1.71314\t val_loss: 2.66421\t val_acc: 0.19310\t val_recall: 0.19679\t val_f1: 0.14073\n",
      "\tearly stopping at ep.22\t train loss: 1.78197\t val loss: 2.62947\t best val loss: 2.40619\n",
      "CV_2 (9.43s) -\t train_loss: 1.78197\t val_loss: 2.62947\t val_acc: 0.21520\t val_recall: 0.21140\t val_f1: 0.16664\n",
      "\tearly stopping at ep.16\t train loss: 1.69238\t val loss: 2.41619\t best val loss: 2.29606\n",
      "CV_3 (6.83s) -\t train_loss: 1.69238\t val_loss: 2.41619\t val_acc: 0.19180\t val_recall: 0.19394\t val_f1: 0.15763\n",
      "\tearly stopping at ep.14\t train loss: 1.62912\t val loss: 2.47749\t best val loss: 2.35195\n",
      "CV_4 (6.06s) -\t train_loss: 1.62912\t val_loss: 2.47749\t val_acc: 0.21460\t val_recall: 0.21637\t val_f1: 0.18327\n",
      "\tearly stopping at ep.40\t train loss: 1.65099\t val loss: 2.35027\t best val loss: 2.29900\n",
      "CV_5 (17.37s) -\t train_loss: 1.65099\t val_loss: 2.35027\t val_acc: 0.21930\t val_recall: 0.22027\t val_f1: 0.18219\n",
      "## Experiment results (49.34s) - {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.01, 'optimiser': None, 'bn': False, 'batch_size': 16}\n",
      "# Metrics - train loss: 1.69352 \t val loss: 2.50753 \t val accuracy: 0.20680 \t val recall: 0.20775 \t val f1: 0.16609\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.01, 'optimiser': None, 'bn': False, 'batch_size': 8}\n",
      "\tearly stopping at ep.30\t train loss: 1.53227\t val loss: 2.14898\t best val loss: 2.13343\n",
      "CV_1 (21.38s) -\t train_loss: 1.53227\t val_loss: 2.14898\t val_acc: 0.24930\t val_recall: 0.25400\t val_f1: 0.21982\n",
      "\tearly stopping at ep.27\t train loss: 1.48982\t val loss: 2.47916\t best val loss: 2.16414\n",
      "CV_2 (19.68s) -\t train_loss: 1.48982\t val_loss: 2.47916\t val_acc: 0.27090\t val_recall: 0.27050\t val_f1: 0.22757\n",
      "\tearly stopping at ep.17\t train loss: 1.57529\t val loss: 2.19713\t best val loss: 2.16229\n",
      "CV_3 (13.09s) -\t train_loss: 1.57529\t val_loss: 2.19713\t val_acc: 0.26150\t val_recall: 0.26484\t val_f1: 0.23957\n",
      "\tearly stopping at ep.14\t train loss: 1.54937\t val loss: 2.68803\t best val loss: 2.23225\n",
      "CV_4 (10.46s) -\t train_loss: 1.54937\t val_loss: 2.68803\t val_acc: 0.21930\t val_recall: 0.21960\t val_f1: 0.19273\n",
      "\tearly stopping at ep.26\t train loss: 1.49701\t val loss: 2.23973\t best val loss: 2.20762\n",
      "CV_5 (19.55s) -\t train_loss: 1.49701\t val_loss: 2.23973\t val_acc: 0.29010\t val_recall: 0.29095\t val_f1: 0.25825\n",
      "## Experiment results (84.42s) - {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.01, 'optimiser': None, 'bn': False, 'batch_size': 8}\n",
      "# Metrics - train loss: 1.52875 \t val loss: 2.35061 \t val accuracy: 0.25822 \t val recall: 0.25998 \t val f1: 0.22759\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.01, 'optimiser': None, 'bn': False, 'batch_size': 4}\n",
      "\tearly stopping at ep.20\t train loss: 1.38292\t val loss: 2.31221\t best val loss: 2.00969\n",
      "CV_1 (26.08s) -\t train_loss: 1.38292\t val_loss: 2.31221\t val_acc: 0.33480\t val_recall: 0.33443\t val_f1: 0.31493\n",
      "\tearly stopping at ep.16\t train loss: 1.40180\t val loss: 2.16198\t best val loss: 2.04135\n",
      "CV_2 (22.79s) -\t train_loss: 1.40180\t val_loss: 2.16198\t val_acc: 0.28970\t val_recall: 0.29127\t val_f1: 0.27565\n",
      "\tearly stopping at ep.21\t train loss: 1.36922\t val loss: 2.12555\t best val loss: 1.96352\n",
      "CV_3 (29.52s) -\t train_loss: 1.36922\t val_loss: 2.12555\t val_acc: 0.31630\t val_recall: 0.31957\t val_f1: 0.30500\n",
      "\tearly stopping at ep.18\t train loss: 1.36958\t val loss: 2.05844\t best val loss: 1.98278\n",
      "CV_4 (25.31s) -\t train_loss: 1.36958\t val_loss: 2.05844\t val_acc: 0.33680\t val_recall: 0.33666\t val_f1: 0.32939\n",
      "\tearly stopping at ep.16\t train loss: 1.41461\t val loss: 2.18634\t best val loss: 2.03078\n",
      "CV_5 (22.65s) -\t train_loss: 1.41461\t val_loss: 2.18634\t val_acc: 0.29690\t val_recall: 0.29721\t val_f1: 0.28167\n",
      "## Experiment results (126.61s) - {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.01, 'optimiser': None, 'bn': False, 'batch_size': 4}\n",
      "# Metrics - train loss: 1.38763 \t val loss: 2.16890 \t val accuracy: 0.31490 \t val recall: 0.31583 \t val f1: 0.30133\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.01, 'optimiser': None, 'bn': False, 'batch_size': 2}\n",
      "\tearly stopping at ep.24\t train loss: 1.32550\t val loss: 1.88698\t best val loss: 1.75719\n",
      "CV_1 (63.18s) -\t train_loss: 1.32550\t val_loss: 1.88698\t val_acc: 0.38500\t val_recall: 0.38492\t val_f1: 0.37949\n",
      "\tearly stopping at ep.26\t train loss: 1.31042\t val loss: 1.87975\t best val loss: 1.71735\n",
      "CV_2 (64.58s) -\t train_loss: 1.31042\t val_loss: 1.87975\t val_acc: 0.38030\t val_recall: 0.38028\t val_f1: 0.37055\n",
      "\tearly stopping at ep.20\t train loss: 1.34249\t val loss: 1.91662\t best val loss: 1.72575\n",
      "CV_3 (52.95s) -\t train_loss: 1.34249\t val_loss: 1.91662\t val_acc: 0.37290\t val_recall: 0.37301\t val_f1: 0.36672\n",
      "\tearly stopping at ep.31\t train loss: 1.32326\t val loss: 1.75788\t best val loss: 1.69754\n",
      "CV_4 (76.88s) -\t train_loss: 1.32326\t val_loss: 1.75788\t val_acc: 0.40740\t val_recall: 0.40631\t val_f1: 0.40141\n",
      "\tearly stopping at ep.18\t train loss: 1.33419\t val loss: 1.80865\t best val loss: 1.77856\n",
      "CV_5 (44.72s) -\t train_loss: 1.33419\t val_loss: 1.80865\t val_acc: 0.39350\t val_recall: 0.39405\t val_f1: 0.37853\n",
      "## Experiment results (302.59s) - {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.01, 'optimiser': None, 'bn': False, 'batch_size': 2}\n",
      "# Metrics - train loss: 1.32717 \t val loss: 1.84998 \t val accuracy: 0.38782 \t val recall: 0.38771 \t val f1: 0.37934\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.01, 'optimiser': None, 'bn': True, 'batch_size': 16}\n",
      "\tearly stopping at ep.31\t train loss: 1.33479\t val loss: 2.43647\t best val loss: 2.33327\n",
      "CV_1 (32.68s) -\t train_loss: 1.33479\t val_loss: 2.43647\t val_acc: 0.27890\t val_recall: 0.28129\t val_f1: 0.26847\n",
      "\tearly stopping at ep.50\t train loss: 1.34938\t val loss: 2.66992\t best val loss: 2.24278\n",
      "CV_2 (52.81s) -\t train_loss: 1.34938\t val_loss: 2.66992\t val_acc: 0.26690\t val_recall: 0.26650\t val_f1: 0.24356\n",
      "\tearly stopping at ep.25\t train loss: 1.30628\t val loss: 2.39255\t best val loss: 2.36526\n",
      "CV_3 (26.37s) -\t train_loss: 1.30628\t val_loss: 2.39255\t val_acc: 0.26120\t val_recall: 0.26090\t val_f1: 0.23830\n",
      "\tearly stopping at ep.43\t train loss: 1.16079\t val loss: 2.53461\t best val loss: 2.26400\n",
      "CV_4 (45.23s) -\t train_loss: 1.16079\t val_loss: 2.53461\t val_acc: 0.28110\t val_recall: 0.28318\t val_f1: 0.27467\n",
      "\tearly stopping at ep.32\t train loss: 1.18753\t val loss: 2.60710\t best val loss: 2.38152\n",
      "CV_5 (33.55s) -\t train_loss: 1.18753\t val_loss: 2.60710\t val_acc: 0.19430\t val_recall: 0.19310\t val_f1: 0.19369\n",
      "## Experiment results (191.00s) - {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.01, 'optimiser': None, 'bn': True, 'batch_size': 16}\n",
      "# Metrics - train loss: 1.26775 \t val loss: 2.52813 \t val accuracy: 0.25648 \t val recall: 0.25699 \t val f1: 0.24374\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.01, 'optimiser': None, 'bn': True, 'batch_size': 8}\n",
      "\tearly stopping at ep.37\t train loss: 1.33445\t val loss: 1.87908\t best val loss: 1.80469\n",
      "CV_1 (69.43s) -\t train_loss: 1.33445\t val_loss: 1.87908\t val_acc: 0.36320\t val_recall: 0.36635\t val_f1: 0.34670\n",
      "\tearly stopping at ep.22\t train loss: 1.42570\t val loss: 1.90832\t best val loss: 1.87797\n",
      "CV_2 (42.35s) -\t train_loss: 1.42570\t val_loss: 1.90832\t val_acc: 0.36860\t val_recall: 0.36730\t val_f1: 0.35671\n",
      "\tearly stopping at ep.25\t train loss: 1.33767\t val loss: 1.90437\t best val loss: 1.84227\n",
      "CV_3 (47.45s) -\t train_loss: 1.33767\t val_loss: 1.90437\t val_acc: 0.36910\t val_recall: 0.37049\t val_f1: 0.35946\n",
      "\tearly stopping at ep.42\t train loss: 1.37786\t val loss: 1.90882\t best val loss: 1.78687\n",
      "CV_4 (79.58s) -\t train_loss: 1.37786\t val_loss: 1.90882\t val_acc: 0.35760\t val_recall: 0.35767\t val_f1: 0.34850\n",
      "CV_5 (94.08s) -\t train_loss: 1.38316\t val_loss: 1.90954\t val_acc: 0.37020\t val_recall: 0.37035\t val_f1: 0.35899\n",
      "## Experiment results (333.24s) - {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.01, 'optimiser': None, 'bn': True, 'batch_size': 8}\n",
      "# Metrics - train loss: 1.37177 \t val loss: 1.90203 \t val accuracy: 0.36574 \t val recall: 0.36643 \t val f1: 0.35407\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.01, 'optimiser': None, 'bn': True, 'batch_size': 4}\n",
      "\tearly stopping at ep.47\t train loss: 1.86438\t val loss: 1.74008\t best val loss: 1.69030\n",
      "CV_1 (166.98s) -\t train_loss: 1.86438\t val_loss: 1.74008\t val_acc: 0.38260\t val_recall: 0.38362\t val_f1: 0.37058\n",
      "\tearly stopping at ep.39\t train loss: 1.87909\t val loss: 1.71887\t best val loss: 1.70738\n",
      "CV_2 (139.40s) -\t train_loss: 1.87909\t val_loss: 1.71887\t val_acc: 0.39100\t val_recall: 0.39062\t val_f1: 0.37064\n",
      "\tearly stopping at ep.24\t train loss: 1.86895\t val loss: 1.76204\t best val loss: 1.73584\n",
      "CV_3 (85.64s) -\t train_loss: 1.86895\t val_loss: 1.76204\t val_acc: 0.37570\t val_recall: 0.37546\t val_f1: 0.34260\n",
      "\tearly stopping at ep.34\t train loss: 1.87363\t val loss: 1.72595\t best val loss: 1.71155\n",
      "CV_4 (121.05s) -\t train_loss: 1.87363\t val_loss: 1.72595\t val_acc: 0.39620\t val_recall: 0.39491\t val_f1: 0.37775\n",
      "\tearly stopping at ep.47\t train loss: 1.86523\t val loss: 1.72925\t best val loss: 1.70027\n",
      "CV_5 (167.13s) -\t train_loss: 1.86523\t val_loss: 1.72925\t val_acc: 0.39100\t val_recall: 0.39092\t val_f1: 0.38787\n",
      "## Experiment results (680.55s) - {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.01, 'optimiser': None, 'bn': True, 'batch_size': 4}\n",
      "# Metrics - train loss: 1.87026 \t val loss: 1.73524 \t val accuracy: 0.38730 \t val recall: 0.38711 \t val f1: 0.36989\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.01, 'optimiser': None, 'bn': True, 'batch_size': 2}\n",
      "\tearly stopping at ep.27\t train loss: 2.30251\t val loss: 2.30446\t best val loss: 2.16608\n",
      "CV_1 (185.46s) -\t train_loss: 2.30251\t val_loss: 2.30446\t val_acc: 0.10100\t val_recall: 0.09941\t val_f1: 0.01840\n",
      "\tearly stopping at ep.16\t train loss: 2.28716\t val loss: 2.28955\t best val loss: 2.27412\n",
      "CV_2 (111.21s) -\t train_loss: 2.28716\t val_loss: 2.28955\t val_acc: 0.12650\t val_recall: 0.12954\t val_f1: 0.04992\n",
      "\tearly stopping at ep.11\t train loss: 2.29624\t val loss: 2.28835\t best val loss: 2.22206\n",
      "CV_3 (76.69s) -\t train_loss: 2.29624\t val_loss: 2.28835\t val_acc: 0.14050\t val_recall: 0.14205\t val_f1: 0.09673\n",
      "\tearly stopping at ep.29\t train loss: 2.30304\t val loss: 2.30472\t best val loss: 2.16073\n",
      "CV_4 (200.10s) -\t train_loss: 2.30304\t val_loss: 2.30472\t val_acc: 0.10080\t val_recall: 0.10000\t val_f1: 0.01831\n",
      "\tearly stopping at ep.12\t train loss: 2.28264\t val loss: 2.25766\t best val loss: 2.25243\n",
      "CV_5 (82.99s) -\t train_loss: 2.28264\t val_loss: 2.25766\t val_acc: 0.12850\t val_recall: 0.12900\t val_f1: 0.09369\n",
      "## Experiment results (656.78s) - {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.01, 'optimiser': None, 'bn': True, 'batch_size': 2}\n",
      "# Metrics - train loss: 2.29432 \t val loss: 2.28895 \t val accuracy: 0.11946 \t val recall: 0.12000 \t val f1: 0.05541\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.01, 'optimiser': 'Adam', 'bn': False, 'batch_size': 16}\n",
      "\tearly stopping at ep.14\t train loss: 2.30895\t val loss: 2.32174\t best val loss: 2.31111\n",
      "CV_1 (10.41s) -\t train_loss: 2.30895\t val_loss: 2.32174\t val_acc: 0.09530\t val_recall: 0.10000\t val_f1: 0.01740\n",
      "\tearly stopping at ep.23\t train loss: 2.31351\t val loss: 2.31376\t best val loss: 2.31034\n",
      "CV_2 (16.51s) -\t train_loss: 2.31351\t val_loss: 2.31376\t val_acc: 0.10310\t val_recall: 0.10000\t val_f1: 0.01869\n",
      "\tearly stopping at ep.13\t train loss: 2.30706\t val loss: 2.34005\t best val loss: 2.31075\n",
      "CV_3 (9.10s) -\t train_loss: 2.30706\t val_loss: 2.34005\t val_acc: 0.09830\t val_recall: 0.10010\t val_f1: 0.01808\n",
      "\tearly stopping at ep.14\t train loss: 2.31337\t val loss: 2.32172\t best val loss: 2.31053\n",
      "CV_4 (9.57s) -\t train_loss: 2.31337\t val_loss: 2.32172\t val_acc: 0.09750\t val_recall: 0.10000\t val_f1: 0.01777\n",
      "\tearly stopping at ep.33\t train loss: 2.31067\t val loss: 2.32134\t best val loss: 2.31022\n",
      "CV_5 (23.40s) -\t train_loss: 2.31067\t val_loss: 2.32134\t val_acc: 0.10190\t val_recall: 0.10000\t val_f1: 0.01850\n",
      "## Experiment results (69.25s) - {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.01, 'optimiser': 'Adam', 'bn': False, 'batch_size': 16}\n",
      "# Metrics - train loss: 2.31071 \t val loss: 2.32372 \t val accuracy: 0.09922 \t val recall: 0.10002 \t val f1: 0.01809\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.01, 'optimiser': 'Adam', 'bn': False, 'batch_size': 8}\n",
      "\tearly stopping at ep.19\t train loss: 2.31541\t val loss: 2.31825\t best val loss: 2.31053\n",
      "CV_1 (23.57s) -\t train_loss: 2.31541\t val_loss: 2.31825\t val_acc: 0.10300\t val_recall: 0.10000\t val_f1: 0.01868\n",
      "\tearly stopping at ep.18\t train loss: 2.31390\t val loss: 2.32800\t best val loss: 2.31318\n",
      "CV_2 (22.82s) -\t train_loss: 2.31390\t val_loss: 2.32800\t val_acc: 0.10270\t val_recall: 0.10000\t val_f1: 0.01863\n",
      "\tearly stopping at ep.15\t train loss: 2.31274\t val loss: 2.31111\t best val loss: 2.31076\n",
      "CV_3 (18.92s) -\t train_loss: 2.31274\t val_loss: 2.31111\t val_acc: 0.10580\t val_recall: 0.10000\t val_f1: 0.01914\n",
      "\tearly stopping at ep.27\t train loss: 2.31281\t val loss: 2.31533\t best val loss: 2.30529\n",
      "CV_4 (34.09s) -\t train_loss: 2.31281\t val_loss: 2.31533\t val_acc: 0.09800\t val_recall: 0.10000\t val_f1: 0.01785\n",
      "\tearly stopping at ep.20\t train loss: 2.31101\t val loss: 2.32005\t best val loss: 2.30837\n",
      "CV_5 (24.90s) -\t train_loss: 2.31101\t val_loss: 2.32005\t val_acc: 0.10060\t val_recall: 0.10000\t val_f1: 0.01828\n",
      "## Experiment results (124.53s) - {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.01, 'optimiser': 'Adam', 'bn': False, 'batch_size': 8}\n",
      "# Metrics - train loss: 2.31318 \t val loss: 2.31855 \t val accuracy: 0.10202 \t val recall: 0.10000 \t val f1: 0.01851\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.01, 'optimiser': 'Adam', 'bn': False, 'batch_size': 4}\n",
      "\tearly stopping at ep.13\t train loss: 2.31496\t val loss: 2.31806\t best val loss: 2.30834\n",
      "CV_1 (32.30s) -\t train_loss: 2.31496\t val_loss: 2.31806\t val_acc: 0.10160\t val_recall: 0.10000\t val_f1: 0.01845\n",
      "\tearly stopping at ep.14\t train loss: 2.31378\t val loss: 2.32823\t best val loss: 2.30998\n",
      "CV_2 (33.15s) -\t train_loss: 2.31378\t val_loss: 2.32823\t val_acc: 0.09910\t val_recall: 0.10000\t val_f1: 0.01803\n",
      "\tearly stopping at ep.14\t train loss: 2.31419\t val loss: 2.30639\t best val loss: 2.30739\n",
      "CV_3 (34.80s) -\t train_loss: 2.31419\t val_loss: 2.30639\t val_acc: 0.10180\t val_recall: 0.10000\t val_f1: 0.01848\n",
      "\tearly stopping at ep.14\t train loss: 2.31529\t val loss: 2.31173\t best val loss: 2.30820\n",
      "CV_4 (33.14s) -\t train_loss: 2.31529\t val_loss: 2.31173\t val_acc: 0.10340\t val_recall: 0.10000\t val_f1: 0.01874\n",
      "\tearly stopping at ep.13\t train loss: 2.31341\t val loss: 2.31930\t best val loss: 2.31014\n",
      "CV_5 (31.92s) -\t train_loss: 2.31341\t val_loss: 2.31930\t val_acc: 0.10190\t val_recall: 0.10000\t val_f1: 0.01850\n",
      "## Experiment results (165.54s) - {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.01, 'optimiser': 'Adam', 'bn': False, 'batch_size': 4}\n",
      "# Metrics - train loss: 2.31433 \t val loss: 2.31674 \t val accuracy: 0.10156 \t val recall: 0.10000 \t val f1: 0.01844\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.01, 'optimiser': 'Adam', 'bn': False, 'batch_size': 2}\n",
      "\tearly stopping at ep.22\t train loss: 2.31171\t val loss: 2.30663\t best val loss: 2.30614\n",
      "CV_1 (103.50s) -\t train_loss: 2.31171\t val_loss: 2.30663\t val_acc: 0.10300\t val_recall: 0.10000\t val_f1: 0.01868\n",
      "\tearly stopping at ep.15\t train loss: 2.31210\t val loss: 2.32116\t best val loss: 2.30796\n",
      "CV_2 (68.40s) -\t train_loss: 2.31210\t val_loss: 2.32116\t val_acc: 0.09870\t val_recall: 0.10000\t val_f1: 0.01797\n",
      "\tearly stopping at ep.12\t train loss: 2.31120\t val loss: 2.31463\t best val loss: 2.30714\n",
      "CV_3 (56.14s) -\t train_loss: 2.31120\t val_loss: 2.31463\t val_acc: 0.10180\t val_recall: 0.10000\t val_f1: 0.01848\n",
      "\tearly stopping at ep.15\t train loss: 2.31230\t val loss: 2.31787\t best val loss: 2.30607\n",
      "CV_4 (68.88s) -\t train_loss: 2.31230\t val_loss: 2.31787\t val_acc: 0.09750\t val_recall: 0.10000\t val_f1: 0.01777\n",
      "\tearly stopping at ep.20\t train loss: 2.31231\t val loss: 2.30891\t best val loss: 2.30754\n",
      "CV_5 (91.29s) -\t train_loss: 2.31231\t val_loss: 2.30891\t val_acc: 0.09970\t val_recall: 0.10000\t val_f1: 0.01813\n",
      "## Experiment results (388.46s) - {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.01, 'optimiser': 'Adam', 'bn': False, 'batch_size': 2}\n",
      "# Metrics - train loss: 2.31192 \t val loss: 2.31384 \t val accuracy: 0.10014 \t val recall: 0.10000 \t val f1: 0.01820\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.01, 'optimiser': 'Adam', 'bn': True, 'batch_size': 16}\n",
      "\tearly stopping at ep.37\t train loss: 1.43135\t val loss: 1.68823\t best val loss: 1.65614\n",
      "CV_1 (48.79s) -\t train_loss: 1.43135\t val_loss: 1.68823\t val_acc: 0.41440\t val_recall: 0.41671\t val_f1: 0.39785\n",
      "CV_2 (65.95s) -\t train_loss: 1.39594\t val_loss: 1.75837\t val_acc: 0.39630\t val_recall: 0.39493\t val_f1: 0.36975\n",
      "\tearly stopping at ep.44\t train loss: 1.42952\t val loss: 1.73885\t best val loss: 1.64591\n",
      "CV_3 (58.35s) -\t train_loss: 1.42952\t val_loss: 1.73885\t val_acc: 0.40190\t val_recall: 0.40032\t val_f1: 0.38605\n",
      "CV_4 (65.59s) -\t train_loss: 1.41341\t val_loss: 1.66555\t val_acc: 0.42020\t val_recall: 0.41900\t val_f1: 0.41652\n",
      "CV_5 (66.30s) -\t train_loss: 1.43172\t val_loss: 1.65828\t val_acc: 0.41810\t val_recall: 0.41657\t val_f1: 0.40407\n",
      "## Experiment results (305.35s) - {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.01, 'optimiser': 'Adam', 'bn': True, 'batch_size': 16}\n",
      "# Metrics - train loss: 1.42039 \t val loss: 1.70186 \t val accuracy: 0.41018 \t val recall: 0.40950 \t val f1: 0.39485\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.01, 'optimiser': 'Adam', 'bn': True, 'batch_size': 8}\n",
      "\tearly stopping at ep.41\t train loss: 1.57976\t val loss: 1.56154\t best val loss: 1.52703\n",
      "CV_1 (99.54s) -\t train_loss: 1.57976\t val_loss: 1.56154\t val_acc: 0.43620\t val_recall: 0.43719\t val_f1: 0.41934\n",
      "\tearly stopping at ep.42\t train loss: 1.56440\t val loss: 1.54866\t best val loss: 1.51396\n",
      "CV_2 (101.22s) -\t train_loss: 1.56440\t val_loss: 1.54866\t val_acc: 0.44800\t val_recall: 0.44625\t val_f1: 0.41957\n",
      "\tearly stopping at ep.38\t train loss: 1.56401\t val loss: 1.53262\t best val loss: 1.53309\n",
      "CV_3 (92.52s) -\t train_loss: 1.56401\t val_loss: 1.53262\t val_acc: 0.44830\t val_recall: 0.44784\t val_f1: 0.43630\n",
      "\tearly stopping at ep.32\t train loss: 1.55812\t val loss: 1.58033\t best val loss: 1.53824\n",
      "CV_4 (77.15s) -\t train_loss: 1.55812\t val_loss: 1.58033\t val_acc: 0.43910\t val_recall: 0.43854\t val_f1: 0.42951\n",
      "\tearly stopping at ep.46\t train loss: 1.56860\t val loss: 1.54559\t best val loss: 1.52664\n",
      "CV_5 (111.40s) -\t train_loss: 1.56860\t val_loss: 1.54559\t val_acc: 0.45580\t val_recall: 0.45521\t val_f1: 0.44514\n",
      "## Experiment results (482.19s) - {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.01, 'optimiser': 'Adam', 'bn': True, 'batch_size': 8}\n",
      "# Metrics - train loss: 1.56698 \t val loss: 1.55375 \t val accuracy: 0.44548 \t val recall: 0.44500 \t val f1: 0.42997\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.01, 'optimiser': 'Adam', 'bn': True, 'batch_size': 4}\n",
      "CV_1 (234.38s) -\t train_loss: 1.87135\t val_loss: 1.65942\t val_acc: 0.42710\t val_recall: 0.42798\t val_f1: 0.41820\n",
      "\tearly stopping at ep.25\t train loss: 1.88971\t val loss: 1.65453\t best val loss: 1.65045\n",
      "CV_2 (115.82s) -\t train_loss: 1.88971\t val_loss: 1.65453\t val_acc: 0.42120\t val_recall: 0.41933\t val_f1: 0.40900\n",
      "CV_3 (231.38s) -\t train_loss: 1.85816\t val_loss: 1.66178\t val_acc: 0.40740\t val_recall: 0.40843\t val_f1: 0.38820\n",
      "\tearly stopping at ep.39\t train loss: 1.87195\t val loss: 1.64044\t best val loss: 1.64045\n",
      "CV_4 (179.66s) -\t train_loss: 1.87195\t val_loss: 1.64044\t val_acc: 0.42590\t val_recall: 0.42613\t val_f1: 0.41796\n",
      "\tearly stopping at ep.39\t train loss: 1.88285\t val loss: 1.65078\t best val loss: 1.64900\n",
      "CV_5 (180.26s) -\t train_loss: 1.88285\t val_loss: 1.65078\t val_acc: 0.42780\t val_recall: 0.42845\t val_f1: 0.42118\n",
      "## Experiment results (941.84s) - {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.01, 'optimiser': 'Adam', 'bn': True, 'batch_size': 4}\n",
      "# Metrics - train loss: 1.87480 \t val loss: 1.65339 \t val accuracy: 0.42188 \t val recall: 0.42206 \t val f1: 0.41091\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.01, 'optimiser': 'Adam', 'bn': True, 'batch_size': 2}\n",
      "\tearly stopping at ep.22\t train loss: 2.28298\t val loss: 2.27824\t best val loss: 2.21205\n",
      "CV_1 (197.70s) -\t train_loss: 2.28298\t val_loss: 2.27824\t val_acc: 0.15560\t val_recall: 0.15770\t val_f1: 0.09877\n",
      "\tearly stopping at ep.11\t train loss: 2.27657\t val loss: 2.25859\t best val loss: 2.21723\n",
      "CV_2 (99.55s) -\t train_loss: 2.27657\t val_loss: 2.25859\t val_acc: 0.15260\t val_recall: 0.15169\t val_f1: 0.09903\n",
      "\tearly stopping at ep.21\t train loss: 2.28399\t val loss: 2.25609\t best val loss: 2.21948\n",
      "CV_3 (189.04s) -\t train_loss: 2.28399\t val_loss: 2.25609\t val_acc: 0.15490\t val_recall: 0.15120\t val_f1: 0.09353\n",
      "\tearly stopping at ep.30\t train loss: 2.29350\t val loss: 2.27503\t best val loss: 2.21595\n",
      "CV_4 (273.18s) -\t train_loss: 2.29350\t val_loss: 2.27503\t val_acc: 0.14560\t val_recall: 0.14682\t val_f1: 0.06863\n",
      "\tearly stopping at ep.17\t train loss: 2.29706\t val loss: 2.29836\t best val loss: 2.21140\n",
      "CV_5 (153.18s) -\t train_loss: 2.29706\t val_loss: 2.29836\t val_acc: 0.12600\t val_recall: 0.12645\t val_f1: 0.07087\n",
      "## Experiment results (913.01s) - {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.01, 'optimiser': 'Adam', 'bn': True, 'batch_size': 2}\n",
      "# Metrics - train loss: 2.28682 \t val loss: 2.27326 \t val accuracy: 0.14694 \t val recall: 0.14677 \t val f1: 0.08617\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.01, 'optimiser': 'Momentum', 'bn': False, 'batch_size': 16}\n",
      "\tearly stopping at ep.13\t train loss: 2.35752\t val loss: 2.69024\t best val loss: 2.46470\n",
      "CV_1 (7.01s) -\t train_loss: 2.35752\t val_loss: 2.69024\t val_acc: 0.10160\t val_recall: 0.10000\t val_f1: 0.01845\n",
      "\tearly stopping at ep.11\t train loss: 2.36234\t val loss: 2.60606\t best val loss: 2.41661\n",
      "CV_2 (5.65s) -\t train_loss: 2.36234\t val_loss: 2.60606\t val_acc: 0.10390\t val_recall: 0.10000\t val_f1: 0.01882\n",
      "\tearly stopping at ep.23\t train loss: 2.36315\t val loss: 2.55421\t best val loss: 2.36672\n",
      "CV_3 (13.15s) -\t train_loss: 2.36315\t val_loss: 2.55421\t val_acc: 0.09600\t val_recall: 0.10000\t val_f1: 0.01752\n",
      "\tearly stopping at ep.31\t train loss: 2.37122\t val loss: 2.56418\t best val loss: 2.39535\n",
      "CV_4 (18.44s) -\t train_loss: 2.37122\t val_loss: 2.56418\t val_acc: 0.10080\t val_recall: 0.10000\t val_f1: 0.01831\n",
      "\tearly stopping at ep.15\t train loss: 2.35927\t val loss: 2.97201\t best val loss: 2.39871\n",
      "CV_5 (8.03s) -\t train_loss: 2.35927\t val_loss: 2.97201\t val_acc: 0.09790\t val_recall: 0.10000\t val_f1: 0.01783\n",
      "## Experiment results (52.52s) - {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.01, 'optimiser': 'Momentum', 'bn': False, 'batch_size': 16}\n",
      "# Metrics - train loss: 2.36270 \t val loss: 2.67734 \t val accuracy: 0.10004 \t val recall: 0.10000 \t val f1: 0.01819\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.01, 'optimiser': 'Momentum', 'bn': False, 'batch_size': 8}\n",
      "\tearly stopping at ep.14\t train loss: 2.39498\t val loss: 2.48895\t best val loss: 2.40897\n",
      "CV_1 (11.54s) -\t train_loss: 2.39498\t val_loss: 2.48895\t val_acc: 0.09670\t val_recall: 0.10000\t val_f1: 0.01763\n",
      "\tearly stopping at ep.32\t train loss: 2.40175\t val loss: 2.56743\t best val loss: 2.35990\n",
      "CV_2 (26.01s) -\t train_loss: 2.40175\t val_loss: 2.56743\t val_acc: 0.10020\t val_recall: 0.10000\t val_f1: 0.01821\n",
      "\tearly stopping at ep.11\t train loss: 2.39429\t val loss: 2.50211\t best val loss: 2.40413\n",
      "CV_3 (9.05s) -\t train_loss: 2.39429\t val_loss: 2.50211\t val_acc: 0.09790\t val_recall: 0.10000\t val_f1: 0.01783\n",
      "\tearly stopping at ep.16\t train loss: 2.40505\t val loss: 2.44810\t best val loss: 2.40042\n",
      "CV_4 (14.55s) -\t train_loss: 2.40505\t val_loss: 2.44810\t val_acc: 0.09750\t val_recall: 0.10000\t val_f1: 0.01777\n",
      "\tearly stopping at ep.17\t train loss: 2.39734\t val loss: 2.50794\t best val loss: 2.41346\n",
      "CV_5 (14.83s) -\t train_loss: 2.39734\t val_loss: 2.50794\t val_acc: 0.09470\t val_recall: 0.10000\t val_f1: 0.01730\n",
      "## Experiment results (76.23s) - {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.01, 'optimiser': 'Momentum', 'bn': False, 'batch_size': 8}\n",
      "# Metrics - train loss: 2.39868 \t val loss: 2.50291 \t val accuracy: 0.09740 \t val recall: 0.10000 \t val f1: 0.01775\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.01, 'optimiser': 'Momentum', 'bn': False, 'batch_size': 4}\n",
      "\tearly stopping at ep.13\t train loss: 2.37292\t val loss: 2.39287\t best val loss: 2.32639\n",
      "CV_1 (19.27s) -\t train_loss: 2.37292\t val_loss: 2.39287\t val_acc: 0.10160\t val_recall: 0.10000\t val_f1: 0.01845\n",
      "\tearly stopping at ep.14\t train loss: 2.37335\t val loss: 2.35402\t best val loss: 2.35059\n",
      "CV_2 (21.04s) -\t train_loss: 2.37335\t val_loss: 2.35402\t val_acc: 0.09650\t val_recall: 0.10000\t val_f1: 0.01760\n",
      "\tearly stopping at ep.17\t train loss: 2.37149\t val loss: 2.37728\t best val loss: 2.35528\n",
      "CV_3 (25.08s) -\t train_loss: 2.37149\t val_loss: 2.37728\t val_acc: 0.10180\t val_recall: 0.10000\t val_f1: 0.01848\n",
      "\tearly stopping at ep.31\t train loss: 2.37091\t val loss: 2.33497\t best val loss: 2.32102\n",
      "CV_4 (49.73s) -\t train_loss: 2.37091\t val_loss: 2.33497\t val_acc: 0.09770\t val_recall: 0.10000\t val_f1: 0.01780\n",
      "\tearly stopping at ep.15\t train loss: 2.36771\t val loss: 2.36025\t best val loss: 2.32828\n",
      "CV_5 (24.04s) -\t train_loss: 2.36771\t val_loss: 2.36025\t val_acc: 0.10060\t val_recall: 0.10000\t val_f1: 0.01828\n",
      "## Experiment results (139.38s) - {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.01, 'optimiser': 'Momentum', 'bn': False, 'batch_size': 4}\n",
      "# Metrics - train loss: 2.37127 \t val loss: 2.36388 \t val accuracy: 0.09964 \t val recall: 0.10000 \t val f1: 0.01812\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.01, 'optimiser': 'Momentum', 'bn': False, 'batch_size': 2}\n",
      "\tearly stopping at ep.11\t train loss: 2.34320\t val loss: 2.38065\t best val loss: 2.31722\n",
      "CV_1 (32.18s) -\t train_loss: 2.34320\t val_loss: 2.38065\t val_acc: 0.09530\t val_recall: 0.10000\t val_f1: 0.01740\n",
      "\tearly stopping at ep.15\t train loss: 2.34249\t val loss: 2.36040\t best val loss: 2.31720\n",
      "CV_2 (43.27s) -\t train_loss: 2.34249\t val_loss: 2.36040\t val_acc: 0.09650\t val_recall: 0.10000\t val_f1: 0.01760\n",
      "\tearly stopping at ep.14\t train loss: 2.34376\t val loss: 2.33137\t best val loss: 2.31902\n",
      "CV_3 (39.95s) -\t train_loss: 2.34376\t val_loss: 2.33137\t val_acc: 0.09790\t val_recall: 0.10000\t val_f1: 0.01783\n",
      "\tearly stopping at ep.40\t train loss: 2.34266\t val loss: 2.33460\t best val loss: 2.31071\n",
      "CV_4 (114.22s) -\t train_loss: 2.34266\t val_loss: 2.33460\t val_acc: 0.09770\t val_recall: 0.10000\t val_f1: 0.01780\n",
      "\tearly stopping at ep.11\t train loss: 2.34467\t val loss: 2.34096\t best val loss: 2.31850\n",
      "CV_5 (32.52s) -\t train_loss: 2.34467\t val_loss: 2.34096\t val_acc: 0.10590\t val_recall: 0.10000\t val_f1: 0.01915\n",
      "## Experiment results (262.37s) - {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.01, 'optimiser': 'Momentum', 'bn': False, 'batch_size': 2}\n",
      "# Metrics - train loss: 2.34336 \t val loss: 2.34960 \t val accuracy: 0.09866 \t val recall: 0.10000 \t val f1: 0.01796\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.01, 'optimiser': 'Momentum', 'bn': True, 'batch_size': 16}\n",
      "\tearly stopping at ep.37\t train loss: 1.43550\t val loss: 1.74437\t best val loss: 1.66359\n",
      "CV_1 (40.51s) -\t train_loss: 1.43550\t val_loss: 1.74437\t val_acc: 0.38480\t val_recall: 0.38511\t val_f1: 0.37663\n",
      "\tearly stopping at ep.30\t train loss: 1.45965\t val loss: 1.72901\t best val loss: 1.69008\n",
      "CV_2 (33.08s) -\t train_loss: 1.45965\t val_loss: 1.72901\t val_acc: 0.39190\t val_recall: 0.39058\t val_f1: 0.38803\n",
      "\tearly stopping at ep.26\t train loss: 1.46542\t val loss: 1.76571\t best val loss: 1.70709\n",
      "CV_3 (29.14s) -\t train_loss: 1.46542\t val_loss: 1.76571\t val_acc: 0.38330\t val_recall: 0.38400\t val_f1: 0.35539\n",
      "CV_4 (55.34s) -\t train_loss: 1.46985\t val_loss: 1.71289\t val_acc: 0.40640\t val_recall: 0.40798\t val_f1: 0.40492\n",
      "\tearly stopping at ep.39\t train loss: 1.46987\t val loss: 1.79665\t best val loss: 1.67673\n",
      "CV_5 (42.55s) -\t train_loss: 1.46987\t val_loss: 1.79665\t val_acc: 0.37880\t val_recall: 0.38006\t val_f1: 0.36108\n",
      "## Experiment results (200.97s) - {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.01, 'optimiser': 'Momentum', 'bn': True, 'batch_size': 16}\n",
      "# Metrics - train loss: 1.46006 \t val loss: 1.74973 \t val accuracy: 0.38904 \t val recall: 0.38955 \t val f1: 0.37721\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.01, 'optimiser': 'Momentum', 'bn': True, 'batch_size': 8}\n",
      "\tearly stopping at ep.28\t train loss: 1.65290\t val loss: 1.69053\t best val loss: 1.59830\n",
      "CV_1 (56.77s) -\t train_loss: 1.65290\t val_loss: 1.69053\t val_acc: 0.39690\t val_recall: 0.39771\t val_f1: 0.38072\n",
      "\tearly stopping at ep.37\t train loss: 1.63633\t val loss: 1.59568\t best val loss: 1.57498\n",
      "CV_2 (73.57s) -\t train_loss: 1.63633\t val_loss: 1.59568\t val_acc: 0.43510\t val_recall: 0.43446\t val_f1: 0.42973\n",
      "\tearly stopping at ep.44\t train loss: 1.61928\t val loss: 1.60052\t best val loss: 1.56064\n",
      "CV_3 (87.28s) -\t train_loss: 1.61928\t val_loss: 1.60052\t val_acc: 0.43360\t val_recall: 0.43454\t val_f1: 0.42122\n",
      "\tearly stopping at ep.36\t train loss: 1.63092\t val loss: 1.61942\t best val loss: 1.59500\n",
      "CV_4 (73.67s) -\t train_loss: 1.63092\t val_loss: 1.61942\t val_acc: 0.42180\t val_recall: 0.42101\t val_f1: 0.40879\n",
      "\tearly stopping at ep.35\t train loss: 1.65865\t val loss: 1.59559\t best val loss: 1.59657\n",
      "CV_5 (69.07s) -\t train_loss: 1.65865\t val_loss: 1.59559\t val_acc: 0.42250\t val_recall: 0.42291\t val_f1: 0.39634\n",
      "## Experiment results (360.72s) - {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.01, 'optimiser': 'Momentum', 'bn': True, 'batch_size': 8}\n",
      "# Metrics - train loss: 1.63962 \t val loss: 1.62035 \t val accuracy: 0.42198 \t val recall: 0.42213 \t val f1: 0.40736\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.01, 'optimiser': 'Momentum', 'bn': True, 'batch_size': 4}\n",
      "\tearly stopping at ep.38\t train loss: 1.93128\t val loss: 1.72429\t best val loss: 1.69010\n",
      "CV_1 (144.48s) -\t train_loss: 1.93128\t val_loss: 1.72429\t val_acc: 0.39440\t val_recall: 0.39348\t val_f1: 0.38440\n",
      "\tearly stopping at ep.29\t train loss: 1.93272\t val loss: 1.71946\t best val loss: 1.69945\n",
      "CV_2 (110.03s) -\t train_loss: 1.93272\t val_loss: 1.71946\t val_acc: 0.39640\t val_recall: 0.39420\t val_f1: 0.38915\n",
      "\tearly stopping at ep.21\t train loss: 1.93020\t val loss: 1.76062\t best val loss: 1.72181\n",
      "CV_3 (79.79s) -\t train_loss: 1.93020\t val_loss: 1.76062\t val_acc: 0.37020\t val_recall: 0.37160\t val_f1: 0.35629\n",
      "\tearly stopping at ep.26\t train loss: 1.92231\t val loss: 1.69695\t best val loss: 1.69593\n",
      "CV_4 (98.22s) -\t train_loss: 1.92231\t val_loss: 1.69695\t val_acc: 0.41320\t val_recall: 0.41272\t val_f1: 0.40672\n",
      "\tearly stopping at ep.22\t train loss: 1.94102\t val loss: 1.75001\t best val loss: 1.72481\n",
      "CV_5 (83.05s) -\t train_loss: 1.94102\t val_loss: 1.75001\t val_acc: 0.37840\t val_recall: 0.37709\t val_f1: 0.36409\n",
      "## Experiment results (515.92s) - {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.01, 'optimiser': 'Momentum', 'bn': True, 'batch_size': 4}\n",
      "# Metrics - train loss: 1.93151 \t val loss: 1.73027 \t val accuracy: 0.39052 \t val recall: 0.38982 \t val f1: 0.38013\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.01, 'optimiser': 'Momentum', 'bn': True, 'batch_size': 2}\n",
      "\tearly stopping at ep.15\t train loss: 2.29293\t val loss: 2.27707\t best val loss: 2.24345\n",
      "CV_1 (110.91s) -\t train_loss: 2.29293\t val_loss: 2.27707\t val_acc: 0.13190\t val_recall: 0.13270\t val_f1: 0.07907\n",
      "\tearly stopping at ep.16\t train loss: 2.27634\t val loss: 2.24569\t best val loss: 2.22706\n",
      "CV_2 (118.07s) -\t train_loss: 2.27634\t val_loss: 2.24569\t val_acc: 0.14950\t val_recall: 0.15180\t val_f1: 0.10155\n",
      "\tearly stopping at ep.27\t train loss: 2.30299\t val loss: 2.30752\t best val loss: 2.25261\n",
      "CV_3 (198.30s) -\t train_loss: 2.30299\t val_loss: 2.30752\t val_acc: 0.08720\t val_recall: 0.08895\t val_f1: 0.01717\n",
      "\tearly stopping at ep.11\t train loss: 2.28786\t val loss: 2.26685\t best val loss: 2.23575\n",
      "CV_4 (81.77s) -\t train_loss: 2.28786\t val_loss: 2.26685\t val_acc: 0.15750\t val_recall: 0.15837\t val_f1: 0.08439\n",
      "\tearly stopping at ep.11\t train loss: 2.27896\t val loss: 2.28844\t best val loss: 2.26189\n",
      "CV_5 (80.98s) -\t train_loss: 2.27896\t val_loss: 2.28844\t val_acc: 0.13620\t val_recall: 0.13608\t val_f1: 0.09558\n",
      "## Experiment results (590.29s) - {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.01, 'optimiser': 'Momentum', 'bn': True, 'batch_size': 2}\n",
      "# Metrics - train loss: 2.28782 \t val loss: 2.27712 \t val accuracy: 0.13246 \t val recall: 0.13358 \t val f1: 0.07555\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 96, 64, 10], 'lr': 0.001, 'optimiser': None, 'bn': False, 'batch_size': 16}\n",
      "\tearly stopping at ep.30\t train loss: 0.41819\t val loss: 2.67758\t best val loss: 2.45093\n",
      "CV_1 (17.44s) -\t train_loss: 0.41819\t val_loss: 2.67758\t val_acc: 0.38410\t val_recall: 0.38421\t val_f1: 0.38114\n",
      "\tearly stopping at ep.11\t train loss: 0.47353\t val loss: 2.93695\t best val loss: 2.53792\n",
      "CV_2 (6.78s) -\t train_loss: 0.47353\t val_loss: 2.93695\t val_acc: 0.36110\t val_recall: 0.36280\t val_f1: 0.35502\n",
      "\tearly stopping at ep.11\t train loss: 0.44255\t val loss: 2.81146\t best val loss: 2.66775\n",
      "CV_3 (6.55s) -\t train_loss: 0.44255\t val_loss: 2.81146\t val_acc: 0.36640\t val_recall: 0.36579\t val_f1: 0.35022\n",
      "\tearly stopping at ep.23\t train loss: 0.43913\t val loss: 2.67629\t best val loss: 2.59289\n",
      "CV_4 (15.90s) -\t train_loss: 0.43913\t val_loss: 2.67629\t val_acc: 0.38120\t val_recall: 0.38057\t val_f1: 0.37458\n",
      "\tearly stopping at ep.11\t train loss: 0.47605\t val loss: 3.14183\t best val loss: 2.44687\n",
      "CV_5 (6.80s) -\t train_loss: 0.47605\t val_loss: 3.14183\t val_acc: 0.33250\t val_recall: 0.33428\t val_f1: 0.32014\n",
      "## Experiment results (53.77s) - {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 96, 64, 10], 'lr': 0.001, 'optimiser': None, 'bn': False, 'batch_size': 16}\n",
      "# Metrics - train loss: 0.44989 \t val loss: 2.84882 \t val accuracy: 0.36506 \t val recall: 0.36553 \t val f1: 0.35622\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 96, 64, 10], 'lr': 0.001, 'optimiser': None, 'bn': False, 'batch_size': 8}\n",
      "\tearly stopping at ep.40\t train loss: 0.60555\t val loss: 2.16945\t best val loss: 1.96969\n",
      "CV_1 (36.59s) -\t train_loss: 0.60555\t val_loss: 2.16945\t val_acc: 0.43290\t val_recall: 0.43266\t val_f1: 0.42999\n",
      "\tearly stopping at ep.18\t train loss: 0.66129\t val loss: 2.30091\t best val loss: 2.04197\n",
      "CV_2 (16.07s) -\t train_loss: 0.66129\t val_loss: 2.30091\t val_acc: 0.40050\t val_recall: 0.39850\t val_f1: 0.39119\n",
      "\tearly stopping at ep.19\t train loss: 0.66399\t val loss: 2.20057\t best val loss: 1.98658\n",
      "CV_3 (16.98s) -\t train_loss: 0.66399\t val_loss: 2.20057\t val_acc: 0.40250\t val_recall: 0.40254\t val_f1: 0.39916\n",
      "\tearly stopping at ep.11\t train loss: 0.73800\t val loss: 2.22511\t best val loss: 2.06264\n",
      "CV_4 (9.72s) -\t train_loss: 0.73800\t val_loss: 2.22511\t val_acc: 0.40810\t val_recall: 0.40707\t val_f1: 0.39447\n",
      "\tearly stopping at ep.18\t train loss: 0.70387\t val loss: 2.07760\t best val loss: 2.00339\n",
      "CV_5 (15.80s) -\t train_loss: 0.70387\t val_loss: 2.07760\t val_acc: 0.42350\t val_recall: 0.42313\t val_f1: 0.42143\n",
      "## Experiment results (95.41s) - {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 96, 64, 10], 'lr': 0.001, 'optimiser': None, 'bn': False, 'batch_size': 8}\n",
      "# Metrics - train loss: 0.67454 \t val loss: 2.19473 \t val accuracy: 0.41350 \t val recall: 0.41278 \t val f1: 0.40725\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 96, 64, 10], 'lr': 0.001, 'optimiser': None, 'bn': False, 'batch_size': 4}\n",
      "\tearly stopping at ep.40\t train loss: 0.88748\t val loss: 1.61856\t best val loss: 1.56298\n",
      "CV_1 (67.30s) -\t train_loss: 0.88748\t val_loss: 1.61856\t val_acc: 0.47330\t val_recall: 0.47489\t val_f1: 0.46610\n",
      "\tearly stopping at ep.25\t train loss: 0.92346\t val loss: 1.58763\t best val loss: 1.58535\n",
      "CV_2 (41.84s) -\t train_loss: 0.92346\t val_loss: 1.58763\t val_acc: 0.47850\t val_recall: 0.47683\t val_f1: 0.47046\n",
      "\tearly stopping at ep.28\t train loss: 0.90562\t val loss: 1.66764\t best val loss: 1.61198\n",
      "CV_3 (48.12s) -\t train_loss: 0.90562\t val_loss: 1.66764\t val_acc: 0.45290\t val_recall: 0.45360\t val_f1: 0.44359\n",
      "\tearly stopping at ep.29\t train loss: 0.89205\t val loss: 1.66350\t best val loss: 1.61111\n",
      "CV_4 (48.87s) -\t train_loss: 0.89205\t val_loss: 1.66350\t val_acc: 0.45880\t val_recall: 0.45708\t val_f1: 0.45534\n",
      "\tearly stopping at ep.20\t train loss: 0.95265\t val loss: 1.66595\t best val loss: 1.61389\n",
      "CV_5 (31.75s) -\t train_loss: 0.95265\t val_loss: 1.66595\t val_acc: 0.45120\t val_recall: 0.45264\t val_f1: 0.44620\n",
      "## Experiment results (238.18s) - {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 96, 64, 10], 'lr': 0.001, 'optimiser': None, 'bn': False, 'batch_size': 4}\n",
      "# Metrics - train loss: 0.91225 \t val loss: 1.64066 \t val accuracy: 0.46294 \t val recall: 0.46301 \t val f1: 0.45634\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 96, 64, 10], 'lr': 0.001, 'optimiser': None, 'bn': False, 'batch_size': 2}\n",
      "\tearly stopping at ep.36\t train loss: 1.19982\t val loss: 1.42895\t best val loss: 1.40689\n",
      "CV_1 (107.11s) -\t train_loss: 1.19982\t val_loss: 1.42895\t val_acc: 0.50270\t val_recall: 0.50355\t val_f1: 0.49990\n",
      "\tearly stopping at ep.34\t train loss: 1.17690\t val loss: 1.51279\t best val loss: 1.41131\n",
      "CV_2 (101.09s) -\t train_loss: 1.17690\t val_loss: 1.51279\t val_acc: 0.47810\t val_recall: 0.47726\t val_f1: 0.47829\n",
      "\tearly stopping at ep.46\t train loss: 1.16652\t val loss: 1.44856\t best val loss: 1.42376\n",
      "CV_3 (137.87s) -\t train_loss: 1.16652\t val_loss: 1.44856\t val_acc: 0.48670\t val_recall: 0.48678\t val_f1: 0.48458\n",
      "\tearly stopping at ep.21\t train loss: 1.21588\t val loss: 1.48818\t best val loss: 1.44070\n",
      "CV_4 (65.49s) -\t train_loss: 1.21588\t val_loss: 1.48818\t val_acc: 0.47770\t val_recall: 0.47760\t val_f1: 0.47249\n",
      "CV_5 (151.34s) -\t train_loss: 1.16473\t val_loss: 1.46077\t val_acc: 0.49550\t val_recall: 0.49528\t val_f1: 0.48361\n",
      "## Experiment results (563.18s) - {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 96, 64, 10], 'lr': 0.001, 'optimiser': None, 'bn': False, 'batch_size': 2}\n",
      "# Metrics - train loss: 1.18477 \t val loss: 1.46785 \t val accuracy: 0.48814 \t val recall: 0.48810 \t val f1: 0.48377\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 96, 64, 10], 'lr': 0.001, 'optimiser': None, 'bn': True, 'batch_size': 16}\n",
      "\tearly stopping at ep.11\t train loss: 0.55667\t val loss: 2.85118\t best val loss: 2.75218\n",
      "CV_1 (14.88s) -\t train_loss: 0.55667\t val_loss: 2.85118\t val_acc: 0.30800\t val_recall: 0.30819\t val_f1: 0.30561\n",
      "\tearly stopping at ep.11\t train loss: 0.53824\t val loss: 3.10341\t best val loss: 2.77650\n",
      "CV_2 (14.82s) -\t train_loss: 0.53824\t val_loss: 3.10341\t val_acc: 0.28480\t val_recall: 0.28384\t val_f1: 0.27885\n",
      "\tearly stopping at ep.11\t train loss: 0.53421\t val loss: 3.17212\t best val loss: 2.60234\n",
      "CV_3 (15.30s) -\t train_loss: 0.53421\t val_loss: 3.17212\t val_acc: 0.30180\t val_recall: 0.30237\t val_f1: 0.29333\n",
      "\tearly stopping at ep.11\t train loss: 0.55986\t val loss: 3.14357\t best val loss: 2.69115\n",
      "CV_4 (16.24s) -\t train_loss: 0.55986\t val_loss: 3.14357\t val_acc: 0.29090\t val_recall: 0.29065\t val_f1: 0.29036\n",
      "\tearly stopping at ep.11\t train loss: 0.54852\t val loss: 3.07680\t best val loss: 2.66994\n",
      "CV_5 (15.44s) -\t train_loss: 0.54852\t val_loss: 3.07680\t val_acc: 0.30790\t val_recall: 0.30766\t val_f1: 0.30604\n",
      "## Experiment results (77.14s) - {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 96, 64, 10], 'lr': 0.001, 'optimiser': None, 'bn': True, 'batch_size': 16}\n",
      "# Metrics - train loss: 0.54750 \t val loss: 3.06941 \t val accuracy: 0.29868 \t val recall: 0.29854 \t val f1: 0.29484\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 96, 64, 10], 'lr': 0.001, 'optimiser': None, 'bn': True, 'batch_size': 8}\n",
      "\tearly stopping at ep.17\t train loss: 1.26122\t val loss: 2.02539\t best val loss: 1.89541\n",
      "CV_1 (37.01s) -\t train_loss: 1.26122\t val_loss: 2.02539\t val_acc: 0.34420\t val_recall: 0.34529\t val_f1: 0.33829\n",
      "\tearly stopping at ep.17\t train loss: 1.24280\t val loss: 2.02468\t best val loss: 1.91259\n",
      "CV_2 (37.54s) -\t train_loss: 1.24280\t val_loss: 2.02468\t val_acc: 0.34300\t val_recall: 0.34303\t val_f1: 0.34072\n",
      "\tearly stopping at ep.32\t train loss: 1.22852\t val loss: 2.07565\t best val loss: 1.90605\n",
      "CV_3 (73.37s) -\t train_loss: 1.22852\t val_loss: 2.07565\t val_acc: 0.33740\t val_recall: 0.33714\t val_f1: 0.33260\n",
      "\tearly stopping at ep.33\t train loss: 1.26011\t val loss: 1.99820\t best val loss: 1.88934\n",
      "CV_4 (74.79s) -\t train_loss: 1.26011\t val_loss: 1.99820\t val_acc: 0.35070\t val_recall: 0.35145\t val_f1: 0.34197\n",
      "\tearly stopping at ep.28\t train loss: 1.21550\t val loss: 2.07923\t best val loss: 1.83987\n",
      "CV_5 (64.42s) -\t train_loss: 1.21550\t val_loss: 2.07923\t val_acc: 0.35010\t val_recall: 0.34999\t val_f1: 0.34487\n",
      "## Experiment results (287.55s) - {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 96, 64, 10], 'lr': 0.001, 'optimiser': None, 'bn': True, 'batch_size': 8}\n",
      "# Metrics - train loss: 1.24163 \t val loss: 2.04063 \t val accuracy: 0.34508 \t val recall: 0.34538 \t val f1: 0.33969\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 96, 64, 10], 'lr': 0.001, 'optimiser': None, 'bn': True, 'batch_size': 4}\n",
      "\tearly stopping at ep.36\t train loss: 1.86443\t val loss: 1.69288\t best val loss: 1.67148\n",
      "CV_1 (144.99s) -\t train_loss: 1.86443\t val_loss: 1.69288\t val_acc: 0.41240\t val_recall: 0.41323\t val_f1: 0.40933\n",
      "\tearly stopping at ep.24\t train loss: 1.85656\t val loss: 1.68366\t best val loss: 1.68395\n",
      "CV_2 (93.67s) -\t train_loss: 1.85656\t val_loss: 1.68366\t val_acc: 0.41420\t val_recall: 0.41286\t val_f1: 0.40663\n",
      "\tearly stopping at ep.40\t train loss: 1.85930\t val loss: 1.70424\t best val loss: 1.68740\n",
      "CV_3 (156.16s) -\t train_loss: 1.85930\t val_loss: 1.70424\t val_acc: 0.39300\t val_recall: 0.39416\t val_f1: 0.38498\n",
      "\tearly stopping at ep.29\t train loss: 1.87152\t val loss: 1.72615\t best val loss: 1.66021\n",
      "CV_4 (112.56s) -\t train_loss: 1.87152\t val_loss: 1.72615\t val_acc: 0.39080\t val_recall: 0.39129\t val_f1: 0.37954\n",
      "\tearly stopping at ep.40\t train loss: 1.84289\t val loss: 1.71634\t best val loss: 1.67828\n",
      "CV_5 (157.06s) -\t train_loss: 1.84289\t val_loss: 1.71634\t val_acc: 0.40030\t val_recall: 0.39919\t val_f1: 0.39394\n",
      "## Experiment results (664.95s) - {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 96, 64, 10], 'lr': 0.001, 'optimiser': None, 'bn': True, 'batch_size': 4}\n",
      "# Metrics - train loss: 1.85894 \t val loss: 1.70465 \t val accuracy: 0.40214 \t val recall: 0.40215 \t val f1: 0.39488\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 96, 64, 10], 'lr': 0.001, 'optimiser': None, 'bn': True, 'batch_size': 2}\n",
      "\tearly stopping at ep.27\t train loss: 2.28941\t val loss: 2.25705\t best val loss: 2.14040\n",
      "CV_1 (202.10s) -\t train_loss: 2.28941\t val_loss: 2.25705\t val_acc: 0.14030\t val_recall: 0.14273\t val_f1: 0.05774\n",
      "\tearly stopping at ep.30\t train loss: 2.30255\t val loss: 2.30305\t best val loss: 2.18607\n",
      "CV_2 (224.99s) -\t train_loss: 2.30255\t val_loss: 2.30305\t val_acc: 0.09970\t val_recall: 0.10000\t val_f1: 0.01813\n",
      "\tearly stopping at ep.31\t train loss: 2.30232\t val loss: 2.30390\t best val loss: 2.15932\n",
      "CV_3 (232.30s) -\t train_loss: 2.30232\t val_loss: 2.30390\t val_acc: 0.09600\t val_recall: 0.10000\t val_f1: 0.01752\n",
      "\tearly stopping at ep.28\t train loss: 2.28750\t val loss: 2.26881\t best val loss: 2.14637\n",
      "CV_4 (210.29s) -\t train_loss: 2.28750\t val_loss: 2.26881\t val_acc: 0.15230\t val_recall: 0.15367\t val_f1: 0.06821\n",
      "\tearly stopping at ep.37\t train loss: 2.30255\t val loss: 2.30301\t best val loss: 2.14774\n",
      "CV_5 (276.44s) -\t train_loss: 2.30255\t val_loss: 2.30301\t val_acc: 0.09470\t val_recall: 0.10000\t val_f1: 0.01730\n",
      "## Experiment results (1146.59s) - {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 96, 64, 10], 'lr': 0.001, 'optimiser': None, 'bn': True, 'batch_size': 2}\n",
      "# Metrics - train loss: 2.29686 \t val loss: 2.28717 \t val accuracy: 0.11660 \t val recall: 0.11928 \t val f1: 0.03578\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 96, 64, 10], 'lr': 0.001, 'optimiser': 'Adam', 'bn': False, 'batch_size': 16}\n",
      "\tearly stopping at ep.29\t train loss: 1.52164\t val loss: 2.09423\t best val loss: 1.96926\n",
      "CV_1 (25.18s) -\t train_loss: 1.52164\t val_loss: 2.09423\t val_acc: 0.34550\t val_recall: 0.34460\t val_f1: 0.33731\n",
      "\tearly stopping at ep.24\t train loss: 1.39230\t val loss: 2.35158\t best val loss: 2.01732\n",
      "CV_2 (21.03s) -\t train_loss: 1.39230\t val_loss: 2.35158\t val_acc: 0.33270\t val_recall: 0.33287\t val_f1: 0.31973\n",
      "\tearly stopping at ep.40\t train loss: 1.45512\t val loss: 2.05257\t best val loss: 2.00143\n",
      "CV_3 (35.77s) -\t train_loss: 1.45512\t val_loss: 2.05257\t val_acc: 0.36480\t val_recall: 0.36412\t val_f1: 0.36539\n",
      "\tearly stopping at ep.28\t train loss: 1.50860\t val loss: 2.08967\t best val loss: 2.01400\n",
      "CV_4 (27.71s) -\t train_loss: 1.50860\t val_loss: 2.08967\t val_acc: 0.33990\t val_recall: 0.33775\t val_f1: 0.33749\n",
      "\tearly stopping at ep.38\t train loss: 1.43267\t val loss: 1.97335\t best val loss: 1.91282\n",
      "CV_5 (33.15s) -\t train_loss: 1.43267\t val_loss: 1.97335\t val_acc: 0.34070\t val_recall: 0.33978\t val_f1: 0.32367\n",
      "## Experiment results (143.14s) - {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 96, 64, 10], 'lr': 0.001, 'optimiser': 'Adam', 'bn': False, 'batch_size': 16}\n",
      "# Metrics - train loss: 1.46207 \t val loss: 2.11228 \t val accuracy: 0.34472 \t val recall: 0.34382 \t val f1: 0.33672\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 96, 64, 10], 'lr': 0.001, 'optimiser': 'Adam', 'bn': False, 'batch_size': 8}\n",
      "\tearly stopping at ep.41\t train loss: 1.45672\t val loss: 1.82008\t best val loss: 1.74316\n",
      "CV_1 (67.22s) -\t train_loss: 1.45672\t val_loss: 1.82008\t val_acc: 0.40760\t val_recall: 0.40739\t val_f1: 0.39764\n",
      "\tearly stopping at ep.43\t train loss: 1.44235\t val loss: 1.85919\t best val loss: 1.77220\n",
      "CV_2 (66.36s) -\t train_loss: 1.44235\t val_loss: 1.85919\t val_acc: 0.39100\t val_recall: 0.39072\t val_f1: 0.37565\n",
      "\tearly stopping at ep.16\t train loss: 1.47684\t val loss: 1.79740\t best val loss: 1.79035\n",
      "CV_3 (24.42s) -\t train_loss: 1.47684\t val_loss: 1.79740\t val_acc: 0.38130\t val_recall: 0.37977\t val_f1: 0.37621\n",
      "\tearly stopping at ep.35\t train loss: 1.45254\t val loss: 1.76811\t best val loss: 1.75136\n",
      "CV_4 (54.40s) -\t train_loss: 1.45254\t val_loss: 1.76811\t val_acc: 0.39580\t val_recall: 0.39378\t val_f1: 0.38922\n",
      "\tearly stopping at ep.39\t train loss: 1.45881\t val loss: 1.78625\t best val loss: 1.78574\n",
      "CV_5 (59.25s) -\t train_loss: 1.45881\t val_loss: 1.78625\t val_acc: 0.39310\t val_recall: 0.39416\t val_f1: 0.38292\n",
      "## Experiment results (271.94s) - {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 96, 64, 10], 'lr': 0.001, 'optimiser': 'Adam', 'bn': False, 'batch_size': 8}\n",
      "# Metrics - train loss: 1.45745 \t val loss: 1.80620 \t val accuracy: 0.39376 \t val recall: 0.39316 \t val f1: 0.38433\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 96, 64, 10], 'lr': 0.001, 'optimiser': 'Adam', 'bn': False, 'batch_size': 4}\n",
      "\tearly stopping at ep.17\t train loss: 1.46089\t val loss: 1.66246\t best val loss: 1.64721\n",
      "CV_1 (49.65s) -\t train_loss: 1.46089\t val_loss: 1.66246\t val_acc: 0.43900\t val_recall: 0.43949\t val_f1: 0.44009\n",
      "\tearly stopping at ep.23\t train loss: 1.41524\t val loss: 1.62715\t best val loss: 1.62100\n",
      "CV_2 (69.16s) -\t train_loss: 1.41524\t val_loss: 1.62715\t val_acc: 0.44050\t val_recall: 0.43860\t val_f1: 0.42792\n",
      "\tearly stopping at ep.16\t train loss: 1.43778\t val loss: 1.66449\t best val loss: 1.65293\n",
      "CV_3 (46.65s) -\t train_loss: 1.43778\t val_loss: 1.66449\t val_acc: 0.43570\t val_recall: 0.43515\t val_f1: 0.42360\n",
      "\tearly stopping at ep.37\t train loss: 1.42056\t val loss: 1.61648\t best val loss: 1.60235\n",
      "CV_4 (113.00s) -\t train_loss: 1.42056\t val_loss: 1.61648\t val_acc: 0.44200\t val_recall: 0.44079\t val_f1: 0.43633\n",
      "\tearly stopping at ep.15\t train loss: 1.44828\t val loss: 1.64425\t best val loss: 1.63881\n",
      "CV_5 (44.14s) -\t train_loss: 1.44828\t val_loss: 1.64425\t val_acc: 0.43260\t val_recall: 0.43303\t val_f1: 0.42231\n",
      "## Experiment results (322.91s) - {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 96, 64, 10], 'lr': 0.001, 'optimiser': 'Adam', 'bn': False, 'batch_size': 4}\n",
      "# Metrics - train loss: 1.43655 \t val loss: 1.64297 \t val accuracy: 0.43796 \t val recall: 0.43741 \t val f1: 0.43005\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 96, 64, 10], 'lr': 0.001, 'optimiser': 'Adam', 'bn': False, 'batch_size': 2}\n",
      "\tearly stopping at ep.28\t train loss: 1.36920\t val loss: 1.51962\t best val loss: 1.48812\n",
      "CV_1 (157.19s) -\t train_loss: 1.36920\t val_loss: 1.51962\t val_acc: 0.47370\t val_recall: 0.47439\t val_f1: 0.46871\n",
      "\tearly stopping at ep.46\t train loss: 1.34282\t val loss: 1.52096\t best val loss: 1.47702\n",
      "CV_2 (258.42s) -\t train_loss: 1.34282\t val_loss: 1.52096\t val_acc: 0.47380\t val_recall: 0.47242\t val_f1: 0.46577\n",
      "\tearly stopping at ep.30\t train loss: 1.35276\t val loss: 1.55589\t best val loss: 1.48879\n",
      "CV_3 (166.47s) -\t train_loss: 1.35276\t val_loss: 1.55589\t val_acc: 0.47500\t val_recall: 0.47660\t val_f1: 0.47046\n",
      "\tearly stopping at ep.25\t train loss: 1.35731\t val loss: 1.49248\t best val loss: 1.48303\n",
      "CV_4 (146.67s) -\t train_loss: 1.35731\t val_loss: 1.49248\t val_acc: 0.48520\t val_recall: 0.48471\t val_f1: 0.48126\n",
      "\tearly stopping at ep.16\t train loss: 1.39131\t val loss: 1.59816\t best val loss: 1.52948\n",
      "CV_5 (91.27s) -\t train_loss: 1.39131\t val_loss: 1.59816\t val_acc: 0.45670\t val_recall: 0.45501\t val_f1: 0.44117\n",
      "## Experiment results (820.29s) - {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 96, 64, 10], 'lr': 0.001, 'optimiser': 'Adam', 'bn': False, 'batch_size': 2}\n",
      "# Metrics - train loss: 1.36268 \t val loss: 1.53742 \t val accuracy: 0.47288 \t val recall: 0.47262 \t val f1: 0.46548\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 96, 64, 10], 'lr': 0.001, 'optimiser': 'Adam', 'bn': True, 'batch_size': 16}\n",
      "\tearly stopping at ep.37\t train loss: 1.29768\t val loss: 1.58249\t best val loss: 1.53091\n",
      "CV_1 (61.53s) -\t train_loss: 1.29768\t val_loss: 1.58249\t val_acc: 0.45360\t val_recall: 0.45519\t val_f1: 0.44828\n",
      "\tearly stopping at ep.35\t train loss: 1.33431\t val loss: 1.58210\t best val loss: 1.54965\n",
      "CV_2 (64.61s) -\t train_loss: 1.33431\t val_loss: 1.58210\t val_acc: 0.44220\t val_recall: 0.44048\t val_f1: 0.43150\n",
      "\tearly stopping at ep.35\t train loss: 1.30496\t val loss: 1.56062\t best val loss: 1.54998\n",
      "CV_3 (59.96s) -\t train_loss: 1.30496\t val_loss: 1.56062\t val_acc: 0.45360\t val_recall: 0.45357\t val_f1: 0.45001\n",
      "\tearly stopping at ep.34\t train loss: 1.32871\t val loss: 1.57121\t best val loss: 1.53837\n",
      "CV_4 (61.58s) -\t train_loss: 1.32871\t val_loss: 1.57121\t val_acc: 0.44980\t val_recall: 0.44952\t val_f1: 0.44441\n",
      "\tearly stopping at ep.45\t train loss: 1.30137\t val loss: 1.59842\t best val loss: 1.53984\n",
      "CV_5 (80.24s) -\t train_loss: 1.30137\t val_loss: 1.59842\t val_acc: 0.44270\t val_recall: 0.44158\t val_f1: 0.43869\n",
      "## Experiment results (328.30s) - {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 96, 64, 10], 'lr': 0.001, 'optimiser': 'Adam', 'bn': True, 'batch_size': 16}\n",
      "# Metrics - train loss: 1.31341 \t val loss: 1.57897 \t val accuracy: 0.44838 \t val recall: 0.44807 \t val f1: 0.44258\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 96, 64, 10], 'lr': 0.001, 'optimiser': 'Adam', 'bn': True, 'batch_size': 8}\n",
      "\tearly stopping at ep.37\t train loss: 1.54717\t val loss: 1.49210\t best val loss: 1.47475\n",
      "CV_1 (103.68s) -\t train_loss: 1.54717\t val_loss: 1.49210\t val_acc: 0.47330\t val_recall: 0.47457\t val_f1: 0.46829\n",
      "\tearly stopping at ep.50\t train loss: 1.51296\t val loss: 1.50776\t best val loss: 1.47329\n",
      "CV_2 (147.89s) -\t train_loss: 1.51296\t val_loss: 1.50776\t val_acc: 0.46500\t val_recall: 0.46405\t val_f1: 0.46252\n",
      "\tearly stopping at ep.36\t train loss: 1.51903\t val loss: 1.52114\t best val loss: 1.47770\n",
      "CV_3 (109.38s) -\t train_loss: 1.51903\t val_loss: 1.52114\t val_acc: 0.45800\t val_recall: 0.45806\t val_f1: 0.45257\n",
      "\tearly stopping at ep.28\t train loss: 1.57164\t val loss: 1.52507\t best val loss: 1.48221\n",
      "CV_4 (79.90s) -\t train_loss: 1.57164\t val_loss: 1.52507\t val_acc: 0.45650\t val_recall: 0.45614\t val_f1: 0.45095\n",
      "\tearly stopping at ep.34\t train loss: 1.54442\t val loss: 1.50825\t best val loss: 1.48371\n",
      "CV_5 (99.97s) -\t train_loss: 1.54442\t val_loss: 1.50825\t val_acc: 0.47190\t val_recall: 0.47208\t val_f1: 0.46483\n",
      "## Experiment results (541.24s) - {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 96, 64, 10], 'lr': 0.001, 'optimiser': 'Adam', 'bn': True, 'batch_size': 8}\n",
      "# Metrics - train loss: 1.53904 \t val loss: 1.51086 \t val accuracy: 0.46494 \t val recall: 0.46498 \t val f1: 0.45983\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 96, 64, 10], 'lr': 0.001, 'optimiser': 'Adam', 'bn': True, 'batch_size': 4}\n",
      "\tearly stopping at ep.23\t train loss: 1.87928\t val loss: 1.62325\t best val loss: 1.61779\n",
      "CV_1 (121.81s) -\t train_loss: 1.87928\t val_loss: 1.62325\t val_acc: 0.43230\t val_recall: 0.43433\t val_f1: 0.41885\n",
      "\tearly stopping at ep.41\t train loss: 1.86379\t val loss: 1.60108\t best val loss: 1.59518\n",
      "CV_2 (217.53s) -\t train_loss: 1.86379\t val_loss: 1.60108\t val_acc: 0.44410\t val_recall: 0.44267\t val_f1: 0.43441\n",
      "\tearly stopping at ep.34\t train loss: 1.87239\t val loss: 1.64144\t best val loss: 1.60868\n",
      "CV_3 (179.85s) -\t train_loss: 1.87239\t val_loss: 1.64144\t val_acc: 0.43040\t val_recall: 0.43158\t val_f1: 0.42123\n",
      "\tearly stopping at ep.46\t train loss: 1.86794\t val loss: 1.58872\t best val loss: 1.58793\n",
      "CV_4 (244.22s) -\t train_loss: 1.86794\t val_loss: 1.58872\t val_acc: 0.44570\t val_recall: 0.44470\t val_f1: 0.43439\n",
      "\tearly stopping at ep.34\t train loss: 1.87542\t val loss: 1.62550\t best val loss: 1.60557\n",
      "CV_5 (180.73s) -\t train_loss: 1.87542\t val_loss: 1.62550\t val_acc: 0.44050\t val_recall: 0.44138\t val_f1: 0.43149\n",
      "## Experiment results (944.58s) - {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 96, 64, 10], 'lr': 0.001, 'optimiser': 'Adam', 'bn': True, 'batch_size': 4}\n",
      "# Metrics - train loss: 1.87176 \t val loss: 1.61600 \t val accuracy: 0.43860 \t val recall: 0.43893 \t val f1: 0.42807\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 96, 64, 10], 'lr': 0.001, 'optimiser': 'Adam', 'bn': True, 'batch_size': 2}\n",
      "\tearly stopping at ep.17\t train loss: 2.26538\t val loss: 2.19831\t best val loss: 2.15801\n",
      "CV_1 (179.25s) -\t train_loss: 2.26538\t val_loss: 2.19831\t val_acc: 0.18990\t val_recall: 0.19225\t val_f1: 0.13885\n",
      "\tearly stopping at ep.21\t train loss: 2.26191\t val loss: 2.20420\t best val loss: 2.18598\n",
      "CV_2 (220.93s) -\t train_loss: 2.26191\t val_loss: 2.20420\t val_acc: 0.18020\t val_recall: 0.18186\t val_f1: 0.13569\n",
      "\tearly stopping at ep.16\t train loss: 2.26597\t val loss: 2.20041\t best val loss: 2.17020\n",
      "CV_3 (166.94s) -\t train_loss: 2.26597\t val_loss: 2.20041\t val_acc: 0.19710\t val_recall: 0.19649\t val_f1: 0.16599\n",
      "\tearly stopping at ep.16\t train loss: 2.26576\t val loss: 2.20232\t best val loss: 2.19002\n",
      "CV_4 (167.39s) -\t train_loss: 2.26576\t val_loss: 2.20232\t val_acc: 0.18940\t val_recall: 0.19128\t val_f1: 0.15033\n",
      "\tearly stopping at ep.18\t train loss: 2.28054\t val loss: 2.23682\t best val loss: 2.16871\n",
      "CV_5 (187.13s) -\t train_loss: 2.28054\t val_loss: 2.23682\t val_acc: 0.17530\t val_recall: 0.17536\t val_f1: 0.12310\n",
      "## Experiment results (922.09s) - {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 96, 64, 10], 'lr': 0.001, 'optimiser': 'Adam', 'bn': True, 'batch_size': 2}\n",
      "# Metrics - train loss: 2.26791 \t val loss: 2.20841 \t val accuracy: 0.18638 \t val recall: 0.18745 \t val f1: 0.14279\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 96, 64, 10], 'lr': 0.001, 'optimiser': 'Momentum', 'bn': False, 'batch_size': 16}\n",
      "\tearly stopping at ep.26\t train loss: 2.24759\t val loss: 2.38681\t best val loss: 2.29836\n",
      "CV_1 (16.78s) -\t train_loss: 2.24759\t val_loss: 2.38681\t val_acc: 0.11780\t val_recall: 0.11875\t val_f1: 0.05982\n",
      "\tearly stopping at ep.17\t train loss: 2.19498\t val loss: 2.32878\t best val loss: 2.31249\n",
      "CV_2 (11.05s) -\t train_loss: 2.19498\t val_loss: 2.32878\t val_acc: 0.11820\t val_recall: 0.11439\t val_f1: 0.04768\n",
      "\tearly stopping at ep.20\t train loss: 2.12031\t val loss: 2.32319\t best val loss: 2.30545\n",
      "CV_3 (12.60s) -\t train_loss: 2.12031\t val_loss: 2.32319\t val_acc: 0.16210\t val_recall: 0.15718\t val_f1: 0.09415\n",
      "\tearly stopping at ep.18\t train loss: 2.09566\t val loss: 2.60039\t best val loss: 2.29851\n",
      "CV_4 (11.73s) -\t train_loss: 2.09566\t val_loss: 2.60039\t val_acc: 0.18660\t val_recall: 0.18732\t val_f1: 0.12570\n",
      "\tearly stopping at ep.14\t train loss: 2.19575\t val loss: 2.35595\t best val loss: 2.31099\n",
      "CV_5 (8.97s) -\t train_loss: 2.19575\t val_loss: 2.35595\t val_acc: 0.13650\t val_recall: 0.13779\t val_f1: 0.07710\n",
      "## Experiment results (61.35s) - {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 96, 64, 10], 'lr': 0.001, 'optimiser': 'Momentum', 'bn': False, 'batch_size': 16}\n",
      "# Metrics - train loss: 2.17086 \t val loss: 2.39902 \t val accuracy: 0.14424 \t val recall: 0.14309 \t val f1: 0.08089\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 96, 64, 10], 'lr': 0.001, 'optimiser': 'Momentum', 'bn': False, 'batch_size': 8}\n",
      "\tearly stopping at ep.17\t train loss: 2.09350\t val loss: 2.20638\t best val loss: 2.17797\n",
      "CV_1 (18.48s) -\t train_loss: 2.09350\t val_loss: 2.20638\t val_acc: 0.16880\t val_recall: 0.16810\t val_f1: 0.10566\n",
      "\tearly stopping at ep.21\t train loss: 2.08815\t val loss: 2.20080\t best val loss: 2.15108\n",
      "CV_2 (21.55s) -\t train_loss: 2.08815\t val_loss: 2.20080\t val_acc: 0.17890\t val_recall: 0.17858\t val_f1: 0.12265\n",
      "\tearly stopping at ep.14\t train loss: 2.09889\t val loss: 2.20364\t best val loss: 2.18142\n",
      "CV_3 (14.23s) -\t train_loss: 2.09889\t val_loss: 2.20364\t val_acc: 0.18410\t val_recall: 0.18482\t val_f1: 0.12763\n",
      "\tearly stopping at ep.27\t train loss: 2.09680\t val loss: 2.26292\t best val loss: 2.15058\n",
      "CV_4 (27.02s) -\t train_loss: 2.09680\t val_loss: 2.26292\t val_acc: 0.17540\t val_recall: 0.17706\t val_f1: 0.11628\n",
      "\tearly stopping at ep.17\t train loss: 2.08144\t val loss: 2.19513\t best val loss: 2.16936\n",
      "CV_5 (16.44s) -\t train_loss: 2.08144\t val_loss: 2.19513\t val_acc: 0.14900\t val_recall: 0.14917\t val_f1: 0.08296\n",
      "## Experiment results (97.99s) - {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 96, 64, 10], 'lr': 0.001, 'optimiser': 'Momentum', 'bn': False, 'batch_size': 8}\n",
      "# Metrics - train loss: 2.09176 \t val loss: 2.21377 \t val accuracy: 0.17124 \t val recall: 0.17155 \t val f1: 0.11103\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 96, 64, 10], 'lr': 0.001, 'optimiser': 'Momentum', 'bn': False, 'batch_size': 4}\n",
      "\tearly stopping at ep.46\t train loss: 2.01554\t val loss: 2.08512\t best val loss: 2.03305\n",
      "CV_1 (84.32s) -\t train_loss: 2.01554\t val_loss: 2.08512\t val_acc: 0.24380\t val_recall: 0.24666\t val_f1: 0.18586\n",
      "\tearly stopping at ep.19\t train loss: 2.01540\t val loss: 2.12469\t best val loss: 2.06463\n",
      "CV_2 (35.20s) -\t train_loss: 2.01540\t val_loss: 2.12469\t val_acc: 0.23180\t val_recall: 0.23449\t val_f1: 0.18735\n",
      "\tearly stopping at ep.14\t train loss: 2.02043\t val loss: 2.07462\t best val loss: 2.06163\n",
      "CV_3 (25.86s) -\t train_loss: 2.02043\t val_loss: 2.07462\t val_acc: 0.22590\t val_recall: 0.22923\t val_f1: 0.18751\n",
      "\tearly stopping at ep.25\t train loss: 2.02937\t val loss: 2.11190\t best val loss: 2.06917\n",
      "CV_4 (46.88s) -\t train_loss: 2.02937\t val_loss: 2.11190\t val_acc: 0.20780\t val_recall: 0.20875\t val_f1: 0.17170\n",
      "\tearly stopping at ep.16\t train loss: 2.03362\t val loss: 2.07885\t best val loss: 2.04753\n",
      "CV_5 (29.68s) -\t train_loss: 2.03362\t val_loss: 2.07885\t val_acc: 0.22820\t val_recall: 0.22812\t val_f1: 0.18231\n",
      "## Experiment results (222.15s) - {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 96, 64, 10], 'lr': 0.001, 'optimiser': 'Momentum', 'bn': False, 'batch_size': 4}\n",
      "# Metrics - train loss: 2.02287 \t val loss: 2.09504 \t val accuracy: 0.22750 \t val recall: 0.22945 \t val f1: 0.18295\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 96, 64, 10], 'lr': 0.001, 'optimiser': 'Momentum', 'bn': False, 'batch_size': 2}\n",
      "\tearly stopping at ep.22\t train loss: 1.70413\t val loss: 1.79901\t best val loss: 1.73682\n",
      "CV_1 (74.87s) -\t train_loss: 1.70413\t val_loss: 1.79901\t val_acc: 0.38010\t val_recall: 0.38172\t val_f1: 0.36999\n",
      "\tearly stopping at ep.29\t train loss: 1.70127\t val loss: 1.77769\t best val loss: 1.69422\n",
      "CV_2 (98.39s) -\t train_loss: 1.70127\t val_loss: 1.77769\t val_acc: 0.37650\t val_recall: 0.37496\t val_f1: 0.36599\n",
      "\tearly stopping at ep.29\t train loss: 1.69185\t val loss: 1.75508\t best val loss: 1.70816\n",
      "CV_3 (101.83s) -\t train_loss: 1.69185\t val_loss: 1.75508\t val_acc: 0.37700\t val_recall: 0.37777\t val_f1: 0.38188\n",
      "\tearly stopping at ep.38\t train loss: 1.69152\t val loss: 1.77335\t best val loss: 1.71551\n",
      "CV_4 (136.74s) -\t train_loss: 1.69152\t val_loss: 1.77335\t val_acc: 0.38590\t val_recall: 0.38535\t val_f1: 0.36963\n",
      "\tearly stopping at ep.23\t train loss: 1.68787\t val loss: 1.84646\t best val loss: 1.74149\n",
      "CV_5 (78.92s) -\t train_loss: 1.68787\t val_loss: 1.84646\t val_acc: 0.36920\t val_recall: 0.36981\t val_f1: 0.36162\n",
      "## Experiment results (491.03s) - {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 96, 64, 10], 'lr': 0.001, 'optimiser': 'Momentum', 'bn': False, 'batch_size': 2}\n",
      "# Metrics - train loss: 1.69533 \t val loss: 1.79032 \t val accuracy: 0.37774 \t val recall: 0.37792 \t val f1: 0.36982\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 96, 64, 10], 'lr': 0.001, 'optimiser': 'Momentum', 'bn': True, 'batch_size': 16}\n",
      "\tearly stopping at ep.36\t train loss: 1.46822\t val loss: 1.56096\t best val loss: 1.52062\n",
      "CV_1 (49.77s) -\t train_loss: 1.46822\t val_loss: 1.56096\t val_acc: 0.44430\t val_recall: 0.44490\t val_f1: 0.44407\n",
      "\tearly stopping at ep.44\t train loss: 1.44159\t val loss: 1.59658\t best val loss: 1.54191\n",
      "CV_2 (73.55s) -\t train_loss: 1.44159\t val_loss: 1.59658\t val_acc: 0.43530\t val_recall: 0.43455\t val_f1: 0.43190\n",
      "\tearly stopping at ep.34\t train loss: 1.42458\t val loss: 1.62886\t best val loss: 1.54538\n",
      "CV_3 (46.83s) -\t train_loss: 1.42458\t val_loss: 1.62886\t val_acc: 0.41330\t val_recall: 0.41321\t val_f1: 0.40796\n",
      "\tearly stopping at ep.38\t train loss: 1.39064\t val loss: 1.63287\t best val loss: 1.55094\n",
      "CV_4 (53.37s) -\t train_loss: 1.39064\t val_loss: 1.63287\t val_acc: 0.42860\t val_recall: 0.42836\t val_f1: 0.42129\n",
      "\tearly stopping at ep.27\t train loss: 1.47584\t val loss: 1.58950\t best val loss: 1.57823\n",
      "CV_5 (36.86s) -\t train_loss: 1.47584\t val_loss: 1.58950\t val_acc: 0.43250\t val_recall: 0.43230\t val_f1: 0.42107\n",
      "## Experiment results (260.70s) - {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 96, 64, 10], 'lr': 0.001, 'optimiser': 'Momentum', 'bn': True, 'batch_size': 16}\n",
      "# Metrics - train loss: 1.44017 \t val loss: 1.60175 \t val accuracy: 0.43080 \t val recall: 0.43067 \t val f1: 0.42526\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 96, 64, 10], 'lr': 0.001, 'optimiser': 'Momentum', 'bn': True, 'batch_size': 8}\n",
      "\tearly stopping at ep.42\t train loss: 1.65852\t val loss: 1.53683\t best val loss: 1.53384\n",
      "CV_1 (96.55s) -\t train_loss: 1.65852\t val_loss: 1.53683\t val_acc: 0.45710\t val_recall: 0.45829\t val_f1: 0.45392\n",
      "\tearly stopping at ep.33\t train loss: 1.62725\t val loss: 1.54498\t best val loss: 1.53678\n",
      "CV_2 (77.50s) -\t train_loss: 1.62725\t val_loss: 1.54498\t val_acc: 0.45100\t val_recall: 0.44969\t val_f1: 0.44478\n",
      "\tearly stopping at ep.37\t train loss: 1.62232\t val loss: 1.55341\t best val loss: 1.54146\n",
      "CV_3 (85.18s) -\t train_loss: 1.62232\t val_loss: 1.55341\t val_acc: 0.45190\t val_recall: 0.45349\t val_f1: 0.44862\n",
      "\tearly stopping at ep.30\t train loss: 1.63958\t val loss: 1.55879\t best val loss: 1.53680\n",
      "CV_4 (65.94s) -\t train_loss: 1.63958\t val_loss: 1.55879\t val_acc: 0.44700\t val_recall: 0.44651\t val_f1: 0.43750\n",
      "\tearly stopping at ep.28\t train loss: 1.63130\t val loss: 1.56728\t best val loss: 1.54889\n",
      "CV_5 (62.59s) -\t train_loss: 1.63130\t val_loss: 1.56728\t val_acc: 0.44440\t val_recall: 0.44347\t val_f1: 0.43576\n",
      "## Experiment results (388.17s) - {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 96, 64, 10], 'lr': 0.001, 'optimiser': 'Momentum', 'bn': True, 'batch_size': 8}\n",
      "# Metrics - train loss: 1.63579 \t val loss: 1.55226 \t val accuracy: 0.45028 \t val recall: 0.45029 \t val f1: 0.44411\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 96, 64, 10], 'lr': 0.001, 'optimiser': 'Momentum', 'bn': True, 'batch_size': 4}\n",
      "\tearly stopping at ep.26\t train loss: 1.93292\t val loss: 1.69984\t best val loss: 1.68595\n",
      "CV_1 (108.75s) -\t train_loss: 1.93292\t val_loss: 1.69984\t val_acc: 0.39970\t val_recall: 0.40002\t val_f1: 0.39372\n",
      "\tearly stopping at ep.39\t train loss: 1.92081\t val loss: 1.69452\t best val loss: 1.65495\n",
      "CV_2 (161.87s) -\t train_loss: 1.92081\t val_loss: 1.69452\t val_acc: 0.41610\t val_recall: 0.41509\t val_f1: 0.40855\n",
      "CV_3 (206.66s) -\t train_loss: 1.91550\t val_loss: 1.68515\t val_acc: 0.40810\t val_recall: 0.40921\t val_f1: 0.40459\n",
      "\tearly stopping at ep.34\t train loss: 1.92668\t val loss: 1.67968\t best val loss: 1.66125\n",
      "CV_4 (140.30s) -\t train_loss: 1.92668\t val_loss: 1.67968\t val_acc: 0.41930\t val_recall: 0.41967\t val_f1: 0.41348\n",
      "\tearly stopping at ep.33\t train loss: 1.92788\t val loss: 1.72321\t best val loss: 1.68257\n",
      "CV_5 (138.08s) -\t train_loss: 1.92788\t val_loss: 1.72321\t val_acc: 0.40450\t val_recall: 0.40492\t val_f1: 0.39505\n",
      "## Experiment results (756.03s) - {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 96, 64, 10], 'lr': 0.001, 'optimiser': 'Momentum', 'bn': True, 'batch_size': 4}\n",
      "# Metrics - train loss: 1.92476 \t val loss: 1.69648 \t val accuracy: 0.40954 \t val recall: 0.40978 \t val f1: 0.40308\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 96, 64, 10], 'lr': 0.001, 'optimiser': 'Momentum', 'bn': True, 'batch_size': 2}\n",
      "\tearly stopping at ep.12\t train loss: 2.29415\t val loss: 2.28937\t best val loss: 2.20814\n",
      "CV_1 (96.77s) -\t train_loss: 2.29415\t val_loss: 2.28937\t val_acc: 0.10740\t val_recall: 0.10756\t val_f1: 0.05912\n",
      "\tearly stopping at ep.11\t train loss: 2.27766\t val loss: 2.24146\t best val loss: 2.20485\n",
      "CV_2 (88.65s) -\t train_loss: 2.27766\t val_loss: 2.24146\t val_acc: 0.19010\t val_recall: 0.18821\t val_f1: 0.13697\n",
      "\tearly stopping at ep.17\t train loss: 2.28983\t val loss: 2.25974\t best val loss: 2.21334\n",
      "CV_3 (135.30s) -\t train_loss: 2.28983\t val_loss: 2.25974\t val_acc: 0.17640\t val_recall: 0.17759\t val_f1: 0.10769\n",
      "\tearly stopping at ep.16\t train loss: 2.28259\t val loss: 2.25505\t best val loss: 2.22899\n",
      "CV_4 (127.89s) -\t train_loss: 2.28259\t val_loss: 2.25505\t val_acc: 0.16060\t val_recall: 0.16219\t val_f1: 0.10612\n",
      "\tearly stopping at ep.11\t train loss: 2.27580\t val loss: 2.23874\t best val loss: 2.22313\n",
      "CV_5 (87.83s) -\t train_loss: 2.27580\t val_loss: 2.23874\t val_acc: 0.14490\t val_recall: 0.14593\t val_f1: 0.10712\n",
      "## Experiment results (536.84s) - {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 96, 64, 10], 'lr': 0.001, 'optimiser': 'Momentum', 'bn': True, 'batch_size': 2}\n",
      "# Metrics - train loss: 2.28401 \t val loss: 2.25687 \t val accuracy: 0.15588 \t val recall: 0.15630 \t val f1: 0.10340\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 96, 64, 10], 'lr': 0.0001, 'optimiser': None, 'bn': False, 'batch_size': 16}\n",
      "\tearly stopping at ep.49\t train loss: 1.18157\t val loss: 1.57586\t best val loss: 1.51373\n",
      "CV_1 (27.83s) -\t train_loss: 1.18157\t val_loss: 1.57586\t val_acc: 0.44470\t val_recall: 0.44401\t val_f1: 0.44504\n",
      "CV_2 (33.15s) -\t train_loss: 1.18483\t val_loss: 1.59848\t val_acc: 0.44550\t val_recall: 0.44459\t val_f1: 0.43744\n",
      "\tearly stopping at ep.46\t train loss: 1.15140\t val loss: 1.57275\t best val loss: 1.53401\n",
      "CV_3 (28.63s) -\t train_loss: 1.15140\t val_loss: 1.57275\t val_acc: 0.44900\t val_recall: 0.44996\t val_f1: 0.44808\n",
      "CV_4 (27.86s) -\t train_loss: 1.15351\t val_loss: 1.53132\t val_acc: 0.45870\t val_recall: 0.45840\t val_f1: 0.46134\n",
      "\tearly stopping at ep.42\t train loss: 1.20637\t val loss: 1.58577\t best val loss: 1.53334\n",
      "CV_5 (26.00s) -\t train_loss: 1.20637\t val_loss: 1.58577\t val_acc: 0.43980\t val_recall: 0.44083\t val_f1: 0.42721\n",
      "## Experiment results (143.75s) - {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 96, 64, 10], 'lr': 0.0001, 'optimiser': None, 'bn': False, 'batch_size': 16}\n",
      "# Metrics - train loss: 1.17554 \t val loss: 1.57284 \t val accuracy: 0.44754 \t val recall: 0.44756 \t val f1: 0.44382\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 96, 64, 10], 'lr': 0.0001, 'optimiser': None, 'bn': False, 'batch_size': 8}\n",
      "CV_1 (49.87s) -\t train_loss: 1.39733\t val_loss: 1.50492\t val_acc: 0.46600\t val_recall: 0.46658\t val_f1: 0.46221\n",
      "CV_2 (46.02s) -\t train_loss: 1.39385\t val_loss: 1.50258\t val_acc: 0.47290\t val_recall: 0.47195\t val_f1: 0.46821\n",
      "CV_3 (47.43s) -\t train_loss: 1.39930\t val_loss: 1.51295\t val_acc: 0.46490\t val_recall: 0.46453\t val_f1: 0.45443\n",
      "CV_4 (47.00s) -\t train_loss: 1.38818\t val_loss: 1.49636\t val_acc: 0.47010\t val_recall: 0.47014\t val_f1: 0.46402\n",
      "CV_5 (46.33s) -\t train_loss: 1.39973\t val_loss: 1.51083\t val_acc: 0.46650\t val_recall: 0.46707\t val_f1: 0.46163\n",
      "## Experiment results (236.90s) - {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 96, 64, 10], 'lr': 0.0001, 'optimiser': None, 'bn': False, 'batch_size': 8}\n",
      "# Metrics - train loss: 1.39568 \t val loss: 1.50553 \t val accuracy: 0.46808 \t val recall: 0.46806 \t val f1: 0.46210\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 96, 64, 10], 'lr': 0.0001, 'optimiser': None, 'bn': False, 'batch_size': 4}\n",
      "CV_1 (85.80s) -\t train_loss: 1.63509\t val_loss: 1.64843\t val_acc: 0.41710\t val_recall: 0.41857\t val_f1: 0.40616\n",
      "CV_2 (90.80s) -\t train_loss: 1.61504\t val_loss: 1.64611\t val_acc: 0.41850\t val_recall: 0.41760\t val_f1: 0.40317\n",
      "CV_3 (92.13s) -\t train_loss: 1.61280\t val_loss: 1.65192\t val_acc: 0.42240\t val_recall: 0.42301\t val_f1: 0.41323\n",
      "CV_4 (93.38s) -\t train_loss: 1.63459\t val_loss: 1.64436\t val_acc: 0.41450\t val_recall: 0.41477\t val_f1: 0.40187\n",
      "CV_5 (88.54s) -\t train_loss: 1.61830\t val_loss: 1.64701\t val_acc: 0.42170\t val_recall: 0.42090\t val_f1: 0.40516\n",
      "## Experiment results (450.88s) - {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 96, 64, 10], 'lr': 0.0001, 'optimiser': None, 'bn': False, 'batch_size': 4}\n",
      "# Metrics - train loss: 1.62316 \t val loss: 1.64757 \t val accuracy: 0.41884 \t val recall: 0.41897 \t val f1: 0.40592\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 96, 64, 10], 'lr': 0.0001, 'optimiser': None, 'bn': False, 'batch_size': 2}\n",
      "\tearly stopping at ep.29\t train loss: 1.85669\t val loss: 1.85468\t best val loss: 1.85539\n",
      "CV_1 (93.21s) -\t train_loss: 1.85669\t val_loss: 1.85468\t val_acc: 0.33910\t val_recall: 0.34038\t val_f1: 0.31100\n",
      "\tearly stopping at ep.34\t train loss: 1.85738\t val loss: 1.86902\t best val loss: 1.86846\n",
      "CV_2 (108.27s) -\t train_loss: 1.85738\t val_loss: 1.86902\t val_acc: 0.33790\t val_recall: 0.33701\t val_f1: 0.31044\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [8], line 32\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     31\u001b[0m         start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 32\u001b[0m         cv_metrics \u001b[38;5;241m=\u001b[39m \u001b[43mrun_model_cv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfold_splits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhyperparams_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m         end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     34\u001b[0m         time_taken \u001b[38;5;241m=\u001b[39m end\u001b[38;5;241m-\u001b[39mstart\n",
      "Cell \u001b[1;32mIn [6], line 23\u001b[0m, in \u001b[0;36mrun_model_cv\u001b[1;34m(fold_splits, hyperparams_dict)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Fit the model\u001b[39;00m\n\u001b[0;32m     22\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 23\u001b[0m nn_output \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSETUP\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mepochs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimiser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimiser\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSETUP\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mearly_stopping\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Extract metrics\u001b[39;00m\n",
      "File \u001b[1;32mg:\\My Drive\\UNI\\COMP4329 DL\\Assigments\\DeepLearningA1NE\\MLP.py:152\u001b[0m, in \u001b[0;36mMLP.fit\u001b[1;34m(self, X, y, X_val, y_val, learning_rate, epochs, batch_size, weight_decay, optimiser, early_stopping)\u001b[0m\n\u001b[0;32m    149\u001b[0m     loss, delta_minibatch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion_CrossEL_Batch(y_minibatch, y_hat)\n\u001b[0;32m    150\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackward(delta_minibatch)\n\u001b[1;32m--> 152\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    153\u001b[0m     loss_minibatch[k][j] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(loss)\n\u001b[0;32m    155\u001b[0m \u001b[38;5;66;03m# Calculate the average loss for the epoch\u001b[39;00m\n",
      "File \u001b[1;32mg:\\My Drive\\UNI\\COMP4329 DL\\Assigments\\DeepLearningA1NE\\MLP.py:90\u001b[0m, in \u001b[0;36mMLP.update\u001b[1;34m(self, lr, weight_decay)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;124;03mUpdates the weights and biases of all layers, including batch normalization parameters if used.\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;124;03m- weight_decay (float): Weight decay rate for neural network weights and bias\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m---> 90\u001b[0m     layer\u001b[38;5;241m.\u001b[39mW \u001b[38;5;241m=\u001b[39m layer\u001b[38;5;241m.\u001b[39mW \u001b[38;5;241m-\u001b[39m \u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mW_optimiser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad_W\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     91\u001b[0m     layer\u001b[38;5;241m.\u001b[39mb \u001b[38;5;241m=\u001b[39m layer\u001b[38;5;241m.\u001b[39mb \u001b[38;5;241m-\u001b[39m layer\u001b[38;5;241m.\u001b[39mb_optimiser\u001b[38;5;241m.\u001b[39mupdate(layer\u001b[38;5;241m.\u001b[39mgrad_b)\n\u001b[0;32m     92\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m weight_decay \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Seed for reproducibility\n",
    "np.random.seed(0)\n",
    "\n",
    "# Generate training and validation datasets for each fold\n",
    "fold_splits = []\n",
    "for i in range(n_folds):\n",
    "        # Generate training and validation sets for this fold\n",
    "        val_indices = folds[i]\n",
    "        train_indices = np.hstack(folds[:i] + folds[i+1:])\n",
    "        \n",
    "        X_train, y_train = X[train_indices], y[train_indices]\n",
    "        X_val, y_val = X[val_indices], y[val_indices]\n",
    "        fold_splits.append((X_train, y_train, X_val, y_val))\n",
    "\n",
    "outputs = []\n",
    "\n",
    "print('Begin Hyper-parameter Tuning -')\n",
    "for hyperparams in itertools.product(weight_decay_options, drop_out_options, hidden_layer_options, lr_options, optimiser_options, bn_option, batch_size_options):\n",
    "        # Convert hyperparams to dictionary for easier access\n",
    "        hyperparams_dict = {\n",
    "                'weight_decay': hyperparams[0],\n",
    "                'dropout': hyperparams[1],\n",
    "                'hidden_layers': hyperparams[2],\n",
    "                'lr': hyperparams[3],\n",
    "                'optimiser': hyperparams[4],\n",
    "                'bn': hyperparams[5],\n",
    "                'batch_size': hyperparams[6]\n",
    "        }\n",
    "        print(f'\\n\\n### Running model {str(hyperparams_dict)}')\n",
    "        try:\n",
    "                start = time.time()\n",
    "                cv_metrics = run_model_cv(fold_splits, hyperparams_dict)\n",
    "                end = time.time()\n",
    "                time_taken = end-start\n",
    "                \n",
    "                cv_train_loss_scores, cv_val_loss_scores, cv_val_accuracy_scores, cv_val_recall_scores, \\\n",
    "                        cv_val_f1_scores, cv_times, detailed_train_losses, detailed_val_losses = cv_metrics\n",
    "                train_loss = np.mean(cv_train_loss_scores)\n",
    "                val_loss = np.mean(cv_val_loss_scores)\n",
    "                val_accuracy = np.mean(cv_val_accuracy_scores)\n",
    "                val_recall = np.mean(cv_val_recall_scores)\n",
    "                val_f1 = np.mean(cv_val_f1_scores)\n",
    "                \n",
    "                print(f'## Experiment results ({time_taken:.2f}s) - {str(hyperparams_dict)}')\n",
    "                print(f'# Metrics - train loss: {train_loss:.5f} \\t val loss: {val_loss:.5f} \\t val accuracy: {val_accuracy:.5f} \\t val recall: {val_recall:.5f} \\t val f1: {val_f1:.5f}')\n",
    "                # Print the experiment results string and print to file\n",
    "                \n",
    "                log_to_file(hyperparams_dict, cv_metrics, time_taken)\n",
    "                \n",
    "        except Exception as e:\n",
    "                print(f'Error: {e}')\n",
    "                log_to_file(hyperparams_dict, [[0]*n_folds]*8, 99999)\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin Hyper-parameter Tuning -\n",
      "\n",
      "\n",
      "### Running model (0.0001, 16, 'Adam', True)\n",
      "Epoch 1/1, Train loss: 2.23815, Val loss: 2.10822\n",
      "CV_1 (2.19s) -\t train_loss: 2.23815\t val_loss: 2.10822\t val_acc: 0.23400\n",
      "\n",
      "Epoch 1/1, Train loss: 2.22117, Val loss: 2.09725\n",
      "CV_2 (2.25s) -\t train_loss: 2.22117\t val_loss: 2.09725\t val_acc: 0.25470\n",
      "\n",
      "Epoch 1/1, Train loss: 2.18520, Val loss: 2.32576\n",
      "CV_3 (2.38s) -\t train_loss: 2.18520\t val_loss: 2.32576\t val_acc: 0.19520\n",
      "\n",
      "Epoch 1/1, Train loss: 2.18423, Val loss: 2.26142\n",
      "CV_4 (2.19s) -\t train_loss: 2.18423\t val_loss: 2.26142\t val_acc: 0.20870\n",
      "\n",
      "Epoch 1/1, Train loss: 2.20537, Val loss: 2.23239\n",
      "CV_5 (2.14s) -\t train_loss: 2.20537\t val_loss: 2.23239\t val_acc: 0.20060\n",
      "\n",
      "## Experiment results (14.90s) - (0.0001, 16, 'Adam', True)\n",
      "# CV Metrics - train: 2.20682\t val: 2.20501\t val_acc: (mean 0.21864) [0.234, 0.2547, 0.1952, 0.2087, 0.2006]\n"
     ]
    }
   ],
   "source": [
    "# Seed for reproducibility\n",
    "np.random.seed(0)\n",
    "\n",
    "# Generate training and validation datasets for each fold\n",
    "fold_splits = []\n",
    "for i in range(n_folds):\n",
    "        # Generate training and validation sets for this fold\n",
    "        val_indices = folds[i]\n",
    "        train_indices = np.hstack(folds[:i] + folds[i+1:])\n",
    "        \n",
    "        X_train, y_train = X[train_indices], y[train_indices]\n",
    "        X_val, y_val = X[val_indices], y[val_indices]\n",
    "        fold_splits.append((X_train, y_train, X_val, y_val))\n",
    "\n",
    "outputs = []\n",
    "\n",
    "print('Begin Hyper-parameter Tuning -')\n",
    "for hyperparams in itertools.product(lr_options, batch_size_options, optimiser_options, bn_option):\n",
    "        print(f'\\n\\n### Running model {str(hyperparams)}')\n",
    "        start = time.time()\n",
    "        cv_metrics = run_model_cv(fold_splits, hyperparams)\n",
    "        end = time.time()\n",
    "        time_took = end-start\n",
    "        \n",
    "        train_losses, val_losses, val_accuracies, times = cv_metrics\n",
    "        train_loss = np.mean(train_losses)\n",
    "        val_loss = np.mean(val_losses)\n",
    "        \n",
    "        print(f'## Experiment results ({time_took:.2f}s) - {str(hyperparams)}')\n",
    "        print(f'# CV Metrics - train: {train_loss:.5f}\\t val: {val_loss:.5f}\\t val_acc: (mean {np.mean(val_accuracies):.5f}) {str(val_accuracies)}')\n",
    "        outputs.append((hyperparams, cv_metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((0.0001, 16, 'Adam', True), ([2.2381500206835176, 2.221170012953086, 2.1851958137467906, 2.1842262844334037, 2.2053723065266966], [2.108224230188417, 2.097252578339957, 2.3257567537189514, 2.261423767114958, 2.2323944851865822], [0.234, 0.2547, 0.1952, 0.2087, 0.2006], [2.1857736110687256, 2.2517547607421875, 2.376272678375244, 2.189086675643921, 2.1445536613464355]))]\n"
     ]
    }
   ],
   "source": [
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
