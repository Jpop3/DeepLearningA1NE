{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sebyb\\anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\Users\\Sebyb\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.4SP5SUA7CBGXUEOC35YP2ASOICYYEQZZ.gfortran-win_amd64.dll\n",
      "c:\\Users\\Sebyb\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "# import itertools\n",
    "import numpy as np\n",
    "from MLP import *\n",
    "import os\n",
    "import time\n",
    "import itertools\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score\n",
    "import csv\n",
    "import concurrent.futures\n",
    "import json\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from MLP import *\n",
    "import os\n",
    "import time\n",
    "import itertools\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score\n",
    "import csv\n",
    "import concurrent.futures\n",
    "import json\n",
    "import pickle\n",
    "import traceback\n",
    "\n",
    "X = np.load('Assignment1-Dataset/train_data.npy')\n",
    "labels = np.load('Assignment1-Dataset/train_label.npy')\n",
    "y = np.array([MLP.class_to_one_hot(label, 10) for label in labels])\n",
    "\n",
    "# Seed for reproducibility\n",
    "np.random.seed(0)\n",
    "\n",
    "# 5-fold Cross Validation\n",
    "n_samples = X.shape[0]\n",
    "n_folds = 5\n",
    "indices = np.arange(n_samples)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "fold_sizes = np.full(n_folds, n_samples // n_folds, dtype=int)\n",
    "fold_sizes[:n_samples % n_folds] += 1\n",
    "current = 0\n",
    "folds = []\n",
    "for fold_size in fold_sizes:\n",
    "    start, stop = current, current + fold_size\n",
    "    folds.append(indices[start:stop])\n",
    "    current = stop\n",
    "\n",
    "# Generate training and validation datasets for each fold\n",
    "fold_splits = []\n",
    "for i in range(n_folds):\n",
    "        # Generate training and validation sets for this fold\n",
    "        val_indices = folds[i]\n",
    "        train_indices = np.hstack(folds[:i] + folds[i+1:])\n",
    "        \n",
    "        X_train, y_train = X[train_indices], y[train_indices]\n",
    "        X_val, y_val = X[val_indices], y[val_indices]\n",
    "        fold_splits.append((X_train, y_train, X_val, y_val))\n",
    "        \n",
    "# Make copies of the fold_splits data for async processing\n",
    "fold_splits = [(X_train.copy(), y_train.copy(), X_val.copy(), y_val.copy()) for (X_train, y_train, X_val, y_val) in fold_splits]\n",
    "\n",
    "fold_splits = fold_splits[:1]\n",
    "\n",
    "def can_be_pickled(obj):\n",
    "    try:\n",
    "        pickle.dumps(obj)\n",
    "        return True\n",
    "    except (pickle.PicklingError, TypeError) as e:\n",
    "        print(f\"Cannot be pickled: {e}\")\n",
    "        return False\n",
    "\n",
    "print(can_be_pickled(fold_splits))\n",
    "    \n",
    "### Constants ###\n",
    "\n",
    "SETUP = {\n",
    "    'epochs': 2,\n",
    "    'activations': [None, 'ReLU', 'ReLU', 'softmax'],\n",
    "    'input_size': 128,\n",
    "    'early_stopping': (10, 0.001)\n",
    "}\n",
    "\n",
    "### Options for Hyper-Parameters ###\n",
    "\n",
    "weight_decay_options = [0.0, 0.0001, 0.001]\n",
    "drop_out_options = [[0.0, 0.0, 0.0, 0.0], [0.5, 0.2, 0.2, 0.0], [0.1, 0.3, 0.3, 0.1]]\n",
    "hidden_layer_options = [[128, 64, 32, 10]]\n",
    "lr_options = [0.001, 0.0001, 0.01]\n",
    "optimiser_options = [None, 'Adam', 'Momentum']\n",
    "bn_option = [False, True]\n",
    "batch_size_options = [10,5,2]\n",
    "\n",
    "\n",
    "def log_detailed_metrics(hyperparams_dict, cv_metrics, filename=\"detailed_model_performance.csv\"):\n",
    "    # Check if file exists, if not, write headers\n",
    "    file_exists = os.path.isfile(filename)\n",
    "    \n",
    "    with open(filename, mode='a', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        if not file_exists:\n",
    "            writer.writerow([\"Fold\", \"Hyperparameters\", \"Train Losses\", \"Val Losses\", \"Val Accuracy\", \"Val Recall\", \"Val F1\", \"Time Taken\", \"Early Stop Epoch\"])\n",
    "\n",
    "        for i, metrics in enumerate(cv_metrics):\n",
    "            # Serialize detailed metrics as JSON strings\n",
    "            detailed_train_loss = json.dumps(metrics['detailed_train_loss'])\n",
    "            detailed_val_loss = json.dumps(metrics['detailed_val_loss'])\n",
    "            writer.writerow([\n",
    "                i + 1,\n",
    "                json.dumps(hyperparams_dict),  # Hyperparameters as JSON\n",
    "                detailed_train_loss,\n",
    "                detailed_val_loss,\n",
    "                metrics['val_accuracy'],\n",
    "                metrics['val_recall'],\n",
    "                metrics['val_f1'],\n",
    "                metrics['time_taken'],\n",
    "                metrics['early_stop_epoch']\n",
    "            ])\n",
    "\n",
    "def train_and_evaluate_fold(X_train, y_train, X_val, y_val, hyperparams_dict, static_params):\n",
    "    \"\"\"\n",
    "    Trains the model for one fold and evaluates it.\n",
    "    This function is designed to be run in a separate process.\n",
    "    \"\"\"\n",
    "    \n",
    "    return {\n",
    "        'train_loss': 1,\n",
    "        'val_loss': 1,\n",
    "        'val_accuracy': 1,\n",
    "        'val_recall': 1,\n",
    "        'val_f1': 1,\n",
    "        'time_taken': 1,\n",
    "        'early_stop_epoch': 1,\n",
    "        'detailed_train_loss': 1,\n",
    "        'detailed_val_loss': 1\n",
    "    }\n",
    "    # log_path = \"process_log.txt\"\n",
    "    # with open(log_path, \"a\") as log_file:\n",
    "    #     log_file.write(f\"Starting fold with params: {hyperparams_dict}\\n\")\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        hidden_layers = hyperparams_dict['hidden_layers']\n",
    "        lr = hyperparams_dict['lr']\n",
    "        batch_size = hyperparams_dict['batch_size']\n",
    "        optimiser = hyperparams_dict['optimiser']\n",
    "        bn = hyperparams_dict['bn']\n",
    "        weight_decay = hyperparams_dict['weight_decay']\n",
    "        dropout = hyperparams_dict['dropout']\n",
    "        \n",
    "        print('Starting a fold with hyperparameters:', hyperparams_dict)\n",
    "        # Initialize MLP model with current hyperparameters\n",
    "        nn = MLP(hidden_layers, static_params['activations'], bn, weight_decay, dropout)\n",
    "\n",
    "        # Fit the model\n",
    "        start = time.time()\n",
    "        nn_output = nn.fit(X_train, y_train, X_val, y_val, learning_rate=lr, epochs=static_params['epochs'], batch_size=batch_size, optimiser=optimiser, early_stopping=static_params['early_stopping'])\n",
    "        end = time.time()\n",
    "\n",
    "        # Extract metrics\n",
    "        train_loss, val_loss, early_stop_epoch = nn_output\n",
    "        time_taken = end - start\n",
    "\n",
    "        # Predict on validation set\n",
    "        output_val = nn.predict(X_val)\n",
    "\n",
    "        # Calculate metrics\n",
    "        val_accuracy = accuracy_score(np.argmax(y_val, axis=1), np.argmax(output_val, axis=1))\n",
    "        val_recall = recall_score(np.argmax(y_val, axis=1), np.argmax(output_val, axis=1), average='macro')\n",
    "        val_f1 = f1_score(np.argmax(y_val, axis=1), np.argmax(output_val, axis=1), average='macro')\n",
    "        \n",
    "        # Print metrics\n",
    "        print(f\"Train Loss: {train_loss[-1]}, Val Loss: {val_loss[-1]}, Val Accuracy: {val_accuracy}, Val Recall: {val_recall}, Val F1: {val_f1}, Time Taken: {time_taken}\")\n",
    "\n",
    "        return {\n",
    "            'train_loss': train_loss[-1],\n",
    "            'val_loss': val_loss[-1],\n",
    "            'val_accuracy': val_accuracy,\n",
    "            'val_recall': val_recall,\n",
    "            'val_f1': val_f1,\n",
    "            'time_taken': time_taken,\n",
    "            'early_stop_epoch': early_stop_epoch,\n",
    "            'detailed_train_loss': train_loss,\n",
    "            'detailed_val_loss': val_loss\n",
    "        }\n",
    "    except Exception as e:\n",
    "        with open('process_log.txt', \"a\") as log_file:\n",
    "            log_file.write(f\"Error: {e}\\n\")\n",
    "        return {\n",
    "            'train_loss': None,\n",
    "            'val_loss': None,\n",
    "            'val_accuracy': None,\n",
    "            'val_recall': None,\n",
    "            'val_f1': None,\n",
    "            'time_taken': None,\n",
    "            'early_stop_epoch': None,\n",
    "            'detailed_train_loss': None,\n",
    "            'detailed_val_loss': None\n",
    "        }\n",
    "        \n",
    "\n",
    "# Function to run your model CV asynchronously\n",
    "def run_model_cv_async(fold_splits, hyperparams_dict, static_params):\n",
    "    results = []\n",
    "    # with concurrent.futures.ProcessPoolExecutor(max_workers=10) as executor:\n",
    "    #     # Schedule the execution of each fold\n",
    "    #     print('Sending worker')\n",
    "    #     futures = [executor.submit(train_and_evaluate_fold, X_train, y_train, X_val, y_val, hyperparams_dict)\n",
    "    #                for (X_train, y_train, X_val, y_val) in fold_splits]\n",
    "\n",
    "    #     for future in concurrent.futures.as_completed(futures):\n",
    "    #         results.append(future.result())\n",
    "            \n",
    "    with concurrent.futures.ProcessPoolExecutor(max_workers=5) as executor:\n",
    "        futures = []\n",
    "        for X_train, y_train, X_val, y_val in fold_splits:\n",
    "            print('Sending worker')\n",
    "            print(f'X_train: {can_be_pickled(X_train)}, y_train: {can_be_pickled(y_train)}, X_val: {can_be_pickled(X_val)}, y_val: {can_be_pickled(y_val)}, hyperparams_dict: {can_be_pickled(hyperparams_dict)}, static_params: {can_be_pickled(static_params)}')\n",
    "            # Correctly pass the slices of your dataset and hyperparameters to the function\n",
    "            future = executor.submit(train_and_evaluate_fold, X_train, y_train, X_val, y_val, hyperparams_dict, static_params)\n",
    "            futures.append(future)\n",
    "            print('Worker sent')\n",
    "\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            results.append(future.result())\n",
    "            print('Worker finished')\n",
    "\n",
    "    return results\n",
    "    \n",
    "    print('Running in serial mode')\n",
    "    for (X_train, y_train, X_val, y_val) in fold_splits:\n",
    "        results.append(train_and_evaluate_fold(X_train, y_train, X_val, y_val, hyperparams_dict))\n",
    "\n",
    "    # After all futures are completed, results are collected\n",
    "    # Now, you can aggregate or process results as needed\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin Hyper-parameter Tuning -\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.001, 'optimiser': None, 'bn': False, 'batch_size': 10}\n",
      "True\n",
      "Sending worker\n",
      "X_train: True, y_train: True, X_val: True, y_val: True, hyperparams_dict: True, static_params: True\n",
      "Worker sent\n",
      "Error: A process in the process pool was terminated abruptly while the future was running or pending.\n",
      "Error: Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Sebyb\\AppData\\Local\\Temp\\ipykernel_33764\\1252734569.py\", line 19, in <module>\n",
      "    cv_metrics = run_model_cv_async(fold_splits, hyperparams_dict, SETUP)\n",
      "  File \"C:\\Users\\Sebyb\\AppData\\Local\\Temp\\ipykernel_33764\\4239297946.py\", line 211, in run_model_cv_async\n",
      "    results.append(future.result())\n",
      "  File \"c:\\Users\\Sebyb\\anaconda3\\lib\\concurrent\\futures\\_base.py\", line 439, in result\n",
      "    return self.__get_result()\n",
      "  File \"c:\\Users\\Sebyb\\anaconda3\\lib\\concurrent\\futures\\_base.py\", line 391, in __get_result\n",
      "    raise self._exception\n",
      "concurrent.futures.process.BrokenProcessPool: A process in the process pool was terminated abruptly while the future was running or pending.\n",
      "\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.001, 'optimiser': None, 'bn': False, 'batch_size': 5}\n",
      "True\n",
      "Sending worker\n",
      "X_train: True, y_train: True, X_val: True, y_val: True, hyperparams_dict: True, static_params: True\n",
      "Worker sent\n",
      "Error: A process in the process pool was terminated abruptly while the future was running or pending.\n",
      "Error: Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Sebyb\\AppData\\Local\\Temp\\ipykernel_33764\\1252734569.py\", line 19, in <module>\n",
      "    cv_metrics = run_model_cv_async(fold_splits, hyperparams_dict, SETUP)\n",
      "  File \"C:\\Users\\Sebyb\\AppData\\Local\\Temp\\ipykernel_33764\\4239297946.py\", line 211, in run_model_cv_async\n",
      "    results.append(future.result())\n",
      "  File \"c:\\Users\\Sebyb\\anaconda3\\lib\\concurrent\\futures\\_base.py\", line 439, in result\n",
      "    return self.__get_result()\n",
      "  File \"c:\\Users\\Sebyb\\anaconda3\\lib\\concurrent\\futures\\_base.py\", line 391, in __get_result\n",
      "    raise self._exception\n",
      "concurrent.futures.process.BrokenProcessPool: A process in the process pool was terminated abruptly while the future was running or pending.\n",
      "\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.001, 'optimiser': None, 'bn': False, 'batch_size': 2}\n",
      "True\n",
      "Sending worker\n",
      "X_train: True, y_train: True, X_val: True, y_val: True, hyperparams_dict: True, static_params: True\n",
      "Worker sent\n",
      "Error: A process in the process pool was terminated abruptly while the future was running or pending.\n",
      "Error: Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Sebyb\\AppData\\Local\\Temp\\ipykernel_33764\\1252734569.py\", line 19, in <module>\n",
      "    cv_metrics = run_model_cv_async(fold_splits, hyperparams_dict, SETUP)\n",
      "  File \"C:\\Users\\Sebyb\\AppData\\Local\\Temp\\ipykernel_33764\\4239297946.py\", line 211, in run_model_cv_async\n",
      "    results.append(future.result())\n",
      "  File \"c:\\Users\\Sebyb\\anaconda3\\lib\\concurrent\\futures\\_base.py\", line 439, in result\n",
      "    return self.__get_result()\n",
      "  File \"c:\\Users\\Sebyb\\anaconda3\\lib\\concurrent\\futures\\_base.py\", line 391, in __get_result\n",
      "    raise self._exception\n",
      "concurrent.futures.process.BrokenProcessPool: A process in the process pool was terminated abruptly while the future was running or pending.\n",
      "\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.001, 'optimiser': None, 'bn': True, 'batch_size': 10}\n",
      "True\n",
      "Sending worker\n",
      "X_train: True, y_train: True, X_val: True, y_val: True, hyperparams_dict: True, static_params: True\n",
      "Worker sent\n",
      "Error: A process in the process pool was terminated abruptly while the future was running or pending.\n",
      "Error: Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Sebyb\\AppData\\Local\\Temp\\ipykernel_33764\\1252734569.py\", line 19, in <module>\n",
      "    cv_metrics = run_model_cv_async(fold_splits, hyperparams_dict, SETUP)\n",
      "  File \"C:\\Users\\Sebyb\\AppData\\Local\\Temp\\ipykernel_33764\\4239297946.py\", line 211, in run_model_cv_async\n",
      "    results.append(future.result())\n",
      "  File \"c:\\Users\\Sebyb\\anaconda3\\lib\\concurrent\\futures\\_base.py\", line 439, in result\n",
      "    return self.__get_result()\n",
      "  File \"c:\\Users\\Sebyb\\anaconda3\\lib\\concurrent\\futures\\_base.py\", line 391, in __get_result\n",
      "    raise self._exception\n",
      "concurrent.futures.process.BrokenProcessPool: A process in the process pool was terminated abruptly while the future was running or pending.\n",
      "\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.001, 'optimiser': None, 'bn': True, 'batch_size': 5}\n",
      "True\n",
      "Sending worker\n",
      "X_train: True, y_train: True, X_val: True, y_val: True, hyperparams_dict: True, static_params: True\n",
      "Worker sent\n",
      "Error: A process in the process pool was terminated abruptly while the future was running or pending.\n",
      "Error: Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Sebyb\\AppData\\Local\\Temp\\ipykernel_33764\\1252734569.py\", line 19, in <module>\n",
      "    cv_metrics = run_model_cv_async(fold_splits, hyperparams_dict, SETUP)\n",
      "  File \"C:\\Users\\Sebyb\\AppData\\Local\\Temp\\ipykernel_33764\\4239297946.py\", line 211, in run_model_cv_async\n",
      "    results.append(future.result())\n",
      "  File \"c:\\Users\\Sebyb\\anaconda3\\lib\\concurrent\\futures\\_base.py\", line 439, in result\n",
      "    return self.__get_result()\n",
      "  File \"c:\\Users\\Sebyb\\anaconda3\\lib\\concurrent\\futures\\_base.py\", line 391, in __get_result\n",
      "    raise self._exception\n",
      "concurrent.futures.process.BrokenProcessPool: A process in the process pool was terminated abruptly while the future was running or pending.\n",
      "\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.001, 'optimiser': None, 'bn': True, 'batch_size': 2}\n",
      "True\n",
      "Sending worker\n",
      "X_train: True, y_train: True, X_val: True, y_val: True, hyperparams_dict: True, static_params: True\n",
      "Worker sent\n",
      "Error: A process in the process pool was terminated abruptly while the future was running or pending.\n",
      "Error: Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Sebyb\\AppData\\Local\\Temp\\ipykernel_33764\\1252734569.py\", line 19, in <module>\n",
      "    cv_metrics = run_model_cv_async(fold_splits, hyperparams_dict, SETUP)\n",
      "  File \"C:\\Users\\Sebyb\\AppData\\Local\\Temp\\ipykernel_33764\\4239297946.py\", line 211, in run_model_cv_async\n",
      "    results.append(future.result())\n",
      "  File \"c:\\Users\\Sebyb\\anaconda3\\lib\\concurrent\\futures\\_base.py\", line 439, in result\n",
      "    return self.__get_result()\n",
      "  File \"c:\\Users\\Sebyb\\anaconda3\\lib\\concurrent\\futures\\_base.py\", line 391, in __get_result\n",
      "    raise self._exception\n",
      "concurrent.futures.process.BrokenProcessPool: A process in the process pool was terminated abruptly while the future was running or pending.\n",
      "\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.001, 'optimiser': 'Adam', 'bn': False, 'batch_size': 10}\n",
      "True\n",
      "Sending worker\n",
      "X_train: True, y_train: True, X_val: True, y_val: True, hyperparams_dict: True, static_params: True\n",
      "Worker sent\n",
      "Error: A process in the process pool was terminated abruptly while the future was running or pending.\n",
      "Error: Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Sebyb\\AppData\\Local\\Temp\\ipykernel_33764\\1252734569.py\", line 19, in <module>\n",
      "    cv_metrics = run_model_cv_async(fold_splits, hyperparams_dict, SETUP)\n",
      "  File \"C:\\Users\\Sebyb\\AppData\\Local\\Temp\\ipykernel_33764\\4239297946.py\", line 211, in run_model_cv_async\n",
      "    results.append(future.result())\n",
      "  File \"c:\\Users\\Sebyb\\anaconda3\\lib\\concurrent\\futures\\_base.py\", line 439, in result\n",
      "    return self.__get_result()\n",
      "  File \"c:\\Users\\Sebyb\\anaconda3\\lib\\concurrent\\futures\\_base.py\", line 391, in __get_result\n",
      "    raise self._exception\n",
      "concurrent.futures.process.BrokenProcessPool: A process in the process pool was terminated abruptly while the future was running or pending.\n",
      "\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.001, 'optimiser': 'Adam', 'bn': False, 'batch_size': 5}\n",
      "True\n",
      "Sending worker\n",
      "X_train: True, y_train: True, X_val: True, y_val: True, hyperparams_dict: True, static_params: True\n",
      "Worker sent\n",
      "Error: A process in the process pool was terminated abruptly while the future was running or pending.\n",
      "Error: Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Sebyb\\AppData\\Local\\Temp\\ipykernel_33764\\1252734569.py\", line 19, in <module>\n",
      "    cv_metrics = run_model_cv_async(fold_splits, hyperparams_dict, SETUP)\n",
      "  File \"C:\\Users\\Sebyb\\AppData\\Local\\Temp\\ipykernel_33764\\4239297946.py\", line 211, in run_model_cv_async\n",
      "    results.append(future.result())\n",
      "  File \"c:\\Users\\Sebyb\\anaconda3\\lib\\concurrent\\futures\\_base.py\", line 439, in result\n",
      "    return self.__get_result()\n",
      "  File \"c:\\Users\\Sebyb\\anaconda3\\lib\\concurrent\\futures\\_base.py\", line 391, in __get_result\n",
      "    raise self._exception\n",
      "concurrent.futures.process.BrokenProcessPool: A process in the process pool was terminated abruptly while the future was running or pending.\n",
      "\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.001, 'optimiser': 'Adam', 'bn': False, 'batch_size': 2}\n",
      "True\n",
      "Sending worker\n",
      "X_train: True, y_train: True, X_val: True, y_val: True, hyperparams_dict: True, static_params: True\n",
      "Worker sent\n",
      "Error: A process in the process pool was terminated abruptly while the future was running or pending.\n",
      "Error: Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Sebyb\\AppData\\Local\\Temp\\ipykernel_33764\\1252734569.py\", line 19, in <module>\n",
      "    cv_metrics = run_model_cv_async(fold_splits, hyperparams_dict, SETUP)\n",
      "  File \"C:\\Users\\Sebyb\\AppData\\Local\\Temp\\ipykernel_33764\\4239297946.py\", line 211, in run_model_cv_async\n",
      "    results.append(future.result())\n",
      "  File \"c:\\Users\\Sebyb\\anaconda3\\lib\\concurrent\\futures\\_base.py\", line 439, in result\n",
      "    return self.__get_result()\n",
      "  File \"c:\\Users\\Sebyb\\anaconda3\\lib\\concurrent\\futures\\_base.py\", line 391, in __get_result\n",
      "    raise self._exception\n",
      "concurrent.futures.process.BrokenProcessPool: A process in the process pool was terminated abruptly while the future was running or pending.\n",
      "\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.001, 'optimiser': 'Adam', 'bn': True, 'batch_size': 10}\n",
      "True\n",
      "Sending worker\n",
      "X_train: True, y_train: True, X_val: True, y_val: True, hyperparams_dict: True, static_params: True\n",
      "Worker sent\n",
      "Error: A process in the process pool was terminated abruptly while the future was running or pending.\n",
      "Error: Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Sebyb\\AppData\\Local\\Temp\\ipykernel_33764\\1252734569.py\", line 19, in <module>\n",
      "    cv_metrics = run_model_cv_async(fold_splits, hyperparams_dict, SETUP)\n",
      "  File \"C:\\Users\\Sebyb\\AppData\\Local\\Temp\\ipykernel_33764\\4239297946.py\", line 211, in run_model_cv_async\n",
      "    results.append(future.result())\n",
      "  File \"c:\\Users\\Sebyb\\anaconda3\\lib\\concurrent\\futures\\_base.py\", line 439, in result\n",
      "    return self.__get_result()\n",
      "  File \"c:\\Users\\Sebyb\\anaconda3\\lib\\concurrent\\futures\\_base.py\", line 391, in __get_result\n",
      "    raise self._exception\n",
      "concurrent.futures.process.BrokenProcessPool: A process in the process pool was terminated abruptly while the future was running or pending.\n",
      "\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.001, 'optimiser': 'Adam', 'bn': True, 'batch_size': 5}\n",
      "True\n",
      "Sending worker\n",
      "X_train: True, y_train: True, X_val: True, y_val: True, hyperparams_dict: True, static_params: True\n",
      "Worker sent\n",
      "Error: A process in the process pool was terminated abruptly while the future was running or pending.\n",
      "Error: Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Sebyb\\AppData\\Local\\Temp\\ipykernel_33764\\1252734569.py\", line 19, in <module>\n",
      "    cv_metrics = run_model_cv_async(fold_splits, hyperparams_dict, SETUP)\n",
      "  File \"C:\\Users\\Sebyb\\AppData\\Local\\Temp\\ipykernel_33764\\4239297946.py\", line 211, in run_model_cv_async\n",
      "    results.append(future.result())\n",
      "  File \"c:\\Users\\Sebyb\\anaconda3\\lib\\concurrent\\futures\\_base.py\", line 439, in result\n",
      "    return self.__get_result()\n",
      "  File \"c:\\Users\\Sebyb\\anaconda3\\lib\\concurrent\\futures\\_base.py\", line 391, in __get_result\n",
      "    raise self._exception\n",
      "concurrent.futures.process.BrokenProcessPool: A process in the process pool was terminated abruptly while the future was running or pending.\n",
      "\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.001, 'optimiser': 'Adam', 'bn': True, 'batch_size': 2}\n",
      "True\n",
      "Sending worker\n",
      "X_train: True, y_train: True, X_val: True, y_val: True, hyperparams_dict: True, static_params: True\n",
      "Worker sent\n",
      "Error: A process in the process pool was terminated abruptly while the future was running or pending.\n",
      "Error: Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Sebyb\\AppData\\Local\\Temp\\ipykernel_33764\\1252734569.py\", line 19, in <module>\n",
      "    cv_metrics = run_model_cv_async(fold_splits, hyperparams_dict, SETUP)\n",
      "  File \"C:\\Users\\Sebyb\\AppData\\Local\\Temp\\ipykernel_33764\\4239297946.py\", line 211, in run_model_cv_async\n",
      "    results.append(future.result())\n",
      "  File \"c:\\Users\\Sebyb\\anaconda3\\lib\\concurrent\\futures\\_base.py\", line 439, in result\n",
      "    return self.__get_result()\n",
      "  File \"c:\\Users\\Sebyb\\anaconda3\\lib\\concurrent\\futures\\_base.py\", line 391, in __get_result\n",
      "    raise self._exception\n",
      "concurrent.futures.process.BrokenProcessPool: A process in the process pool was terminated abruptly while the future was running or pending.\n",
      "\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.001, 'optimiser': 'Momentum', 'bn': False, 'batch_size': 10}\n",
      "True\n",
      "Sending worker\n",
      "X_train: True, y_train: True, X_val: True, y_val: True, hyperparams_dict: True, static_params: True\n",
      "Worker sent\n",
      "Error: A process in the process pool was terminated abruptly while the future was running or pending.\n",
      "Error: Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Sebyb\\AppData\\Local\\Temp\\ipykernel_33764\\1252734569.py\", line 19, in <module>\n",
      "    cv_metrics = run_model_cv_async(fold_splits, hyperparams_dict, SETUP)\n",
      "  File \"C:\\Users\\Sebyb\\AppData\\Local\\Temp\\ipykernel_33764\\4239297946.py\", line 211, in run_model_cv_async\n",
      "    results.append(future.result())\n",
      "  File \"c:\\Users\\Sebyb\\anaconda3\\lib\\concurrent\\futures\\_base.py\", line 439, in result\n",
      "    return self.__get_result()\n",
      "  File \"c:\\Users\\Sebyb\\anaconda3\\lib\\concurrent\\futures\\_base.py\", line 391, in __get_result\n",
      "    raise self._exception\n",
      "concurrent.futures.process.BrokenProcessPool: A process in the process pool was terminated abruptly while the future was running or pending.\n",
      "\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.001, 'optimiser': 'Momentum', 'bn': False, 'batch_size': 5}\n",
      "True\n",
      "Sending worker\n",
      "X_train: True, y_train: True, X_val: True, y_val: True, hyperparams_dict: True, static_params: True\n",
      "Worker sent\n",
      "Error: A process in the process pool was terminated abruptly while the future was running or pending.\n",
      "Error: Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Sebyb\\AppData\\Local\\Temp\\ipykernel_33764\\1252734569.py\", line 19, in <module>\n",
      "    cv_metrics = run_model_cv_async(fold_splits, hyperparams_dict, SETUP)\n",
      "  File \"C:\\Users\\Sebyb\\AppData\\Local\\Temp\\ipykernel_33764\\4239297946.py\", line 211, in run_model_cv_async\n",
      "    results.append(future.result())\n",
      "  File \"c:\\Users\\Sebyb\\anaconda3\\lib\\concurrent\\futures\\_base.py\", line 439, in result\n",
      "    return self.__get_result()\n",
      "  File \"c:\\Users\\Sebyb\\anaconda3\\lib\\concurrent\\futures\\_base.py\", line 391, in __get_result\n",
      "    raise self._exception\n",
      "concurrent.futures.process.BrokenProcessPool: A process in the process pool was terminated abruptly while the future was running or pending.\n",
      "\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.001, 'optimiser': 'Momentum', 'bn': False, 'batch_size': 2}\n",
      "True\n",
      "Sending worker\n",
      "X_train: True, y_train: True, X_val: True, y_val: True, hyperparams_dict: True, static_params: True\n",
      "Worker sent\n",
      "Error: A process in the process pool was terminated abruptly while the future was running or pending.\n",
      "Error: Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Sebyb\\AppData\\Local\\Temp\\ipykernel_33764\\1252734569.py\", line 19, in <module>\n",
      "    cv_metrics = run_model_cv_async(fold_splits, hyperparams_dict, SETUP)\n",
      "  File \"C:\\Users\\Sebyb\\AppData\\Local\\Temp\\ipykernel_33764\\4239297946.py\", line 211, in run_model_cv_async\n",
      "    results.append(future.result())\n",
      "  File \"c:\\Users\\Sebyb\\anaconda3\\lib\\concurrent\\futures\\_base.py\", line 439, in result\n",
      "    return self.__get_result()\n",
      "  File \"c:\\Users\\Sebyb\\anaconda3\\lib\\concurrent\\futures\\_base.py\", line 391, in __get_result\n",
      "    raise self._exception\n",
      "concurrent.futures.process.BrokenProcessPool: A process in the process pool was terminated abruptly while the future was running or pending.\n",
      "\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.001, 'optimiser': 'Momentum', 'bn': True, 'batch_size': 10}\n",
      "True\n",
      "Sending worker\n",
      "X_train: True, y_train: True, X_val: True, y_val: True, hyperparams_dict: True, static_params: True\n",
      "Worker sent\n",
      "Error: A process in the process pool was terminated abruptly while the future was running or pending.\n",
      "Error: Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Sebyb\\AppData\\Local\\Temp\\ipykernel_33764\\1252734569.py\", line 19, in <module>\n",
      "    cv_metrics = run_model_cv_async(fold_splits, hyperparams_dict, SETUP)\n",
      "  File \"C:\\Users\\Sebyb\\AppData\\Local\\Temp\\ipykernel_33764\\4239297946.py\", line 211, in run_model_cv_async\n",
      "    results.append(future.result())\n",
      "  File \"c:\\Users\\Sebyb\\anaconda3\\lib\\concurrent\\futures\\_base.py\", line 439, in result\n",
      "    return self.__get_result()\n",
      "  File \"c:\\Users\\Sebyb\\anaconda3\\lib\\concurrent\\futures\\_base.py\", line 391, in __get_result\n",
      "    raise self._exception\n",
      "concurrent.futures.process.BrokenProcessPool: A process in the process pool was terminated abruptly while the future was running or pending.\n",
      "\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.001, 'optimiser': 'Momentum', 'bn': True, 'batch_size': 5}\n",
      "True\n",
      "Sending worker\n",
      "X_train: True, y_train: True, X_val: True, y_val: True, hyperparams_dict: True, static_params: True\n",
      "Worker sent\n",
      "Error: A process in the process pool was terminated abruptly while the future was running or pending.\n",
      "Error: Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Sebyb\\AppData\\Local\\Temp\\ipykernel_33764\\1252734569.py\", line 19, in <module>\n",
      "    cv_metrics = run_model_cv_async(fold_splits, hyperparams_dict, SETUP)\n",
      "  File \"C:\\Users\\Sebyb\\AppData\\Local\\Temp\\ipykernel_33764\\4239297946.py\", line 211, in run_model_cv_async\n",
      "    results.append(future.result())\n",
      "  File \"c:\\Users\\Sebyb\\anaconda3\\lib\\concurrent\\futures\\_base.py\", line 439, in result\n",
      "    return self.__get_result()\n",
      "  File \"c:\\Users\\Sebyb\\anaconda3\\lib\\concurrent\\futures\\_base.py\", line 391, in __get_result\n",
      "    raise self._exception\n",
      "concurrent.futures.process.BrokenProcessPool: A process in the process pool was terminated abruptly while the future was running or pending.\n",
      "\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.001, 'optimiser': 'Momentum', 'bn': True, 'batch_size': 2}\n",
      "True\n",
      "Sending worker\n",
      "X_train: True, y_train: True, X_val: True, y_val: True, hyperparams_dict: True, static_params: True\n",
      "Worker sent\n",
      "Error: A process in the process pool was terminated abruptly while the future was running or pending.\n",
      "Error: Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Sebyb\\AppData\\Local\\Temp\\ipykernel_33764\\1252734569.py\", line 19, in <module>\n",
      "    cv_metrics = run_model_cv_async(fold_splits, hyperparams_dict, SETUP)\n",
      "  File \"C:\\Users\\Sebyb\\AppData\\Local\\Temp\\ipykernel_33764\\4239297946.py\", line 211, in run_model_cv_async\n",
      "    results.append(future.result())\n",
      "  File \"c:\\Users\\Sebyb\\anaconda3\\lib\\concurrent\\futures\\_base.py\", line 439, in result\n",
      "    return self.__get_result()\n",
      "  File \"c:\\Users\\Sebyb\\anaconda3\\lib\\concurrent\\futures\\_base.py\", line 391, in __get_result\n",
      "    raise self._exception\n",
      "concurrent.futures.process.BrokenProcessPool: A process in the process pool was terminated abruptly while the future was running or pending.\n",
      "\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.0001, 'optimiser': None, 'bn': False, 'batch_size': 10}\n",
      "True\n",
      "Sending worker\n",
      "X_train: True, y_train: True, X_val: True, y_val: True, hyperparams_dict: True, static_params: True\n",
      "Worker sent\n",
      "Error: A process in the process pool was terminated abruptly while the future was running or pending.\n",
      "Error: Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Sebyb\\AppData\\Local\\Temp\\ipykernel_33764\\1252734569.py\", line 19, in <module>\n",
      "    cv_metrics = run_model_cv_async(fold_splits, hyperparams_dict, SETUP)\n",
      "  File \"C:\\Users\\Sebyb\\AppData\\Local\\Temp\\ipykernel_33764\\4239297946.py\", line 211, in run_model_cv_async\n",
      "    results.append(future.result())\n",
      "  File \"c:\\Users\\Sebyb\\anaconda3\\lib\\concurrent\\futures\\_base.py\", line 439, in result\n",
      "    return self.__get_result()\n",
      "  File \"c:\\Users\\Sebyb\\anaconda3\\lib\\concurrent\\futures\\_base.py\", line 391, in __get_result\n",
      "    raise self._exception\n",
      "concurrent.futures.process.BrokenProcessPool: A process in the process pool was terminated abruptly while the future was running or pending.\n",
      "\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.0001, 'optimiser': None, 'bn': False, 'batch_size': 5}\n",
      "True\n",
      "Sending worker\n",
      "X_train: True, y_train: True, X_val: True, y_val: True, hyperparams_dict: True, static_params: True\n",
      "Worker sent\n",
      "Error: A process in the process pool was terminated abruptly while the future was running or pending.\n",
      "Error: Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Sebyb\\AppData\\Local\\Temp\\ipykernel_33764\\1252734569.py\", line 19, in <module>\n",
      "    cv_metrics = run_model_cv_async(fold_splits, hyperparams_dict, SETUP)\n",
      "  File \"C:\\Users\\Sebyb\\AppData\\Local\\Temp\\ipykernel_33764\\4239297946.py\", line 211, in run_model_cv_async\n",
      "    results.append(future.result())\n",
      "  File \"c:\\Users\\Sebyb\\anaconda3\\lib\\concurrent\\futures\\_base.py\", line 439, in result\n",
      "    return self.__get_result()\n",
      "  File \"c:\\Users\\Sebyb\\anaconda3\\lib\\concurrent\\futures\\_base.py\", line 391, in __get_result\n",
      "    raise self._exception\n",
      "concurrent.futures.process.BrokenProcessPool: A process in the process pool was terminated abruptly while the future was running or pending.\n",
      "\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.0001, 'optimiser': None, 'bn': False, 'batch_size': 2}\n",
      "True\n",
      "Sending worker\n",
      "X_train: True, y_train: True, X_val: True, y_val: True, hyperparams_dict: True, static_params: True\n",
      "Worker sent\n",
      "Error: A process in the process pool was terminated abruptly while the future was running or pending.\n",
      "Error: Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Sebyb\\AppData\\Local\\Temp\\ipykernel_33764\\1252734569.py\", line 19, in <module>\n",
      "    cv_metrics = run_model_cv_async(fold_splits, hyperparams_dict, SETUP)\n",
      "  File \"C:\\Users\\Sebyb\\AppData\\Local\\Temp\\ipykernel_33764\\4239297946.py\", line 211, in run_model_cv_async\n",
      "    results.append(future.result())\n",
      "  File \"c:\\Users\\Sebyb\\anaconda3\\lib\\concurrent\\futures\\_base.py\", line 439, in result\n",
      "    return self.__get_result()\n",
      "  File \"c:\\Users\\Sebyb\\anaconda3\\lib\\concurrent\\futures\\_base.py\", line 391, in __get_result\n",
      "    raise self._exception\n",
      "concurrent.futures.process.BrokenProcessPool: A process in the process pool was terminated abruptly while the future was running or pending.\n",
      "\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.0001, 'optimiser': None, 'bn': True, 'batch_size': 10}\n",
      "True\n",
      "Sending worker\n",
      "X_train: True, y_train: True, X_val: True, y_val: True, hyperparams_dict: True, static_params: True\n",
      "Worker sent\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Use the asynchronous version here\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m cv_metrics \u001b[38;5;241m=\u001b[39m \u001b[43mrun_model_cv_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfold_splits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhyperparams_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSETUP\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     21\u001b[0m time_taken \u001b[38;5;241m=\u001b[39m end \u001b[38;5;241m-\u001b[39m start\n",
      "Cell \u001b[1;32mIn[29], line 210\u001b[0m, in \u001b[0;36mrun_model_cv_async\u001b[1;34m(fold_splits, hyperparams_dict, static_params)\u001b[0m\n\u001b[0;32m    207\u001b[0m     futures\u001b[38;5;241m.\u001b[39mappend(future)\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWorker sent\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 210\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m future \u001b[38;5;129;01min\u001b[39;00m concurrent\u001b[38;5;241m.\u001b[39mfutures\u001b[38;5;241m.\u001b[39mas_completed(futures):\n\u001b[0;32m    211\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend(future\u001b[38;5;241m.\u001b[39mresult())\n\u001b[0;32m    212\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWorker finished\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Sebyb\\anaconda3\\lib\\concurrent\\futures\\_base.py:245\u001b[0m, in \u001b[0;36mas_completed\u001b[1;34m(fs, timeout)\u001b[0m\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m wait_timeout \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    241\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m(\n\u001b[0;32m    242\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m (of \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m) futures unfinished\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (\n\u001b[0;32m    243\u001b[0m                 \u001b[38;5;28mlen\u001b[39m(pending), total_futures))\n\u001b[1;32m--> 245\u001b[0m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m waiter\u001b[38;5;241m.\u001b[39mlock:\n\u001b[0;32m    248\u001b[0m     finished \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39mfinished_futures\n",
      "File \u001b[1;32mc:\\Users\\Sebyb\\anaconda3\\lib\\threading.py:581\u001b[0m, in \u001b[0;36mEvent.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    579\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[0;32m    580\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[1;32m--> 581\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[1;32mc:\\Users\\Sebyb\\anaconda3\\lib\\threading.py:312\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    310\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[0;32m    311\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 312\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    313\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    314\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "outputs = []\n",
    "\n",
    "print('Begin Hyper-parameter Tuning -')\n",
    "for hyperparams in itertools.product(weight_decay_options, drop_out_options, hidden_layer_options, lr_options, optimiser_options, bn_option, batch_size_options):\n",
    "    hyperparams_dict = {\n",
    "        'weight_decay': hyperparams[0],\n",
    "        'dropout': hyperparams[1],\n",
    "        'hidden_layers': hyperparams[2],\n",
    "        'lr': hyperparams[3],\n",
    "        'optimiser': hyperparams[4],\n",
    "        'bn': hyperparams[5],\n",
    "        'batch_size': hyperparams[6]\n",
    "    }\n",
    "    print(f'\\n\\n### Running model {str(hyperparams_dict)}')\n",
    "    print(can_be_pickled(hyperparams_dict))\n",
    "    try:\n",
    "        start = time.time()\n",
    "        # Use the asynchronous version here\n",
    "        cv_metrics = run_model_cv_async(fold_splits, hyperparams_dict, SETUP)\n",
    "        end = time.time()\n",
    "        time_taken = end - start\n",
    "        \n",
    "        # Process and log the cv_metrics as needed\n",
    "        # Ensure cv_metrics are aggregated properly from the async results\n",
    "        \n",
    "        log_detailed_metrics(hyperparams_dict, cv_metrics, time_taken)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'Error: {e}')\n",
    "        traceback_str = traceback.format_exc()  # This gives you the full traceback as a string\n",
    "        print(f'Error: {traceback_str}')\n",
    "        # try:\n",
    "        #     log_detailed_metrics(hyperparams_dict, None, 99999)  # Log a dummy value in case of error\n",
    "        # except:\n",
    "        #     pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "BrokenProcessPool",
     "evalue": "A process in the process pool was terminated abruptly while the future was running or pending.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBrokenProcessPool\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m futures \u001b[38;5;241m=\u001b[39m [executor\u001b[38;5;241m.\u001b[39msubmit(simple_function, i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m)]\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m future \u001b[38;5;129;01min\u001b[39;00m concurrent\u001b[38;5;241m.\u001b[39mfutures\u001b[38;5;241m.\u001b[39mas_completed(futures):\n\u001b[1;32m----> 9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\Sebyb\\anaconda3\\lib\\concurrent\\futures\\_base.py:439\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    437\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    438\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 439\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[0;32m    443\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[1;32mc:\\Users\\Sebyb\\anaconda3\\lib\\concurrent\\futures\\_base.py:391\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    390\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 391\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    392\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    393\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    394\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mBrokenProcessPool\u001b[0m: A process in the process pool was terminated abruptly while the future was running or pending."
     ]
    }
   ],
   "source": [
    "import concurrent.futures\n",
    "\n",
    "def simple_function(x):\n",
    "    return x * x\n",
    "\n",
    "with concurrent.futures.ProcessPoolExecutor(max_workers=5) as executor:\n",
    "    futures = [executor.submit(simple_function, i) for i in range(10)]\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        print(future.result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Constants ###\n",
    "\n",
    "# SETUP = {\n",
    "#     'epochs': 40,\n",
    "#     'activations': [None, 'ReLU', 'ReLU', 'softmax'],\n",
    "#     'input_size': 128,\n",
    "#     'early_stopping': (10, 0.001)\n",
    "# }\n",
    "\n",
    "# ### Options for Hyper-Parameters ###\n",
    "\n",
    "# hidden_layer_options = [[128, 64, 32, 10], [128, 96, 64, 10]]\n",
    "# lr_options = [0.001, 0.0001, 0.01]\n",
    "# batch_size_options = [128, 64, 16, 4]\n",
    "# optimiser_options = [None, 'Adam', 'Momentum']\n",
    "# bn_option = [False, True]\n",
    "# weight_decay_options = [0.0, 0.0001, 0.001]\n",
    "# drop_out_options = [[0.0, 0.0, 0.0, 0.0], [0.0, 0.2, 0.2, 0.0], [0.2, 0.3, 0.3, 0.2], [0.0, 0.4, 0.4, 0.0]]\n",
    "\n",
    "# lr_options = [0.005, 0.001, 0.0001]\n",
    "# batch_size_options = [4,8,16]\n",
    "# optimiser_options = ['Adam', 'Momentum'] # None as well\n",
    "# bn_option = [False, True]\n",
    "\n",
    "# dropout_options = []\n",
    "# size_options = []\n",
    "# weight_decay_options\n",
    "# activation_options = ['ReLU', 'tanh']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_to_file(hyperparams_dict, cv_metrics, time_taken, filename=\"model_performance_2.csv\"):\n",
    "    # Check if file exists, if not, write headers\n",
    "    file_exists = os.path.isfile(filename)\n",
    "    hidden_layers = hyperparams_dict['hidden_layers']\n",
    "    lr = hyperparams_dict['lr']\n",
    "    batch_size = hyperparams_dict['batch_size']\n",
    "    optimiser = hyperparams_dict['optimiser']\n",
    "    bn = hyperparams_dict['bn']\n",
    "    weight_decay = hyperparams_dict['weight_decay']\n",
    "    dropout = hyperparams_dict['dropout']\n",
    "    \n",
    "    with open(filename, mode='a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        if not file_exists:\n",
    "            # Write the header\n",
    "            # CV Metrics structure cv_train_loss_scores, cv_val_loss_scores, cv_val_accuracy_scores, cv_val_recall_scores, cv_val_f1_scores, cv_times, detailed_train_losses, detailed_val_losses\n",
    "            \n",
    "            writer.writerow([\"Train Loss\", \"Val Loss\", \"Val Accuracy\", \"Val Recall\", \"Val F1\", \"Learning Rate\", \"Batch Size\", \"Optimizer\", \"Batch Norm\", \"Weight Decay\", \"Dropout\", \"Hidden Layers\", \"Time Taken Test\", \"Time Taken CV\", \"Train Losses CV\", \"Val Losses CV\"])\n",
    "        \n",
    "        # Write the data\n",
    "        for i, (cv_train_loss_score, cv_val_loss_score, cv_val_accuracy_score, cv_val_recall_score, cv_val_f1_score, cv_time, detailed_train_loss, detailed_val_loss) in enumerate(zip(*cv_metrics)):\n",
    "            # writer.writerow([hyperparams[0], hyperparams[1], hyperparams[2], hyperparams[3], hyperparams[4], hyperparams[5], hyperparams[6], cv_train_loss_scores, cv_val_loss_scores, cv_val_accuracy_scores, cv_val_recall_scores, cv_val_f1_scores, time_taken, cv_times, detailed_train_losses, detailed_val_losses])\n",
    "            # writer.writerow([round(cv_train_loss_scores, 4), round(cv_val_loss_scores, 4), round(cv_val_accuracy_scores, 4), round(cv_val_recall_scores, 4), round(cv_val_f1_scores, 4), lr, batch_size, optimiser, bn, weight_decay, dropout, hidden_layers, time_taken, cv_times, detailed_train_losses, detailed_val_losses])\n",
    "            writer.writerow([\\\n",
    "                round(cv_train_loss_score, 4), \n",
    "                round(cv_val_loss_score, 4), \n",
    "                round(cv_val_accuracy_score, 4), \n",
    "                round(cv_val_recall_score, 4), \n",
    "                round(cv_val_f1_score, 4), \n",
    "                hyperparams_dict['lr'], \n",
    "                hyperparams_dict['batch_size'], \n",
    "                hyperparams_dict['optimiser'], \n",
    "                hyperparams_dict['bn'], \n",
    "                hyperparams_dict['weight_decay'], \n",
    "                hyperparams_dict['dropout'], \n",
    "                hyperparams_dict['hidden_layers'], \n",
    "                time_taken, \n",
    "                cv_time,\n",
    "                detailed_train_loss, \n",
    "                detailed_val_loss\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Constants ###\n",
    "\n",
    "SETUP = {\n",
    "    'epochs': 50,\n",
    "    'activations': [None, 'ReLU', 'ReLU', 'softmax'],\n",
    "    'input_size': 128,\n",
    "    'early_stopping': (10, 0.001)\n",
    "}\n",
    "\n",
    "### Options for Hyper-Parameters ###\n",
    "\n",
    "weight_decay_options = [0.0, 0.0001, 0.001]\n",
    "drop_out_options = [[0.0, 0.0, 0.0, 0.0], [0.5, 0.2, 0.2, 0.0], [0.1, 0.3, 0.3, 0.1]]\n",
    "hidden_layer_options = [[128, 64, 32, 10], [128, 96, 64, 10]]\n",
    "lr_options = [0.001, 0.0001, 0.01]\n",
    "optimiser_options = [None, 'Adam', 'Momentum']\n",
    "bn_option = [False, True]\n",
    "batch_size_options = [16,8,4,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_detailed_metrics(hyperparams_dict, cv_metrics, filename=\"detailed_model_performance.csv\"):\n",
    "    # Check if file exists, if not, write headers\n",
    "    file_exists = os.path.isfile(filename)\n",
    "    \n",
    "    with open(filename, mode='a', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        if not file_exists:\n",
    "            writer.writerow([\"Fold\", \"Hyperparameters\", \"Train Losses\", \"Val Losses\", \"Val Accuracy\", \"Val Recall\", \"Val F1\", \"Time Taken\", \"Early Stop Epoch\"])\n",
    "\n",
    "        for i, metrics in enumerate(cv_metrics):\n",
    "            # Serialize detailed metrics as JSON strings\n",
    "            detailed_train_loss = json.dumps(metrics['detailed_train_loss'])\n",
    "            detailed_val_loss = json.dumps(metrics['detailed_val_loss'])\n",
    "            writer.writerow([\n",
    "                i + 1,\n",
    "                json.dumps(hyperparams_dict),  # Hyperparameters as JSON\n",
    "                detailed_train_loss,\n",
    "                detailed_val_loss,\n",
    "                metrics['val_accuracy'],\n",
    "                metrics['val_recall'],\n",
    "                metrics['val_f1'],\n",
    "                metrics['time_taken'],\n",
    "                metrics['early_stop_epoch']\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_fold(X_train, y_train, X_val, y_val, hyperparams_dict):\n",
    "    \"\"\"\n",
    "    Trains the model for one fold and evaluates it.\n",
    "    This function is designed to be run in a separate process.\n",
    "    \"\"\"\n",
    "    hidden_layers, lr, batch_size = hyperparams_dict['hidden_layers'], hyperparams_dict['lr'], hyperparams_dict['batch_size']\n",
    "    optimiser, bn, weight_decay, dropout = hyperparams_dict['optimiser'], hyperparams_dict['bn'], hyperparams_dict['weight_decay'], hyperparams_dict['dropout']\n",
    "\n",
    "    # Initialize MLP model with current hyperparameters\n",
    "    nn = MLP(hidden_layers, SETUP['activations'], bn, weight_decay, dropout)\n",
    "\n",
    "    # Fit the model\n",
    "    start = time.time()\n",
    "    nn_output = nn.fit(X_train, y_train, X_val, y_val, learning_rate=lr, epochs=SETUP['epochs'], batch_size=batch_size, optimiser=optimiser, early_stopping=SETUP['early_stopping'])\n",
    "    end = time.time()\n",
    "\n",
    "    # Extract metrics\n",
    "    train_loss, val_loss, early_stop_epoch = nn_output\n",
    "    time_taken = end - start\n",
    "\n",
    "    # Predict on validation set\n",
    "    output_val = nn.predict(X_val)\n",
    "\n",
    "    # Calculate metrics\n",
    "    val_accuracy = accuracy_score(np.argmax(y_val, axis=1), np.argmax(output_val, axis=1))\n",
    "    val_recall = recall_score(np.argmax(y_val, axis=1), np.argmax(output_val, axis=1), average='macro')\n",
    "    val_f1 = f1_score(np.argmax(y_val, axis=1), np.argmax(output_val, axis=1), average='macro')\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f\"Train Loss: {train_loss[-1]}, Val Loss: {val_loss[-1]}, Val Accuracy: {val_accuracy}, Val Recall: {val_recall}, Val F1: {val_f1}, Time Taken: {time_taken}\")\n",
    "\n",
    "    return {\n",
    "        'train_loss': train_loss[-1],\n",
    "        'val_loss': val_loss[-1],\n",
    "        'val_accuracy': val_accuracy,\n",
    "        'val_recall': val_recall,\n",
    "        'val_f1': val_f1,\n",
    "        'time_taken': time_taken,\n",
    "        'early_stop_epoch': early_stop_epoch,\n",
    "        'detailed_train_loss': train_loss,\n",
    "        'detailed_val_loss': val_loss\n",
    "    }\n",
    "\n",
    "# Function to run your model CV asynchronously\n",
    "def run_model_cv_async(fold_splits, hyperparams_dict):\n",
    "    results = []\n",
    "    with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "        # Schedule the execution of each fold\n",
    "        futures = [executor.submit(train_and_evaluate_fold, X_train, y_train, X_val, y_val, hyperparams_dict)\n",
    "                   for (X_train, y_train, X_val, y_val) in fold_splits]\n",
    "\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            results.append(future.result())\n",
    "\n",
    "    # After all futures are completed, results are collected\n",
    "    # Now, you can aggregate or process results as needed\n",
    "    return results\n",
    "\n",
    "# # Example usage:\n",
    "# # Ensure you have defined `fold_splits` and `hyperparams_dict` appropriately\n",
    "# results = run_model_cv_async(fold_splits, hyperparams_dict)\n",
    "# for result in results:\n",
    "#     print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin Hyper-parameter Tuning -\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.001, 'optimiser': None, 'bn': False, 'batch_size': 16}\n"
     ]
    }
   ],
   "source": [
    "# Seed for reproducibility\n",
    "np.random.seed(0)\n",
    "\n",
    "# Generate training and validation datasets for each fold\n",
    "fold_splits = []\n",
    "for i in range(n_folds):\n",
    "        # Generate training and validation sets for this fold\n",
    "        val_indices = folds[i]\n",
    "        train_indices = np.hstack(folds[:i] + folds[i+1:])\n",
    "        \n",
    "        X_train, y_train = X[train_indices], y[train_indices]\n",
    "        X_val, y_val = X[val_indices], y[val_indices]\n",
    "        fold_splits.append((X_train, y_train, X_val, y_val))\n",
    "\n",
    "outputs = []\n",
    "\n",
    "print('Begin Hyper-parameter Tuning -')\n",
    "for hyperparams in itertools.product(weight_decay_options, drop_out_options, hidden_layer_options, lr_options, optimiser_options, bn_option, batch_size_options):\n",
    "    hyperparams_dict = {\n",
    "        'weight_decay': hyperparams[0],\n",
    "        'dropout': hyperparams[1],\n",
    "        'hidden_layers': hyperparams[2],\n",
    "        'lr': hyperparams[3],\n",
    "        'optimiser': hyperparams[4],\n",
    "        'bn': hyperparams[5],\n",
    "        'batch_size': hyperparams[6]\n",
    "    }\n",
    "    print(f'\\n\\n### Running model {str(hyperparams_dict)}')\n",
    "    try:\n",
    "        start = time.time()\n",
    "        # Use the asynchronous version here\n",
    "        cv_metrics = run_model_cv_async(fold_splits, hyperparams_dict)\n",
    "        end = time.time()\n",
    "        time_taken = end - start\n",
    "        \n",
    "        # Process and log the cv_metrics as needed\n",
    "        # Ensure cv_metrics are aggregated properly from the async results\n",
    "        \n",
    "        log_detailed_metrics(hyperparams_dict, cv_metrics, time_taken)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'Error: {e}')\n",
    "        try:\n",
    "            log_detailed_metrics(hyperparams_dict, None, 99999)  # Log a dummy value in case of error\n",
    "        except:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to run your model\n",
    "def run_model_cv(fold_splits, hyperparams_dict):\n",
    "    # weight_decay_options, drop_out_options, hidden_layer_options, lr_options, optimiser_options, bn_option, batch_size_options\n",
    "    cv_train_loss_scores = []\n",
    "    cv_val_loss_scores = []\n",
    "    detailed_train_losses = []\n",
    "    detailed_val_losses = []\n",
    "    cv_val_accuracy_scores = []\n",
    "    cv_val_recall_scores = []\n",
    "    cv_val_f1_scores = []\n",
    "    cv_times = []\n",
    "    hidden_layers, lr, batch_size = hyperparams_dict['hidden_layers'], hyperparams_dict['lr'], hyperparams_dict['batch_size']\n",
    "    optimiser, bn, weight_decay, dropout = hyperparams_dict['optimiser'], hyperparams_dict['bn'], hyperparams_dict['weight_decay'], hyperparams_dict['dropout']\n",
    "    \n",
    "    for i in range(n_folds):\n",
    "        X_train, y_train, X_val, y_val = fold_splits[i]\n",
    "        \n",
    "        # Initialize MLP model with current hyperparameters\n",
    "        nn = MLP(hidden_layers, SETUP['activations'], bn, weight_decay, dropout)\n",
    "        \n",
    "        # Fit the model\n",
    "        start = time.time()\n",
    "        nn_output = nn.fit(X_train, y_train, X_val, y_val, learning_rate=lr, epochs=SETUP['epochs'], batch_size=batch_size, optimiser=optimiser, early_stopping=SETUP['early_stopping'])\n",
    "        end = time.time()\n",
    "        \n",
    "        # Extract metrics\n",
    "        train_loss, val_loss, early_stop_epoch = nn_output\n",
    "        time_taken = end - start\n",
    "        cv_times.append(time_taken)\n",
    "        \n",
    "        # Predict on validation set\n",
    "        output_val = nn.predict(X_val)\n",
    "        \n",
    "        # Calulate metrics\n",
    "        val_accuracy = accuracy_score(np.argmax(y_val, axis=1), np.argmax(output_val, axis=1))\n",
    "        val_recall = recall_score(np.argmax(y_val, axis=1), np.argmax(output_val, axis=1), average='macro')\n",
    "        val_f1 = f1_score(np.argmax(y_val, axis=1), np.argmax(output_val, axis=1), average='macro')\n",
    "        \n",
    "        # Append metrics for this fold\n",
    "        cv_train_loss_scores.append(train_loss[-1])\n",
    "        cv_val_loss_scores.append(val_loss[-1])\n",
    "        detailed_train_losses.append(train_loss)\n",
    "        detailed_val_losses.append(val_loss)\n",
    "        cv_val_accuracy_scores.append(val_accuracy)\n",
    "        cv_val_recall_scores.append(val_recall)\n",
    "        cv_val_f1_scores.append(val_f1)\n",
    "\n",
    "        print(f'CV_{i+1} ({time_taken:.2f}s) -\\t train_loss: {train_loss[-1]:.5f}\\t val_loss: {val_loss[-1]:.5f}\\t val_acc: {val_accuracy:.5f}\\t val_recall: {val_recall:.5f}\\t val_f1: {val_f1:.5f}')\n",
    "    \n",
    "    return cv_train_loss_scores, cv_val_loss_scores, cv_val_accuracy_scores, cv_val_recall_scores, cv_val_f1_scores, cv_times, detailed_train_losses, detailed_val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin Hyper-parameter Tuning -\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.001, 'optimiser': None, 'bn': False, 'batch_size': 16}\n",
      "\tearly stopping at ep.17\t train loss: 0.88703\t val loss: 2.44500\t best val loss: 2.37403\n",
      "CV_1 (7.34s) -\t train_loss: 0.88703\t val_loss: 2.44500\t val_acc: 0.33450\t val_recall: 0.33385\t val_f1: 0.32795\n",
      "\tearly stopping at ep.11\t train loss: 0.78747\t val loss: 2.82258\t best val loss: 2.37687\n",
      "CV_2 (5.21s) -\t train_loss: 0.78747\t val_loss: 2.82258\t val_acc: 0.32850\t val_recall: 0.32779\t val_f1: 0.32160\n",
      "\tearly stopping at ep.30\t train loss: 0.83872\t val loss: 2.44399\t best val loss: 2.38876\n",
      "CV_3 (12.88s) -\t train_loss: 0.83872\t val_loss: 2.44399\t val_acc: 0.35430\t val_recall: 0.35525\t val_f1: 0.34826\n",
      "\tearly stopping at ep.12\t train loss: 0.82074\t val loss: 2.47603\t best val loss: 2.46724\n",
      "CV_4 (5.09s) -\t train_loss: 0.82074\t val_loss: 2.47603\t val_acc: 0.34810\t val_recall: 0.34858\t val_f1: 0.34445\n",
      "\tearly stopping at ep.33\t train loss: 0.76007\t val loss: 2.75853\t best val loss: 2.32553\n",
      "CV_5 (14.25s) -\t train_loss: 0.76007\t val_loss: 2.75853\t val_acc: 0.35730\t val_recall: 0.35617\t val_f1: 0.35308\n",
      "## Experiment results (44.95s) - {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.001, 'optimiser': None, 'bn': False, 'batch_size': 16}\n",
      "# Metrics - train loss: 0.81881 \t val loss: 2.58923 \t val accuracy: 0.34454 \t val recall: 0.34433 \t val f1: 0.33907\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.001, 'optimiser': None, 'bn': False, 'batch_size': 8}\n",
      "\tearly stopping at ep.14\t train loss: 0.86718\t val loss: 1.96615\t best val loss: 1.88581\n",
      "CV_1 (10.19s) -\t train_loss: 0.86718\t val_loss: 1.96615\t val_acc: 0.39330\t val_recall: 0.39384\t val_f1: 0.38720\n",
      "\tearly stopping at ep.32\t train loss: 0.78311\t val loss: 2.15054\t best val loss: 1.91465\n",
      "CV_2 (28.71s) -\t train_loss: 0.78311\t val_loss: 2.15054\t val_acc: 0.39620\t val_recall: 0.39462\t val_f1: 0.39793\n",
      "\tearly stopping at ep.33\t train loss: 0.81120\t val loss: 2.06469\t best val loss: 1.96513\n",
      "CV_3 (27.36s) -\t train_loss: 0.81120\t val_loss: 2.06469\t val_acc: 0.41180\t val_recall: 0.41165\t val_f1: 0.40550\n",
      "\tearly stopping at ep.14\t train loss: 0.86021\t val loss: 2.12154\t best val loss: 1.92820\n",
      "CV_4 (10.14s) -\t train_loss: 0.86021\t val_loss: 2.12154\t val_acc: 0.37540\t val_recall: 0.37661\t val_f1: 0.37412\n",
      "\tearly stopping at ep.16\t train loss: 0.85920\t val loss: 2.12743\t best val loss: 1.87576\n",
      "CV_5 (12.03s) -\t train_loss: 0.85920\t val_loss: 2.12743\t val_acc: 0.40060\t val_recall: 0.39909\t val_f1: 0.38818\n",
      "## Experiment results (88.60s) - {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.001, 'optimiser': None, 'bn': False, 'batch_size': 8}\n",
      "# Metrics - train loss: 0.83618 \t val loss: 2.08607 \t val accuracy: 0.39546 \t val recall: 0.39516 \t val f1: 0.39059\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.001, 'optimiser': None, 'bn': False, 'batch_size': 4}\n",
      "\tearly stopping at ep.21\t train loss: 1.05286\t val loss: 1.70487\t best val loss: 1.59948\n",
      "CV_1 (27.89s) -\t train_loss: 1.05286\t val_loss: 1.70487\t val_acc: 0.43910\t val_recall: 0.44166\t val_f1: 0.42470\n",
      "\tearly stopping at ep.30\t train loss: 1.01642\t val loss: 1.62413\t best val loss: 1.59429\n",
      "CV_2 (39.69s) -\t train_loss: 1.01642\t val_loss: 1.62413\t val_acc: 0.45460\t val_recall: 0.45341\t val_f1: 0.45248\n",
      "\tearly stopping at ep.15\t train loss: 1.07209\t val loss: 1.66409\t best val loss: 1.62657\n",
      "CV_3 (20.86s) -\t train_loss: 1.07209\t val_loss: 1.66409\t val_acc: 0.44220\t val_recall: 0.44172\t val_f1: 0.43774\n",
      "\tearly stopping at ep.32\t train loss: 1.00083\t val loss: 1.68042\t best val loss: 1.57493\n",
      "CV_4 (42.38s) -\t train_loss: 1.00083\t val_loss: 1.68042\t val_acc: 0.44690\t val_recall: 0.44604\t val_f1: 0.44427\n",
      "\tearly stopping at ep.38\t train loss: 1.03120\t val loss: 1.66753\t best val loss: 1.55952\n",
      "CV_5 (50.06s) -\t train_loss: 1.03120\t val_loss: 1.66753\t val_acc: 0.45090\t val_recall: 0.45083\t val_f1: 0.43804\n",
      "## Experiment results (181.06s) - {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.001, 'optimiser': None, 'bn': False, 'batch_size': 4}\n",
      "# Metrics - train loss: 1.03468 \t val loss: 1.66821 \t val accuracy: 0.44674 \t val recall: 0.44673 \t val f1: 0.43945\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.001, 'optimiser': None, 'bn': False, 'batch_size': 2}\n",
      "\tearly stopping at ep.37\t train loss: 1.23708\t val loss: 1.44786\t best val loss: 1.42962\n",
      "CV_1 (93.76s) -\t train_loss: 1.23708\t val_loss: 1.44786\t val_acc: 0.48900\t val_recall: 0.49077\t val_f1: 0.48058\n",
      "\tearly stopping at ep.43\t train loss: 1.23243\t val loss: 1.43095\t best val loss: 1.41871\n",
      "CV_2 (116.62s) -\t train_loss: 1.23243\t val_loss: 1.43095\t val_acc: 0.48630\t val_recall: 0.48469\t val_f1: 0.48216\n",
      "\tearly stopping at ep.28\t train loss: 1.24398\t val loss: 1.48947\t best val loss: 1.47187\n",
      "CV_3 (76.31s) -\t train_loss: 1.24398\t val_loss: 1.48947\t val_acc: 0.47700\t val_recall: 0.47738\t val_f1: 0.47103\n",
      "\tearly stopping at ep.47\t train loss: 1.23498\t val loss: 1.45687\t best val loss: 1.43657\n",
      "CV_4 (120.69s) -\t train_loss: 1.23498\t val_loss: 1.45687\t val_acc: 0.48910\t val_recall: 0.48913\t val_f1: 0.48866\n",
      "\tearly stopping at ep.33\t train loss: 1.23873\t val loss: 1.46013\t best val loss: 1.42760\n",
      "CV_5 (86.96s) -\t train_loss: 1.23873\t val_loss: 1.46013\t val_acc: 0.48620\t val_recall: 0.48705\t val_f1: 0.48292\n",
      "## Experiment results (494.53s) - {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.001, 'optimiser': None, 'bn': False, 'batch_size': 2}\n",
      "# Metrics - train loss: 1.23744 \t val loss: 1.45706 \t val accuracy: 0.48552 \t val recall: 0.48580 \t val f1: 0.48107\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.001, 'optimiser': None, 'bn': True, 'batch_size': 16}\n",
      "\tearly stopping at ep.11\t train loss: 0.85672\t val loss: 2.87876\t best val loss: 2.64957\n",
      "CV_1 (11.63s) -\t train_loss: 0.85672\t val_loss: 2.87876\t val_acc: 0.27430\t val_recall: 0.27447\t val_f1: 0.27587\n",
      "\tearly stopping at ep.13\t train loss: 0.89339\t val loss: 3.12291\t best val loss: 2.75233\n",
      "CV_2 (13.98s) -\t train_loss: 0.89339\t val_loss: 3.12291\t val_acc: 0.26250\t val_recall: 0.26163\t val_f1: 0.26095\n",
      "\tearly stopping at ep.11\t train loss: 0.84573\t val loss: 2.93766\t best val loss: 2.68660\n",
      "CV_3 (11.84s) -\t train_loss: 0.84573\t val_loss: 2.93766\t val_acc: 0.26410\t val_recall: 0.26369\t val_f1: 0.26120\n",
      "\tearly stopping at ep.11\t train loss: 0.85626\t val loss: 2.75295\t best val loss: 2.67460\n",
      "CV_4 (11.93s) -\t train_loss: 0.85626\t val_loss: 2.75295\t val_acc: 0.30290\t val_recall: 0.30303\t val_f1: 0.29654\n",
      "\tearly stopping at ep.11\t train loss: 0.84862\t val loss: 2.77385\t best val loss: 2.65860\n",
      "CV_5 (11.71s) -\t train_loss: 0.84862\t val_loss: 2.77385\t val_acc: 0.28830\t val_recall: 0.28733\t val_f1: 0.28198\n",
      "## Experiment results (61.39s) - {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.001, 'optimiser': None, 'bn': True, 'batch_size': 16}\n",
      "# Metrics - train loss: 0.86015 \t val loss: 2.89323 \t val accuracy: 0.27842 \t val recall: 0.27803 \t val f1: 0.27531\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.001, 'optimiser': None, 'bn': True, 'batch_size': 8}\n",
      "\tearly stopping at ep.21\t train loss: 1.36252\t val loss: 1.85305\t best val loss: 1.75563\n",
      "CV_1 (40.19s) -\t train_loss: 1.36252\t val_loss: 1.85305\t val_acc: 0.37380\t val_recall: 0.37445\t val_f1: 0.36806\n",
      "\tearly stopping at ep.15\t train loss: 1.36782\t val loss: 1.87332\t best val loss: 1.80363\n",
      "CV_2 (29.10s) -\t train_loss: 1.36782\t val_loss: 1.87332\t val_acc: 0.37970\t val_recall: 0.37871\t val_f1: 0.37654\n",
      "\tearly stopping at ep.16\t train loss: 1.44037\t val loss: 1.87343\t best val loss: 1.78689\n",
      "CV_3 (30.48s) -\t train_loss: 1.44037\t val_loss: 1.87343\t val_acc: 0.35970\t val_recall: 0.35897\t val_f1: 0.35252\n",
      "\tearly stopping at ep.16\t train loss: 1.41059\t val loss: 1.85946\t best val loss: 1.79366\n",
      "CV_4 (31.52s) -\t train_loss: 1.41059\t val_loss: 1.85946\t val_acc: 0.37330\t val_recall: 0.37309\t val_f1: 0.36538\n",
      "\tearly stopping at ep.28\t train loss: 1.40067\t val loss: 1.93786\t best val loss: 1.83506\n",
      "CV_5 (53.72s) -\t train_loss: 1.40067\t val_loss: 1.93786\t val_acc: 0.34600\t val_recall: 0.34728\t val_f1: 0.34031\n",
      "## Experiment results (185.30s) - {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.001, 'optimiser': None, 'bn': True, 'batch_size': 8}\n",
      "# Metrics - train loss: 1.39640 \t val loss: 1.87943 \t val accuracy: 0.36650 \t val recall: 0.36650 \t val f1: 0.36056\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.001, 'optimiser': None, 'bn': True, 'batch_size': 4}\n",
      "\tearly stopping at ep.26\t train loss: 1.88341\t val loss: 1.71538\t best val loss: 1.69360\n",
      "CV_1 (101.46s) -\t train_loss: 1.88341\t val_loss: 1.71538\t val_acc: 0.39730\t val_recall: 0.39841\t val_f1: 0.38883\n",
      "\tearly stopping at ep.25\t train loss: 1.88773\t val loss: 1.72332\t best val loss: 1.69668\n",
      "CV_2 (90.43s) -\t train_loss: 1.88773\t val_loss: 1.72332\t val_acc: 0.39890\t val_recall: 0.39780\t val_f1: 0.39008\n",
      "\tearly stopping at ep.21\t train loss: 1.88016\t val loss: 1.75129\t best val loss: 1.71029\n",
      "CV_3 (76.29s) -\t train_loss: 1.88016\t val_loss: 1.75129\t val_acc: 0.37960\t val_recall: 0.37939\t val_f1: 0.37752\n",
      "\tearly stopping at ep.23\t train loss: 1.89378\t val loss: 1.71599\t best val loss: 1.71000\n",
      "CV_4 (83.36s) -\t train_loss: 1.89378\t val_loss: 1.71599\t val_acc: 0.39880\t val_recall: 0.39870\t val_f1: 0.38976\n",
      "\tearly stopping at ep.25\t train loss: 1.87608\t val loss: 1.72632\t best val loss: 1.71195\n",
      "CV_5 (89.69s) -\t train_loss: 1.87608\t val_loss: 1.72632\t val_acc: 0.39420\t val_recall: 0.39437\t val_f1: 0.38458\n",
      "## Experiment results (441.49s) - {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.001, 'optimiser': None, 'bn': True, 'batch_size': 4}\n",
      "# Metrics - train loss: 1.88423 \t val loss: 1.72646 \t val accuracy: 0.39376 \t val recall: 0.39373 \t val f1: 0.38615\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.001, 'optimiser': None, 'bn': True, 'batch_size': 2}\n",
      "\tearly stopping at ep.26\t train loss: 2.30252\t val loss: 2.30356\t best val loss: 2.17064\n",
      "CV_1 (181.44s) -\t train_loss: 2.30252\t val_loss: 2.30356\t val_acc: 0.09530\t val_recall: 0.10000\t val_f1: 0.01740\n",
      "\tearly stopping at ep.13\t train loss: 2.28856\t val loss: 2.26322\t best val loss: 2.22687\n",
      "CV_2 (93.16s) -\t train_loss: 2.28856\t val_loss: 2.26322\t val_acc: 0.15610\t val_recall: 0.15362\t val_f1: 0.10786\n",
      "\tearly stopping at ep.24\t train loss: 2.30258\t val loss: 2.30238\t best val loss: 2.16847\n",
      "CV_3 (170.29s) -\t train_loss: 2.30258\t val_loss: 2.30238\t val_acc: 0.09800\t val_recall: 0.10000\t val_f1: 0.01785\n",
      "\tearly stopping at ep.26\t train loss: 2.29479\t val loss: 2.29318\t best val loss: 2.18127\n",
      "CV_4 (181.66s) -\t train_loss: 2.29479\t val_loss: 2.29318\t val_acc: 0.12380\t val_recall: 0.12318\t val_f1: 0.06606\n",
      "\tearly stopping at ep.26\t train loss: 2.30253\t val loss: 2.30403\t best val loss: 2.18006\n",
      "CV_5 (185.49s) -\t train_loss: 2.30253\t val_loss: 2.30403\t val_acc: 0.09790\t val_recall: 0.10000\t val_f1: 0.01783\n",
      "## Experiment results (812.34s) - {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.001, 'optimiser': None, 'bn': True, 'batch_size': 2}\n",
      "# Metrics - train loss: 2.29820 \t val loss: 2.29327 \t val accuracy: 0.11422 \t val recall: 0.11536 \t val f1: 0.04540\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.001, 'optimiser': 'Adam', 'bn': False, 'batch_size': 16}\n",
      "\tearly stopping at ep.22\t train loss: 1.54703\t val loss: 2.00533\t best val loss: 1.92574\n",
      "CV_1 (15.95s) -\t train_loss: 1.54703\t val_loss: 2.00533\t val_acc: 0.33610\t val_recall: 0.33603\t val_f1: 0.32538\n",
      "\tearly stopping at ep.14\t train loss: 1.43407\t val loss: 2.05378\t best val loss: 1.94769\n",
      "CV_2 (12.72s) -\t train_loss: 1.43407\t val_loss: 2.05378\t val_acc: 0.33660\t val_recall: 0.33484\t val_f1: 0.31685\n",
      "\tearly stopping at ep.30\t train loss: 1.45570\t val loss: 1.99932\t best val loss: 1.92199\n",
      "CV_3 (20.57s) -\t train_loss: 1.45570\t val_loss: 1.99932\t val_acc: 0.35070\t val_recall: 0.34897\t val_f1: 0.33566\n",
      "\tearly stopping at ep.36\t train loss: 1.45909\t val loss: 1.96865\t best val loss: 1.87899\n",
      "CV_4 (25.43s) -\t train_loss: 1.45909\t val_loss: 1.96865\t val_acc: 0.34040\t val_recall: 0.33873\t val_f1: 0.31388\n",
      "\tearly stopping at ep.38\t train loss: 1.44176\t val loss: 2.04502\t best val loss: 1.97204\n",
      "CV_5 (27.39s) -\t train_loss: 1.44176\t val_loss: 2.04502\t val_acc: 0.34550\t val_recall: 0.34490\t val_f1: 0.33419\n",
      "## Experiment results (102.26s) - {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.001, 'optimiser': 'Adam', 'bn': False, 'batch_size': 16}\n",
      "# Metrics - train loss: 1.46753 \t val loss: 2.01442 \t val accuracy: 0.34186 \t val recall: 0.34069 \t val f1: 0.32519\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.001, 'optimiser': 'Adam', 'bn': False, 'batch_size': 8}\n",
      "\tearly stopping at ep.33\t train loss: 1.45264\t val loss: 1.75524\t best val loss: 1.71159\n",
      "CV_1 (40.96s) -\t train_loss: 1.45264\t val_loss: 1.75524\t val_acc: 0.41430\t val_recall: 0.41612\t val_f1: 0.41082\n",
      "\tearly stopping at ep.48\t train loss: 1.43801\t val loss: 1.72160\t best val loss: 1.65153\n",
      "CV_2 (60.66s) -\t train_loss: 1.43801\t val_loss: 1.72160\t val_acc: 0.40960\t val_recall: 0.40941\t val_f1: 0.39571\n",
      "\tearly stopping at ep.23\t train loss: 1.45574\t val loss: 1.76745\t best val loss: 1.71303\n",
      "CV_3 (28.85s) -\t train_loss: 1.45574\t val_loss: 1.76745\t val_acc: 0.38620\t val_recall: 0.38823\t val_f1: 0.37733\n",
      "\tearly stopping at ep.33\t train loss: 1.48513\t val loss: 1.75944\t best val loss: 1.72890\n",
      "CV_4 (41.65s) -\t train_loss: 1.48513\t val_loss: 1.75944\t val_acc: 0.38170\t val_recall: 0.38219\t val_f1: 0.37633\n",
      "\tearly stopping at ep.36\t train loss: 1.46808\t val loss: 1.76210\t best val loss: 1.73201\n",
      "CV_5 (45.09s) -\t train_loss: 1.46808\t val_loss: 1.76210\t val_acc: 0.40820\t val_recall: 0.40825\t val_f1: 0.39681\n",
      "## Experiment results (217.43s) - {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.001, 'optimiser': 'Adam', 'bn': False, 'batch_size': 8}\n",
      "# Metrics - train loss: 1.45992 \t val loss: 1.75317 \t val accuracy: 0.40000 \t val recall: 0.40084 \t val f1: 0.39140\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.001, 'optimiser': 'Adam', 'bn': False, 'batch_size': 4}\n",
      "\tearly stopping at ep.40\t train loss: 1.41704\t val loss: 1.59136\t best val loss: 1.56526\n",
      "CV_1 (94.72s) -\t train_loss: 1.41704\t val_loss: 1.59136\t val_acc: 0.43850\t val_recall: 0.43982\t val_f1: 0.43279\n",
      "\tearly stopping at ep.33\t train loss: 1.40963\t val loss: 1.62225\t best val loss: 1.57550\n",
      "CV_2 (80.56s) -\t train_loss: 1.40963\t val_loss: 1.62225\t val_acc: 0.43010\t val_recall: 0.42867\t val_f1: 0.40976\n",
      "\tearly stopping at ep.46\t train loss: 1.42468\t val loss: 1.59877\t best val loss: 1.55895\n",
      "CV_3 (109.17s) -\t train_loss: 1.42468\t val_loss: 1.59877\t val_acc: 0.44550\t val_recall: 0.44541\t val_f1: 0.43824\n",
      "\tearly stopping at ep.22\t train loss: 1.41978\t val loss: 1.61667\t best val loss: 1.58093\n",
      "CV_4 (52.62s) -\t train_loss: 1.41978\t val_loss: 1.61667\t val_acc: 0.43720\t val_recall: 0.43710\t val_f1: 0.43858\n",
      "\tearly stopping at ep.35\t train loss: 1.40998\t val loss: 1.65730\t best val loss: 1.54811\n",
      "CV_5 (85.30s) -\t train_loss: 1.40998\t val_loss: 1.65730\t val_acc: 0.44310\t val_recall: 0.44387\t val_f1: 0.42899\n",
      "## Experiment results (422.56s) - {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.001, 'optimiser': 'Adam', 'bn': False, 'batch_size': 4}\n",
      "# Metrics - train loss: 1.41622 \t val loss: 1.61727 \t val accuracy: 0.43888 \t val recall: 0.43897 \t val f1: 0.42967\n",
      "\n",
      "\n",
      "### Running model {'weight_decay': 0.0, 'dropout': [0.0, 0.0, 0.0, 0.0], 'hidden_layers': [128, 64, 32, 10], 'lr': 0.001, 'optimiser': 'Adam', 'bn': False, 'batch_size': 2}\n",
      "\tearly stopping at ep.25\t train loss: 1.38701\t val loss: 1.50587\t best val loss: 1.49426\n",
      "CV_1 (116.24s) -\t train_loss: 1.38701\t val_loss: 1.50587\t val_acc: 0.46630\t val_recall: 0.46755\t val_f1: 0.45899\n"
     ]
    }
   ],
   "source": [
    "# Seed for reproducibility\n",
    "np.random.seed(0)\n",
    "\n",
    "# Generate training and validation datasets for each fold\n",
    "fold_splits = []\n",
    "for i in range(n_folds):\n",
    "        # Generate training and validation sets for this fold\n",
    "        val_indices = folds[i]\n",
    "        train_indices = np.hstack(folds[:i] + folds[i+1:])\n",
    "        \n",
    "        X_train, y_train = X[train_indices], y[train_indices]\n",
    "        X_val, y_val = X[val_indices], y[val_indices]\n",
    "        fold_splits.append((X_train, y_train, X_val, y_val))\n",
    "\n",
    "outputs = []\n",
    "\n",
    "print('Begin Hyper-parameter Tuning -')\n",
    "for hyperparams in itertools.product(weight_decay_options, drop_out_options, hidden_layer_options, lr_options, optimiser_options, bn_option, batch_size_options):\n",
    "        # Convert hyperparams to dictionary for easier access\n",
    "        hyperparams_dict = {\n",
    "                'weight_decay': hyperparams[0],\n",
    "                'dropout': hyperparams[1],\n",
    "                'hidden_layers': hyperparams[2],\n",
    "                'lr': hyperparams[3],\n",
    "                'optimiser': hyperparams[4],\n",
    "                'bn': hyperparams[5],\n",
    "                'batch_size': hyperparams[6]\n",
    "        }\n",
    "        print(f'\\n\\n### Running model {str(hyperparams_dict)}')\n",
    "        try:\n",
    "                start = time.time()\n",
    "                cv_metrics = run_model_cv(fold_splits, hyperparams_dict)\n",
    "                end = time.time()\n",
    "                time_taken = end-start\n",
    "                \n",
    "                cv_train_loss_scores, cv_val_loss_scores, cv_val_accuracy_scores, cv_val_recall_scores, \\\n",
    "                        cv_val_f1_scores, cv_times, detailed_train_losses, detailed_val_losses = cv_metrics\n",
    "                train_loss = np.mean(cv_train_loss_scores)\n",
    "                val_loss = np.mean(cv_val_loss_scores)\n",
    "                val_accuracy = np.mean(cv_val_accuracy_scores)\n",
    "                val_recall = np.mean(cv_val_recall_scores)\n",
    "                val_f1 = np.mean(cv_val_f1_scores)\n",
    "                \n",
    "                print(f'## Experiment results ({time_taken:.2f}s) - {str(hyperparams_dict)}')\n",
    "                print(f'# Metrics - train loss: {train_loss:.5f} \\t val loss: {val_loss:.5f} \\t val accuracy: {val_accuracy:.5f} \\t val recall: {val_recall:.5f} \\t val f1: {val_f1:.5f}')\n",
    "                # Print the experiment results string and print to file\n",
    "                \n",
    "                log_to_file(hyperparams_dict, cv_metrics, time_taken)\n",
    "                \n",
    "        except Exception as e:\n",
    "                print(f'Error: {e}')\n",
    "                log_to_file(hyperparams_dict, [[0]*n_folds]*8, 99999)\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin Hyper-parameter Tuning -\n",
      "\n",
      "\n",
      "### Running model (0.0001, 16, 'Adam', True)\n",
      "Epoch 1/1, Train loss: 2.23815, Val loss: 2.10822\n",
      "CV_1 (2.19s) -\t train_loss: 2.23815\t val_loss: 2.10822\t val_acc: 0.23400\n",
      "\n",
      "Epoch 1/1, Train loss: 2.22117, Val loss: 2.09725\n",
      "CV_2 (2.25s) -\t train_loss: 2.22117\t val_loss: 2.09725\t val_acc: 0.25470\n",
      "\n",
      "Epoch 1/1, Train loss: 2.18520, Val loss: 2.32576\n",
      "CV_3 (2.38s) -\t train_loss: 2.18520\t val_loss: 2.32576\t val_acc: 0.19520\n",
      "\n",
      "Epoch 1/1, Train loss: 2.18423, Val loss: 2.26142\n",
      "CV_4 (2.19s) -\t train_loss: 2.18423\t val_loss: 2.26142\t val_acc: 0.20870\n",
      "\n",
      "Epoch 1/1, Train loss: 2.20537, Val loss: 2.23239\n",
      "CV_5 (2.14s) -\t train_loss: 2.20537\t val_loss: 2.23239\t val_acc: 0.20060\n",
      "\n",
      "## Experiment results (14.90s) - (0.0001, 16, 'Adam', True)\n",
      "# CV Metrics - train: 2.20682\t val: 2.20501\t val_acc: (mean 0.21864) [0.234, 0.2547, 0.1952, 0.2087, 0.2006]\n"
     ]
    }
   ],
   "source": [
    "# Seed for reproducibility\n",
    "np.random.seed(0)\n",
    "\n",
    "# Generate training and validation datasets for each fold\n",
    "fold_splits = []\n",
    "for i in range(n_folds):\n",
    "        # Generate training and validation sets for this fold\n",
    "        val_indices = folds[i]\n",
    "        train_indices = np.hstack(folds[:i] + folds[i+1:])\n",
    "        \n",
    "        X_train, y_train = X[train_indices], y[train_indices]\n",
    "        X_val, y_val = X[val_indices], y[val_indices]\n",
    "        fold_splits.append((X_train, y_train, X_val, y_val))\n",
    "\n",
    "outputs = []\n",
    "\n",
    "print('Begin Hyper-parameter Tuning -')\n",
    "for hyperparams in itertools.product(lr_options, batch_size_options, optimiser_options, bn_option):\n",
    "        print(f'\\n\\n### Running model {str(hyperparams)}')\n",
    "        start = time.time()\n",
    "        cv_metrics = run_model_cv(fold_splits, hyperparams)\n",
    "        end = time.time()\n",
    "        time_took = end-start\n",
    "        \n",
    "        train_losses, val_losses, val_accuracies, times = cv_metrics\n",
    "        train_loss = np.mean(train_losses)\n",
    "        val_loss = np.mean(val_losses)\n",
    "        \n",
    "        print(f'## Experiment results ({time_took:.2f}s) - {str(hyperparams)}')\n",
    "        print(f'# CV Metrics - train: {train_loss:.5f}\\t val: {val_loss:.5f}\\t val_acc: (mean {np.mean(val_accuracies):.5f}) {str(val_accuracies)}')\n",
    "        outputs.append((hyperparams, cv_metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((0.0001, 16, 'Adam', True), ([2.2381500206835176, 2.221170012953086, 2.1851958137467906, 2.1842262844334037, 2.2053723065266966], [2.108224230188417, 2.097252578339957, 2.3257567537189514, 2.261423767114958, 2.2323944851865822], [0.234, 0.2547, 0.1952, 0.2087, 0.2006], [2.1857736110687256, 2.2517547607421875, 2.376272678375244, 2.189086675643921, 2.1445536613464355]))]\n"
     ]
    }
   ],
   "source": [
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
